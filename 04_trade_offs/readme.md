## 5.c. Resource Optimization Trade-offs

### 5.c.0. Рамка: всё — компромисс

> “Everything in software architecture is a trade-off.”
> — Mark Richards

За предыдущие части мы уже:

* расковыряли кэш и консистентность;
* посмотрели на распределённые транзакции и eventual consistency;
* поговорили про шардирование, rebalancing;
* а в производительности — про latency, tail, throughput и законы масштабирования.

Теперь честный вопрос:
**чем мы за всё это платим?**

Платим мы всегда одними и теми же штуками:

* CPU;
* памятью и кэшем;
* диском и сетью;
* сложностью системы;
* и банально деньгами.

И всё это должно при этом как-то укладываться в:

* наши **SLO**  (Service Level Objective) — внутренние цели по задержкам и доступности;
* и обещания наружу (**SLA**) (Service Level Agreement), если мы их даём.

Идеальная система “быстрая, дешёвая, надёжная, простая” не существует.
Всегда есть вопрос: **какой именно ценой мы покупаем себе производительность и надёжность**.

Это тот же вопрос, который любят задавать на архитектурных собеседованиях:

> “На что вы готовы поменять latency, если бизнес хочет в два раза удешевить инфраструктуру?”

Именно под такие разговоры и нужна голова, натренированная думать в категориях trade-off’ов, а не “хочу, чтобы всё было хорошо одновременно”.

---

### 5.c.1. CPU & Concurrency: эффективность vs предсказуемость

Начнём с самого любимого графика менеджмента — **CPU utilization**.

Обычно диалог такой:

> — Почему у нас CPU 40–50%?
> Давайте оптимизируем, чтобы было хотя бы 80–90, а то железо простаивает.

Звучит логично, пока не вспоминаем теорию очередей.

на монтаже картинка “knee curve”:
![img.png](img.png)

* по **X** — загрузка системы ρ (от 0 до 1);
* по **Y** — среднее время ожидания / длина очереди.

Кривая какая:

* до 50–60% всё плавно;
* ближе к 70–80% начинает заметно загибаться;
* после 85–90% уходит вверх как ракета.

Интуиция:

> Чем ближе мы подходим к максимальной пропускной способности,
> тем быстрее растёт очередь и время ожидания.
> При ρ → 1 среднее время ожидания стремится к бесконечности.

То есть:

* пока CPU загружен на 50–60% — запросы приходят, обрабатываются, уходят;
* при 80–85% — система ещё “держится”, но хвосты уже начинают гулять;
* при 90–95% — мы формально “эффективно используем ресурсы”,
  но по факту живём на вертикальном участке кривой: любая небольшая вспышка нагрузки → очереди взлетают → latency и p99 улетают.

И вот тут первый **ресурсный trade-off**:

* **Latency-critical сервисы** (UI-API, auth, checkout):

    * мы сознательно оставляем **headroom по CPU** — 40–60%, иногда 60–70%;
    * не гоняемся за “100% utilization”;
    * платим за “недоиспользованное железо”,
      но покупаем **предсказуемые хвосты и запас на пики**.
* **Batch / аналитика, оффлайн-обработка**:

    * можем смело жить на 80–90% CPU;
    * хвосты там почти не важны — важно, чтобы джоба закончилась к утру.

Математически это одна и та же кривая,
архитектурно — два разных решения:

> “Здесь мы платим деньгами, чтобы не ловить бесконечные очереди”,
> “А здесь мы экономим на железе, потому что задержка не критична”.

---

### 5.c.2. Память и кэш: hit ratio vs стоимость vs сложность

Следующий любимый способ “оптимизировать ресурсы” — сказать:

> “Давайте всё закэшируем, и проблема с производительностью уйдёт”.

Технически идея верная:

* хороший кэш повышает **throughput** и снижает **нагрузку** на БД/внешние API;
* за счёт этого выигрываем и по latency, и по стоимости нижних уровней.

Но у этого компромисса три стороны:

1. **Память**
   Больше кэша → нужны жирнее инстансы / больше Redis-нод.

2. **Сложность**
   Инвалидация, TTL, согласованность между уровнями, cache stampede / thundering herd, coalescing, per-key mutex — всё это не бесплатно по мозгам.

3. **Риски хвостов**
   Промахи по кэшу на горячих ключах могут устраивать локальные штормы:

    * 1000 запросов одновременно мимо кэша в БД;
    * рост очередей и p99 именно в те моменты, когда кэш “дышит”.

В итоге trade-off выглядит так:

* хотим меньше грузить БД и внешние сервисы → увеличиваем кэш →
  платим за память **и** за сложность инвалидации;
* хотим простую систему и дешёвые машины → уменьшаем кэш →
  платим ростом нагрузки на нижние уровни и потенциальными хвостами при промахах.

Здесь важно не скатиться в религию “всё кэшировать”:

* кэшировать имеет смысл **hot-данные**, часто читаемые и относительно стабильные;
* бизнес-критичную, часто меняющуюся информацию с жёсткими требованиями по консистентности можно дешевле держать в БД, чем строить вокруг неё монстра из кэшей и инвалидации.

---

### 5.c.3. Data Layer: надёжность и удобство vs latency vs стоимость

На уровне данных базовая боль всегда одна и та же:

> “Сделайте, чтобы оно не падало и было удобно работать.”

А дальше начинаются решения, за которые мы платим ресурсами.

#### Репликация

* **Read-heavy** сценарии:

    * реплики БД помогают масштабировать чтения;
    * можно держать больше read-трафика без роста latency;
    * цена: больше машин, сложнее следить за лагами репликации.

* **Write-heavy + строгая консистентность**:

    * синхронная репликация, кворумы (R+W>N) →
      время записи = медленнейшая реплика в кворуме;
    * throughput по записи упирается в самый медленный/дальний узел;
    * цена: **latency на запись + деньги за железо и сеть**.

> Хотим сильную отказоустойчивость и строгую консистентность —
> платим задержками и стоимостью инфраструктуры.
> Ослабляем требования — экономим, но получаем тему из части про eventual consistency.

#### Индексы и денормализация

* Индекс:

    * ускоряет **чтение**;
    * замедляет **запись** (каждый insert/update = работа с индексом).
* Материализованные вьюхи / денормализация:

    * ускоряют сложные запросы;
    * усложняют пайплайн записи — нужно понимать, как обновлять всё это добро без дурацких гонок.

Trade-off:

* хотим быстрые SELECT и гибкие запросы → добавляем индексы, вьюхи, денормализацию → платим write-throughput и сложностью обновлений;
* хотим максимально быстрые записи → режем индексы, меняем модель данных → усложняем фронтам жизнь, переносим логику в приложение.

---

### 5.c.4. Сеть и топология: отказоустойчивость vs RTT vs деньги

Дальше — любимое “давайте сделаем мульти-AZ/мульти-регион, чтобы не падало”.

На картинке можно показать два кейса:

* всё в одном регионе / AZ(Availability Zone);
* сервисы и базы раскиданы по зонам/регионам.

**Один регион / одна зона:**

* плюсы:

    * маленький RTT (Round-Trip Time) - время “туда и обратно” для запроса по сети.;
    * дешёвый трафик;
    * проще отлаживать.
* минусы:

    * падает регион/АЗ — всё, до свидания.

**Multi-AZ / multi-region:**

* плюсы:

    * лучше доступность;
    * возможен DR-сценарий (Disaster Recovery) “один регион умер, другой живёт”.
* минусы:

    * каждый кворум записи = суммарный RTT между зонами/регионами;
    * реальная latency запросов растёт;
    * cross-region трафик стоит денег и неприятных сюрпризов в счёте.

Trade-off:

* хотим 
    RPO≈0 (Recovery Point Objective - сколько данных мы готовы потерять в случае падения.) 
    и RTO (Recovery Time Objective— за сколько времени система должна подняться после падения.) 
    в минуты → платим дополнительной latency, сложностью топологии и счетом от облака;
* готовы мириться с бóльшим временем восстановления в случае катастрофы →
  держим всё попроще и дешевле.

И опять это должно опираться на **SLO** (Service Level Objective), а не на веру:

> Если у вас продукт, который может спокойно пережить час-два даунтайма раз в год,
> мульти-регион с синхронными кворумами — это не “архитектура уровня бог”,
> а бесполезный золотой унитаз.

---

### 5.c.5. Изоляция vs Utilization: noisy neighbors

Следующий ресурсный компромисс — **изоляция против эффективности**.

Сценарий:

* кладём всё в один общий кластер:

    * API, очереди, batch-джобы, аналитика, админки.
* на бумаге:

    * ресурсы прекрасно утилизируются;
    * системные графики красивые.
* в жизни:

    * по ночам ETL, аналитика, бэкапы — и p99 у фронтовых API улетает;
    * на одной ноде заводится noisy neighbor, который жрёт CPU/диск/сеть;
    * хвосты становятся непредсказуемыми.

**Shared пул:**

* плюсы:

    * дешево на старте;
    * хорошие средние показатели утилизации.
* минусы:

    * эффект noisy neighbor;
    * сложность отладки “почему только иногда всё тормозит”.

**Изолированные пулы:**

* отдельный кластер / pool для:

    * latency-critical API;
    * критичных баз;
    * тяжёлых batch/аналитик.
* плюсы:

    * предсказуемые хвосты;
    * проще ограничить impact:
      “упала аналитика — пользователи даже не заметили”.
* минусы:

    * где-то CPU реально будет “простаивать”;
    * стоимость инфраструктуры выше.

Trade-off:

> Хотим ровные графики p95/p99 и предсказуемое поведение —
> платим за изоляцию и “недоиспользованное” железо.
> Хотим выжать максимум из каждой ноды — принимаем, что noisy neighbor периодически будет ломать нам хвосты.

---

### 5.c.6. Сложность vs “экономия на железе”

И последний, но очень важный ресурс — **мозги и время команды**.

Любая “умная оптимизация ресурсов” выглядит примерно так:

* динамический контроллер concurrency;
* хитрый автоскейлинг по десяти метрикам;
* несколько очередей с приоритетами;
* backpressure, который то душит, то отпускает сервисы;
* десятки конфигов, в которые страшно смотреть.

Цена:

* система становится **непрозрачной**;
* поведение под нагрузкой перестаёт быть очевидным даже для тех, кто её строил;
* каждая новая фича — боль, потому что нужно учитывать полсотни взаимных влияний.

Альтернатива — **примитивная, но чуть переобеспеченная архитектура**:

* простая модель масштабирования (горизонтальный скейл по CPU/нагрузке);
* чуть больше машин, чем минимально необходимо;
* минимальное количество “умных” контроллеров и автоматики.

Trade-off:

* хотим максимально выжать ресурсы →
  усложняем систему, платим временем и рисками;
* хотим простую и управляемую систему →
  переплачиваем за железо, но экономим на людях и авариях.

Это прямое продолжение First Law:

> Хотите платить меньше за облако — будете платить больше сложностью и поддержкой.
> Хотите дешёвую поддержку и понятную архитектуру — будете платить за некоторый ресурсный запас.

---

### 5.c.7. Финал: как принимать решения

Соберём всё, что мы только что обсудили.

1. **CPU**

    * headroom vs высокая утилизация;
    * предсказуемость хвостов vs “железо не простаивает”.

2. **Память и кэш**

    * hit ratio vs стоимость RAM и сложность инвалидации.

3. **Data layer**

    * репликация, индексы, денормализация:

        * надёжность и удобство запросов vs latency и write-throughput.

4. **Сеть и топология**

    * мульти-регион / мульти-AZ:

        * доступность и DR vs RTT и деньги.

5. **Изоляция**

    * выделенные пулы и кластера:

        * стабильные хвосты vs “плохо утилизуем железо”.

6. **Сложность**

    * умные схемы оптимизации ресурсов:

        * экономия на железе vs риск сделать систему неуправляемой.

И всё это завязано на очень простой вопрос:

> **Какие SLO мы берём на себя и какие счета готовы за это платить — в деньгах и в сложности?**

Ровно в таком виде это и прилетает на собесах по архитектуре:

* “что вы упростите или пожертвуете, если нужно вдвое удешевить инфраструктуру?”;
* “что будете менять, если бизнесу критичнее латентность, чем консистентность отчётов?”;
* “где вы согласитесь на более сложную архитектуру, чтобы не платить за железо?”

Хороший ответ здесь — не “я хочу и быстро, и дёшево, и надёжно”,
а честно проговорённые варианты компромиссов и понимание, чем вы готовы платить в первую очередь.

Нет “идеальных” архитектур.
Есть только честно выбранные компромиссы:

* где мы сознательно платим деньгами за производительность и предсказуемость;
* где мы сознательно экономим на железе и принимаем больший хвост или худший SLO;
* где мы ограничиваем себя в гибкости, чтобы не утонуть в сложности.

И это ровно тот слой мышления, которым отличается “я просто пишу микросервис” от “я проектирую систему, которая будет жить, масштабироваться и стоить каких-то денег в реальном мире”.

# Часть 3. Распределённые транзакции и консистентность

### Блок 3.1. От 2PC до Saga

## Вступление

Мы распределили данные по шардам, логику по сервисам, но бизнес-процессы никуда не делись: 
деньги должны списываться один раз, товар — резервироваться, заказ — создаваться. И всё это проходит через разные базы и сервисы.

Мы уже несколько раз упоминали боль “транзакций между шардами”, сейчас разберем

Главный вопрос:

**как выполнить многосервисную операцию так, чтобы сбой в середине не разрушил весь сценарий?**

ACID внутри одной базы решает это одной транзакцией. 
В распределённой системе такого инструмента нет: разные сервисы - разные базы, каждый сервис может зависнуть или ребутнуться, сеть добавляет сложностей.

**Глобальный `BEGIN…COMMIT` невозможен.**
Единственный реальный инструмент — набор протоколов, которые пытаются приблизиться к атомарности.

---

## 3.1.2. Two-Phase Commit (2PC)

2PC — это **конкретный протокол распределённой транзакции**: есть координатор и несколько участников (баз).

* В Postgres/Citus координатором выступает **coordinator node**, которая рассылает команды worker-нодам.
* Сам микросервис — просто клиент, который обязан уметь послать базе нужные команды (`PREPARE`, `COMMIT PREPARED`, `ROLLBACK PREPARED`).

### Фазы 2PC

С точки зрения протокола есть два этапа:

1. **Фаза голосования (prepare phase)**
   Координатор говорит участникам: “выполните локальную транзакцию и подготовьтесь к коммиту”.

   В Postgres это выглядит так:

   ```sql
   BEGIN;
   UPDATE accounts SET balance = balance - 100 WHERE id = 1;
   UPDATE accounts SET balance = balance + 100 WHERE id = 2;
   PREPARE TRANSACTION 'tx123';
   ```

   После `PREPARE TRANSACTION 'tx123'`:

    * изменения записаны на диск;
    * блокировки удерживаются;
    * транзакция **ещё не зафиксирована** и её результат не виден другим.

2. **Фаза фиксации (commit phase)**
   Координатор, получив `OK` от всех участников, решает исход:

    * если все “готовы” → шлёт `COMMIT PREPARED 'tx123'`;
    * если кто-то не смог подготовиться → шлёт `ROLLBACK PREPARED 'tx123'`.

### Практические проблемы

* координатор — single point of failure;
* не работает между разными СУБД, только внутри одного стека
* `PREPARE` держит блокировки, пока координатор принимает решение;
* tail latency определяется самой медленной нодой и состоянием сети;
* при сетевых разрывах система легко зависает в состоянии “подготовлено, но не закоммичено”.

Поэтому 2PC разумен внутри одной БД/кластера (типа Citus),
но как общий механизм между микросервисами — практически не используется: слишком хрупко и сложно

---

## 3.1.3. Three-Phase Commit (3PC)

Попытка сделать 2PC неблокирующим дополнительной фазой `preCommit` (намерение коммитить), благодаря чему учстник в дальнейшем сам может откатить миграцию, если координатор не перейдет к третьей стадии

требует жёстких таймингов (bounded delay): нужно уметь различать “узел умер” и “пакет задержался”, поэтому редко применяется

---

## 3.1.4. TCC (Try–Confirm–Cancel)

TCC — **прикладной паттерн распределённой транзакции** на уровне бизнес-логики. реализуется контрактами между сервисами.

Координатором выступает **отдельный сервис или workflow-движок** (Temporal к примеру), который вызывает три типа операций у участников:

* **Try** — зарезервировать ресурс
  (поставить hold на деньги, пометить товар как “зарезервирован”, занять слот бронирования).
* **Confirm** — зафиксировать операцию после успеха всех `Try`.
* **Cancel** — снять резерв, если кто-то из участников не прошёл `Try` или произошёл сбой.

Важно, что каждый микросервис должен:

* иметь отдельные методы/эндпоинты `Try/Confirm/Cancel`,
* хранить состояние резерва (ID операции, объём, срок жизни),
* уметь корректно обработать повторные вызовы (идемпотентность).

В отличие от 2PC:

* нет `PREPARE TRANSACTION` и блокировок в СУБД;
* “подготовленное состояние” живёт в бизнес-данных (таблица резервов/холд-операций),
* протокол не привязан к конкретной базе и может работать между разными стеками.

TCC применяют там, где ошибка стоит дорого: финтех, биллинг, бронирования, управление квотами.
Минус — рост сложности: больше состояний, больше оркестрации, нужны очистка протухших резервов и строгая дисциплина по контрактам.

## 3.1.5. Saga

Если TCC — это “сначала резерв, потом либо подтвердить, либо снять”,
то **Saga** — это “делаем шаги по очереди и при сбоях компенсируем уже сделанное”.

Saga — это **workflow из локальных транзакций**:

* каждый шаг — обычная локальная транзакция в своём сервисе;
* глобальной транзакции нет;
* при ошибке запускаются **компенсирующие действия** для уже выполненных шагов.

Пример для оформления заказа:

1. `Billing` списывает деньги.
2. `Inventory` резервирует товар.
3. `Orders` создаёт заказ.
4. `Notifications` отправляет письмо/пуш.

Если на шаге 2 не удалось зарезервировать товар:

* вызываем компенсацию в биллинге — `refund` или снятие hold.

Если на шаге 3 упало создание заказа:

* можно вернуть деньги,
* либо, если домен/регламенты не позволяют автоматический возврат,
  сделать **непрямую компенсацию**: отправить задачу менеджеру, письмо в support, сигнал в CRM — дальше человек разруливает ситуацию.

Компенсация не обязана быть “симметричной обратной операцией” — главное, чтобы процесс приходил в согласованное состояние по бизнес-правилам.

В отличие от TCC:

* нет обязательной фазы `Try` с резервами — шаги часто сразу “делают действие”;
* компенсации могут быть частично автоматическими, частично человеческими;
* больше допускается eventual consistency и временные “кривые” состояния.

Что даёт Saga:

* нет глобальных блокировок и подготовленных транзакций;
* шаги можно ретраить, компенсировать, мониторить;
* удобно масштабировать процессы между разными сервисами и стеками.

Цена — сложность самого workflow и продуманность компенсирующих сценариев.

## 3.1.6. Хореография vs оркестрация

### Хореография

Каждый участник **слушает события** других сервисов и выполняет свой шаг, не зная полной картины процесса.

**Пример:**
`Billing` публикует событие `PaymentCaptured` →
`Inventory` слушает его и пытается зарезервировать товар →
если получилось, кидает `InventoryReserved` →
`Orders` слушает это событие и создаёт заказ →
`Notifications` реагирует на `OrderCreated`.

Если к примеру не удастся зарезервировать товар, будет опубликовано сообщение `InventoryReservationFailed`, и `Billing` выполнит компенсирующее действие

То есть цепочка строится **реактивно**, без единой точки управления.

**Плюсы:**

* простая локальная логика внутри каждого сервиса;
* минимальная централизация.

**Минусы:**

* полный процесс размазан по событиям, его трудно понять;
* добавление нового шага ломает зависимые сервисы;
* отладка проблем затруднена из-за распределенности

---

### Оркестрация

Есть отдельный компонент — **orchestrator**, который управляет последовательностью шагов Saga и компенсациями.

Плюсы:

* процесс описан в одном месте;
* проще мониторить и восстанавливать;
* легче добавлять шаги.

Минусы:
- единая точка отказа

Оркестрация — стандартный путь, когда процесс становится сложнее пары сервисов.

---

## 3.1.7. Workflow движки (temporal)

Как я сказал выше оркестратором TCC/Saga может быть Temporal

Это один из специальных workflow-движков:

* хранят состояние процесса;
* гарантируют ретраи шагов;
* восстанавливаются после падения;
* управляют таймингами;

Есть и другие - Camunda / Conductor

Принцип Temporal прост:

* вы описываете workflow как обычный код;
* движок реплеит его состояние при рестарте,
* а “активити” (внешние операции) выполняются с гарантированными ретраями.

Это делает Saga практичной и удобной, а не кучей кронов, очередей и костылей.

---

## 3.1.8. Outbox + CDC

Без надёжной доставки событий Saga невозможна.

**Transactional Outbox** решает проблему dual-write:

* в одной локальной транзакции пишем бизнес-данные и запись в таблицу `outbox`;
* отдельный процесс (Debezium например (реализует CDC - change data capture)) доставляет событие в брокер;
* при сбое ничего не теряется, потому что данные и событие записаны атомарно.

Это обязательный слой в любых распределённых транзакциях.

---

## 3.1.9. Идемпотентность: зачем и как

В распределённой системе **повтор операции — норма**:

* сообщения могли задублироваться;
* запросы могли заретраиться из-за таймаутов;
* повторные вызовы компенсаций.

Идемпотентность гарантирует: повтор операции не меняет состояние повторно.

Минимальный механизм:

* уникальный `operation_id`;
* таблица/кэш обработанных операций;
* если операция уже выполнялась → вернуть прежний результат.

В API это отражается просто:

* если операция уже выполнена → вернуть `200 OK` или `204 No Content`;
* если в процессе → `409 Conflict` или `202 Accepted`.

---

## 3.1.10. Типичные сбои

Сага — не защита “от всего”. Реально встречаются:

* компенсации не проходят полностью;
* внешнее API висит, шаги тянут хвост latency;
* события приходят дважды или в другом порядке;
* DLQ превращается в кладбище необработанных сообщений.

Смысл Saga — не устранить риски, а сделать их управляемыми и наблюдаемыми.

---

## 3.1.11. Когда Saga не подходит

* сценарии без обратимых шагов (email, пуши, внешние уведомления);
* области, где нужна строгая консистентность, об этом поговорим дальше

Там ближе TCC, синхронные транзакции или вообще отказ от горизонтального масштабирования.

---
## 3.1.12. Event Sourcing как альтернатива

**Event Sourcing** — это другой способ хранить состояние:  
мы не записываем итоговые данные, а сохраняем **последовательность событий**, из которых состояние вычисляется.

Особенности:

* источник истины — журнал событий (`OrderCreated`, `ItemAdded`, `PaymentCaptured`);
* текущее состояние — результат применения всех событий (или снапшота + хвоста);
* чтение почти всегда идёт из проекций, которые строятся асинхронно → по определению eventual consistency;
* “откат” — это новое корректирующее событие, а не rollback транзакции.

Где применяют:

* **финтех, биллинг, трейдинг** — когда важна полная история изменений и возможность аудита;
* **игровые движки, симуляции** — когда состояние — это воспроизводимая последовательность шагов;
* **workflow-интенсивные системы**, где цена зачисления/возврата/коррекции должна быть видна как отдельное действие.

Для классического e-commerce (маркетплейса) Event Sourcing используется **редко**:  
обычно хватает Sagas + Outbox, а хранить весь заказ как лог событий зачастую избыточно.

Но важно воспринимать его как *реальную альтернативу* распределённым транзакциям:  
мы перестаём пытаться “зафиксировать всё сразу”,  
и начинаем строить систему вокруг истории изменений.

## Мост к следующей теме

В распределённых системах есть два крупных класса решений:

* **тяжёлые протоколы транзакций** (2PC/TCC), которые максимально приближают нас к атомарности между участниками ценой сложности и задержек;
* **процессные паттерны** (Saga, outbox, event-driven), которые упрощают жизнь сервисам,
  но делают расхождения между их состояниями нормой.

На уровне данных это выглядит так:

* есть куски, где нам нужна более строгая модель — балансы, права доступа, критичные инварианты;
* есть всё остальное, где мы готовы жить с тем, что разные сервисы какое-то время видят разные версии данных.

Дальше мы разберём, какие именно модели здесь стоят за кадром:

* **eventual consistency** — когда согласованность достигается со временем;
* **strong consistency** — когда система гарантирует более жёсткое совпадение состояния.

И главное — как эти модели влияют на архитектуру и поведение системы для пользователя.

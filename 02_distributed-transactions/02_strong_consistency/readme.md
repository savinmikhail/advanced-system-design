### Блок 3.3. Strong Consistency в распределённых системах (12–15 минут)

## 1. Зачем вообще говорить про strong consistency

В прошлом блоке мы приняли реальность **eventual consistency**: саги, outbox, асинхронные индексы, кеши — данные разъезжаются, но “когда-нибудь” сходятся.

Это устраивает для:

* поиска, лент, рекомендаций;
* аналитики;
* вторичных витрин.

Но есть зоны, где “когда-нибудь” = баг:

* деньги на счету;
  *ограниченные ресурсы, которые нельзя продать дважды — билеты, номера, места;
* критичные права доступа
* операции, где нет компенсации;

Там нам нужна модель, где после успешной записи **любое последующее чтение этого ключа** видит новое значение, независимо от того, на какой узел пришёл запрос. Это и есть strong consistency.

---

## 2. Определение strong consistency и linearizability

Интуитивное определение:

> После успешной записи любое последующее чтение этого же объекта возвращает новое значение,
> даже если запрос ушёл на другой узел или в другой датацентр.

Формально это почти всегда формулируют через **linearizability**:

* все операции над объектом можно упорядочить в одну глобальную последовательность;
* каждая операция “случается” как атомарная точка между своим `call` и `response`;
* никакой клиент не видит историю вида “новое значение → старое значение”, если не было отдельной явной операции, вернувшей старое.

То есть strong consistency = *“как будто у вас одна логическая копия объекта, даже если физических реплик много”*.

---

## 3. Почему strong consistency дорогая

Чтобы дать такие гарантии, система вынуждена:

* **согласовывать записи между узлами** (реплики, регионы) перед ответом клиенту;
* **поддерживать глобальный порядок операций**:

    * через лидера/primary;
    * через протоколы консенсуса (Raft, Paxos и т.п.);
* **жёстко реагировать на сетевые проблемы**: лучше отказать, чем вернуть старое значение.

Отсюда прямые последствия:

* latency записи растёт (дополнительные сетевые раунды);
* tail latency (p95/p99) определяется самой медленной нодой/линком;
* availability падает: при разделении сети или падении части реплик система должна отказывать в записи/чтении, чтобы не нарушить консистентность;
* write-throughput масштабируется сильно хуже “просто накинули реплик”.

Strong consistency — это всегда осознанный trade-off: меньше аномалий и рассинхронов в обмен на скорость, доступность и сложность.

---

## 5. Практические способы достигать strong consistency

### 5.1. Лидер и синхронная репликация

Классика: один **primary**, несколько **replica**.

* все записи идут на primary;
* для сильной консистентности включают синхронную репликацию (primary ждёт подтверждения нужного числа реплик перед `COMMIT`);
* критичные чтения идут на primary; чтения “попроще” могут идти на реплики с пониманием их отставания.

Сильная сторона — простая модель “истина на лидере”.
Слабая — write-throughput и latency упираются в этот лидер и в синхронные подтверждения.

---

### 5.2. Кворумы (quorum read / quorum write)

Если у нас несколько реплик одного объекта, можно работать кворумами:

* запись считается успешной после подтверждения от большинства (quorum write);
* чтение — из большинства (quorum read).

> правильно подобранные read/write-кворумы дают strong consistency для объекта,
> пока число отказавших узлов не превышает порога.

Цена — дополнительная сетевые раунды и рост tail latency.

---

### 5.3. Координация через lock/lease-сервисы

Для сериализации операций над объектом/разделом часто используют:

* ZooKeeper;
* etcd;
* другие lock-сервисы.

Схема:

* процесс получает lease/lock на ресурс;
* пока lease жив, только его операции считаются валидными;
* остальные либо ждут, либо получают отказ.

Это даёт сильные гарантии на узком участке (лидеры, координация шардов), но вводит ещё одну точку, через которую проходят “важные” операции — с соответствующей ценой по доступности и лагающим хвостам.

---

### 5.4. Узкое strong-ядро и остальной мир

На практике почти никогда не делают “вся система strict strong”.

Реализуют:

* небольшой слой/сервис с более жёсткой моделью (ledger, ACL, бронирования);
* вокруг — обычные микросервисы и проекции, которые живут на eventual consistency и подписаны на события от этого ядра.

Важное следствие:

> вопрос “у нас система strong или eventual?” некорректен.
> Корректен вопрос: “где у нас strong, где eventual, и как они согласованы между собой”.

---

## 6. Когда strong consistency не нужна и мешает

Самое полезное упражнение — честно вычеркнуть области, где она **не обязательна**.

На практике 90–95% данных в системе спокойно выдерживают eventual consistency,
а strong consistency нужна только на узком, но очень важном “ядре”.

И это ровно та развилка, которую любят поднимать на собесах:

> “Где в вашей системе вам действительно нужна strong consistency, а где вы сознательно согласитесь на eventual?”

Хороший ответ — это не “я хочу всё strong”, а набор конкретных сценариев:

* денежный ledger, жёсткие инварианты по остаткам, критичные ACL — strong;
* поиск, каталоги, кеши, аналитика, рекомендательные блоки — eventual с понятными компенсирующими механизмами.

---

## Финал: мост к кешированию

Strong consistency — это про аккуратно выделенное ядро, где мы готовы платить за строгие гарантии.
Всё вокруг почти всегда либо eventual, либо прямо строится на кешах и проекциях.

И как только в систему появляется репликация и кэширование, даже это ядро начинает сталкиваться с новыми проблемами: stale reads, инвалидация, write-through, write-around, hot keys.

На этом месте мы естественно переходим к следующей теме — кеширование,
которое усиливает производительность, но постоянно атакует согласованность.
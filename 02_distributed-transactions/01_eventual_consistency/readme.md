### Блок 3.2. Eventual Consistency в распределённых системах

## 1. Мост от распределённых транзакций

Саги и outbox неизбежно приводят нас к **eventual consistency**: 
деньги, товар, заказы и уведомления перестают жить в одной транзакции и обновляются с задержкой относительно друг друга.

Мы сознательно приняли модель:

> прямо сейчас разные сервисы могут видеть разные срезы данных,
> но если поток записей остановить, все хранилища придут к одному состоянию.

Дальше нас интересует не теория, а практические вопросы:

* что это означает для пользователя;
* какие типичные сбои мы получаем;
* как архитектурно уменьшить ущерб.

---

## 2. Что такое eventual consistency

> **Eventual consistency** — когда система допускает временную рассогласованность данных
> между репликами и сервисами,
> но гарантирует, что при отсутствии новых записей все копии данных со временем сойдутся.

Зачем так делать:

* повышаем доступность — не блокируем всю систему из-за одной реплики или региона;
* упрощаем масштабирование — можем шардировать и реплицировать без глобальных транзакций;
* снижаем стоимость записи — меньше блокировок, меньше координации.

Типичные примеры:

* кеши (Redis, CDN, in-process);
* оценки товаров, комментарии;
* корзина покупок;
* каталоги товаров;
* рекомендательные блоки;
* большинство аналитических витрин.

В этих местах:

* небольшие расхождения во времени терпимы;
* есть компенсации и повторные попытки;
* бизнес-последствия ошибки минимальны.

---

## 3. Почему глобальная жёсткая консистентность ломается

Несколько основных причин

### 3.1. Ненадёжная сеть - кто-то может не отвечать, могут теряться пакеты, таймауты случаются

### 3.2. Глобальные блокировки не масштабируются

Глобальный lock/transaction manager теоретически возможен, но на практике:

* любой подвисший участник стопорит всех;
* tail latency определяется самой медленной нодой;
* при росте числа шардов/реплик взаимодействия растут комбинаторно.

### 3.3. CAP-теорема

В распределённой системе сетевые разделения (P) — данность.

Дальше выбор:

* **CP** — жёсткая консистентность, но при проблемах с сетью часть запросов отклоняем;
* **AP** — продолжаем обслуживать запросы, но допускаем временное расхождение данных.

В реальных кластерах с несколькими регионами (Европа/США и т.п.) при сбоях линка почти всегда выбирают режим ближе к AP: каждый регион продолжает принимать заказы локально, а согласованность между регионами становится eventual.

[//]: # (картинку с cap теоремой)

## 4. Где eventual consistency уже живёт

* **Очереди / Kafka.**
  Consumer отстаёт — аналитика, нотификации и деривативные таблицы видят мир с лагом.

* **Поисковый индекс (Elastic/OpenSearch).**
  Запись в Postgres уже есть, документ в индексе появится через секунды.

* **Асинхронные реплики в Postgres.**
  WAL ещё не дотек — чтение с реплики показывает старые данные.

* **CDN / кеши.**
  Edge-кеш живёт с TTL, изменения в origin уже применились, но пользователю ещё отдают старую версию.

---

## 5. Типичные эффекты для пользователя и системы

### 5.1. Окно рассогласованности

Пользователь сделал действие, а интерфейс ещё какое-то время показывает старое состояние:
заказ создан, но в списке не виден; профиль обновлён, но старая аватарка торчит в части экранов.

### 5.2. Нарушение read-your-writes

**Read-your-writes** — свойство, при котором клиент после успешной записи всегда видит свои изменения при следующем чтении.

В eventual-сценариях оно легко ломается: писали в мастер, читаем из реплики/индекса/кеша, который не успел обновиться.

### 5.3. Потерянные обновления

Два клиента читают устаревшее состояние, оба его меняют и записывают. Последний перезаписывает изменения первого. Так появляются “тихие потери” данных.

### 5.4. Дубликаты и reorder событий

Брокеры доставки (**at-least-once**) по дизайну дают дубликаты и возможную перестановку сообщений.
Если операции не идемпотентны, система начинает списывать дважды, создавать дубликаты сущностей и т.п.

### 5.5. Несогласованные сервисы

Сага: `orders` уже создал заказ, `notifications` отправили письмо, а `billing` завалился и откатил списание. Пользователь видит сообщение о заказе, которого формально нет.

### 5.6. Некогерентный кеш

Многоуровневый кеш (L1, Redis, CDN) легко рассинхронизируется по TTL и инвалидациям.
Клиент получает старый state даже после успешной операции.

---

## 6. Как смягчить последствия: локальные приёмы

Важно: мы не “чиним eventual consistency”, а **управляем её эффектами**.

### 6.1. Read-after-write из того же пути

После записи читаем данные тем же способом, куда писали:

* заказ создали в Postgres → редирект на страницу заказа делает SELECT из Postgres, а не из ElasticSearch;
* обновили профиль — отдаем ответ и UI сразу использует payload ответа, а не полагается только на GET запрос

Это локально восстанавливает read-your-writes для автора операции.

### 6.2.Sticky sessions / Пиннинг чтений на мастер / реплику с гарантией

Если есть master + реплики:

* свежие чтения по важным данным временно пинним на master;
* либо используем механизм “реплика должна догнать LSN ≥ X”.

Примитивная версия — “критичные чтения сразу после записи никогда не идут на реплику”.

### 6.3. Оптимистичные блокировки

Чтобы не ловить потерянные обновления можем применять оптимистичные блокировки

Каждая запись содержит поле `version`
`UPDATE` делаем с условием `WHERE id = :id AND version = :v7`.
0 обновлённых строк → кто-то изменил запись раньше, нужно перечитать и решить, что делать.

Это не про межсервисную консистентность, а про защиту от “потерянных апдейтов” на одном хранилище.

Однако при большой конкуренции за ресурс может медленно работать из-за большого числа ретраев

### 6.5. Идемпотентность везде, где есть сеть

Подробно обсудили в предыдущей главе

### 6.6. Ретраи с backoff и ограничениями

Ретраи нужны, но:

* без backoff (увеличения паузы между повторами, обычно экспоненциально) вы превращаете временную деградацию в DDOS по своей же системе;
* нужны лимиты на число попыток и circuit breaker, чтобы не долбиться в мёртвую зависимость.
* нужно решить какой слой будет ретраить (микросервис/nginx/клиент), иначе будет retry storm

---

## 7. Архитектурные паттерны, которые помогают

### 7.1. Outbox pattern

Обсуждали в прошлой главе как работает и зачем нужен

### 7.4. Reconciliation

Любая сложная система со временем накапливает рассинхроны.
Нужны периодические проверки:

* сравнение заказов в `orders` и `billing`;
* сверка Postgres и поискового индекса;
* отчёты о расхождениях + автоматические/ручные процедуры починки.

Это не красиво, но это реальная взрослая практика.

### 7.2. Read-модели и проекции вместо «одной большой таблицы»

Проекции и CQRS сами по себе не уменьшают eventual consistency.
Их задача другая: **жёстко отделить “ядро истины” от “удобных, но отстающих представлений под чтение”**.

Типичный паттерн:

* есть write-модель (ledger(реестр) заказов/платежей) с максимально строгими инвариантами;
* из неё через события/CDC строятся read-модели: витрины, отчёты, поиск, агрегации;
* эти read-модели по определению eventual — и это нормально, потому что бизнес-логика и инварианты опираются на write-сторону.

Плюс в том, что рассинхрон больше не “всеобщий”, а **заперт в отдельном слое**:
команда чётко знает, что:
* в write-модели нельзя жить на eventual;
* в проекциях небольшой лаг допустим и ожидаем.

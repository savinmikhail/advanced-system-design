### Часть 4.1. Tail Latency: откуда берутся хвосты и как с ними жить

### Цель видео

* Показать практические источники хвостов: железо, сеть, софт.
* Разобрать, как ретраи превращают один медленный запрос в retry storm.
* Дать набор техник:
    * уменьшение хвоста;
    * маскировка/обход хвоста.
* Показать, какие метрики и алерты нужны, чтобы всё это мониторить.

---

### План видео (high level)

1. Источники хвостов: железо, сеть, софт (только практические кейсы).
2. Как ретраи раздувают проблему: “всё ретраит” → retry storm.
3. Техники уменьшения хвоста:
    * hedged requests;
    * tied requests;
    * request coalescing;
    * latency-aware load balancing;
    * background jittering;
    * частично: backpressure / ограничение конкуренции.
4. Техники маскировки/обхода хвоста:
    * deadlines и deadline propagation;
    * circuit breaker;
    * graceful degradation (fallback).
5. Метрики и наблюдаемость:
    * перцентили, rate of retries, длина очередей;
    * distributed tracing;
    * алертинг по хвосту.

---

### Сценарий

#### 2. Откуда берутся хвосты: практический разбор (2:00–8:00)

Разобьём причины на три группы:

**1) Аппаратные**

* Диски: медленный диск/подсистема хранения, внезапный fsync, полуподохший диск в RAID.
* CPU: внезапный thermal throttling, oversubscription, когда на одном узле слишком много процессов.
* NUMA / кеши процессора: данные “не там”, дополнительные задержки.

**2) Сеть**

* Потери пакетов → TCP делает ретрансляции и уходит в backoff.
* Неудачный роутинг / перегруженный линк.
* DNS-запросы, которые иногда отвечают за миллисекунды, а иногда за сотни миллисекунд.
* Congestion: вы всё сделали правильно, но соседний сервис залил сеть.

**3) Софт**

* GC-паузы (в JVM-мираx, но не только).
* Locks и мьютексы: на горячем ресурсе возникает очередь.
* Hot key в Redis: один ключ бьют сотни запросов.
* Медленные запросы в БД: плохой план выполнения, не тот индекс.
* Thundering herd: куча запросов одновременно просыпается и лезет в один и тот же ресурс / кэш / очередь.

Важно: **для хвоста не нужно, чтобы что-то было сломано**.
Достаточно маленькой вероятности “плохого сценария”, которая при большом трафике проявляется постоянно.

Если посмотреть на этот список причин, видно одну неприятную штуку:
на уровне кода у нас абстракция “сделать HTTP-запрос / сходить в БД / положить в очередь”.
Но как только мы смотрим на tail latency, абстракция начинает протекать:
видно TCP-ретрансляции, задержки DNS, GC-паузы, lock-и, медленные диски.

Это классический пример закона протекающих абстракций:
удобная обёртка (HTTP-клиент, ORM, драйвер БД) рано или поздно перестаёт скрывать реальность.
В хвосте производительности всегда видны реальные детали железа, сети и софта.

В одном из моих проектов сервера стояли в московском ЦОДе. Как-то летом выдался особенно жаркий день, охлаждение не вывезло, часть стоек вообще начали отключать в попытках охладить.
С нашей стороны — никакого релиза, ни строчки кода не поменяли.

Но по факту: сайт еле шевелится, p99 по ключевым эндпоинтам улетел в секунды, часть запросов падала со всякими 502 504 ошибками

Это хороший пример аппаратного хвоста: проблема не в алгоритмах и не в “плохом PHP”, а в том, что железо физически перестало работать в нормальном режиме.

---

#### 3. Как ретраи превращают один хвост в retry storm (8:00–12:00)

Теперь сценарий:

1. Один запрос стал медленным: сеть потеряла пакет, БД подумала дольше обычного.
2. Клиент (мобильное приложение) не дождался и сделал retry.
3. Между клиентом и сервисом стоит API-гейтвей или ingress, который тоже умеет ретраить.
4. Сам сервис на PHP при обращении к следующему сервису тоже делает retry по таймауту.

В итоге:

* один “хвостовой” запрос порождает **несколько параллельных запросов**,
* нагрузка на и так притормозивший узел растёт,
* всё это превращается в **RETRY STORM** — лавину ретраев.

Характерный симптом:

* p50 почти не меняется,
* **p99 улетает в космос**,
* а нагрузка по CPU/сети на некоторых нодах начинает резко прыгать.

Идея: ретраи нужны, но **не бесплатно**. Их надо ограничивать, осмысленно распределять и связывать по времени.

---

#### 4. Два класса техник: уменьшить хвост vs спрятать хвост (12:00–14:00)

Все приёмы, которые мы будем обсуждать, делятся на два типа:

1. **Уменьшение хвоста**
   Сделать так, чтобы медленных запросов **становилось меньше**.

2. **Маскировка/обход хвоста**
   Сделать так, чтобы редкие медленные запросы **не ломали всю систему** и пользователь видел более стабильное поведение.

Чаще всего в продакшене нужны обе группы.

---

#### 5. Уменьшение хвоста (14:00–23:00)

**Hedged Requests**

Идея:

* мы отправляем запрос **на несколько узлов сразу**;
* берём первый успешный ответ;
* остальные отменяем.

Применение:

* read-heavy сценарии;
* когда важнее стабильная задержка, чем минимальные ресурсы.

Минусы:

* увеличивает нагрузку (мы делаем лишние запросы);
* требует аккуратной настройки, чтобы не устроить себе DoS.

**Tied Requests (связанные запросы)**

Проблема с простыми hedged:

* если сделать вторую попытку слишком поздно, пользователь всё равно долго ждёт;
* если слишком рано — тратим ресурсы.

Tied requests:

* привязывают второй запрос к “состоянию” первого;
* используют задержку, зависящую, например, от текущей очереди или наблюдаемой latency.

Смысл: **не ждать лишнее время перед второй попыткой**, но и не устраивать штурм.

**Request Coalescing**

Если кэш прогрет — запросы быстрые.
Но если кэш промахнулся, 100 одинаковых запросов могут одновременно пойти в БД.

Request coalescing:

* узнаёт, что “у нас уже есть один запрос за этим ключом”;
* новые запросы **подвешивает** и ждёт результат первого;
* результат раздаёт всем.

Итог:

* уменьшаем load на нижние уровни (БД, внешние API);
* хвосты становятся короче, меньше шанс устроить себе шторма.

**Latency-aware Load Balancing**

Обычный round-robin считает, что все ноды одинаковы.
На практике:

* часть нод может быть подогружена;
* часть попала на более медленный диск/соседей.

Latency-aware LB:

* смотрит на **историческую задержку** по нодам;
* реже шлёт запросы тем, у кого latency выше;
* даёт “отдохнуть” перегруженным или деградировавшим нодам.

Это напрямую уменьшает хвост:
мы уменьшаем шанс попасть на самую медленную ноду.

**Background jittering**

Thundering herd:

* крон-задачи запускаются по расписанию “ровно в минуту”;
* воркеры просыпаются все разом;
* массово бьют один ресурс.

Jitter:

* добавляем случайную компоненту в расписания и ретраи;
* вместо 1000 задач в 00:00 получаем 1000 задач, размазанных по минуте.

Хвосты становятся меньше, пики — сглаживаются.

**Частично: Backpressure / ограничение конкуренции**

Полноценный backpressure мы разберём в throughput-части, но в контексте хвоста важная мысль:

> Лучше **быстро отказать** части запросов, чем медленно убить вообще всё.

Ограничения:

* max concurrency на один ресурс;
* лимиты на количество параллельных запросов в БД;
* HTTP-ответы типа 429 вместо того, чтобы держать очередь до бесконечности.

---

#### 6. Маскировка/обход хвоста (23:00–30:00)

**Deadline Propagation**

Вместо “таймаутов по месту” — общая идея:

* запрос приходит с дедлайном: “у меня есть 700 ms на весь путь”.
* каждый сервис умеет:

    * посмотреть на оставшееся время;
    * либо успеть ответить;
    * либо честно сказать “не успею”.

Дедлайн нужно **проталкивать вниз по цепочке** — в контекст, в HTTP-заголовки, в gRPC-метаданные.

Плюсы:

* хвостовые запросы **не зависают бесконечно**;
* система тратит меньше ресурсов на заведомо проигрышные попытки.

**Circuit Breaker**

Механизм:

* следим за процентом ошибок / таймаутов на внешнем ресурсе;
* если он превышает порог — **размыкаем цепь**:

    * новые запросы сразу падают с ошибкой (или fallback);
    * система не долбится в мёртвый ресурс.

Пока breaker открыт:

* периодически делаем “пробные” запросы;
* если стало лучше — закрываем и снова даём трафик.

В контексте хвоста:

* вместо того, чтобы каждый запрос висел до таймаута,
  мы **быстро** отдаём контролируемую ошибку или деградированный ответ.

**Graceful Degradation / Fallback**

Иногда сделать быстро “хоть что-то” лучше, чем долго — “идеально”.

Примеры:

* не удаётся получить свежие рекомендации — отдают дефолтный список бестселлеров;
* не могут посчитать цену с учётом всех скидок и купонов — показывают приблизительную цену или сообщают “утончили стоимость чуть позже”.

В плане хвоста:

* пользователю не приходится ждать до p99–p999;
* система не держит на себе висящие до таймаута запросы.

---

#### 7. Метрики и наблюдаемость (30:00–35:00)

Чтобы всё это работало, нужны метрики:

* **Перцентили по latency**: p50, p95, p99, p999.
* **Rate of retries**:

    * сколько процентов запросов были ретраями;
    * какие сервисы ретраят чаще всего.
* **Длина очередей**:

    * запросов в очереди перед воркерами;
    * размер connection pool’ов.

Distributed tracing:

* помогает увидеть, **какое именно звено в цепочке даёт хвост**;
* позволяет отличить “хвост в БД” от “хвоста в очереди” или “хвоста в сети”.

Alerting:

* алерты по p99, а не по среднему;
* алерты по резким всплескам ретраев;
* алерты по росту длины очередей.

Финальная мысль:

> Tail latency — это не баг, а естественное свойство сложных систем.
> Задача инженера — не сделать хвост нулевым (это невозможно),
> а **держать его под контролем** и не давать ему ломать общий опыт пользователя.

Следом мы перейдём к второй части:
почему добавление серверов не решает проблему throughput и откуда берутся потолки масштабирования.

---

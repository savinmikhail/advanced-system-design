### Часть 4.1. Tail Latency: откуда берутся хвосты и как с ними жить

### Цель видео

* Показать практические источники хвостов: железо, сеть, софт.
* Разобрать, как ретраи превращают один медленный запрос в retry storm.
* Дать набор техник:
    * уменьшение хвоста;
    * маскировка/обход хвоста.
* Показать, какие метрики и алерты нужны, чтобы всё это мониторить.

---

### План видео (high level)

1. Источники хвостов: железо, сеть, софт (только практические кейсы).
2. Как ретраи раздувают проблему: “всё ретраит” → retry storm.
3. Техники уменьшения хвоста:
    * hedged requests;
    * tied requests;
    * request coalescing;
    * latency-aware load balancing;
    * background jittering;
    * частично: backpressure / ограничение конкуренции.
4. Техники маскировки/обхода хвоста:
    * deadlines и deadline propagation;
    * circuit breaker;
    * graceful degradation (fallback).
5. Метрики и наблюдаемость:
    * перцентили, rate of retries, длина очередей;
    * distributed tracing;
    * алертинг по хвосту.

---

### Сценарий

#### 2. Откуда берутся хвосты: практический разбор (2:00–8:00)

Разобьём причины на три группы:

**1) Аппаратные**

* Диски: медленный диск/подсистема хранения, внезапный fsync, полуподохший диск в RAID.
* CPU: внезапный thermal throttling, oversubscription, когда на одном узле слишком много процессов.
* NUMA / кеши процессора: данные “не там”, дополнительные задержки.

**2) Сеть**

* Потери пакетов → TCP делает ретрансляции и уходит в backoff.
* Неудачный роутинг / перегруженный линк.
* DNS-запросы, которые иногда отвечают за миллисекунды, а иногда за сотни миллисекунд.
* Congestion: вы всё сделали правильно, но соседний сервис залил сеть.

**3) Софт**

* GC-паузы (в JVM-мираx, но не только).
* Locks и мьютексы: на горячем ресурсе возникает очередь.
* Hot key в Redis: один ключ бьют сотни запросов.
* Медленные запросы в БД: плохой план выполнения, не тот индекс.
* Thundering herd: куча запросов одновременно просыпается и лезет в один и тот же ресурс / кэш / очередь.

Важно: **для хвоста не нужно, чтобы что-то было сломано**.
Достаточно маленькой вероятности “плохого сценария”, которая при большом трафике проявляется постоянно.

Если посмотреть на этот список причин, видно одну неприятную штуку:
на уровне кода у нас абстракция “сделать HTTP-запрос / сходить в БД / положить в очередь”.
Но как только мы смотрим на tail latency, абстракция начинает протекать:
видно TCP-ретрансляции, задержки DNS, GC-паузы, lock-и, медленные диски.

Это классический пример закона протекающих абстракций:
удобная обёртка (HTTP-клиент, ORM, драйвер БД) рано или поздно перестаёт скрывать реальность.
В хвосте производительности всегда видны реальные детали железа, сети и софта.

В одном из моих проектов сервера стояли в московском ЦОДе. Как-то летом выдался особенно жаркий день, охлаждение не вывезло, часть стоек вообще начали отключать в попытках охладить.
С нашей стороны — никакого релиза, ни строчки кода не поменяли.

Но по факту: сайт еле шевелится, p99 по ключевым эндпоинтам улетел в секунды, часть запросов падала со всякими 502 504 ошибками

Это хороший пример аппаратного хвоста: проблема не в алгоритмах и не в “плохом PHP”, а в том, что железо физически перестало работать в нормальном режиме.

---

Важно заметить: всё, что мы обсуждали до этого момента — это **одиночные хвосты**.
Где-то перегрелся ЦОД, где-то БД выбрала плохой план, где-то DNS внезапно ответил не за 5 ms, а за 500.

Сам по себе такой хвост неприятен, но не смертелен: один пользователь подождал, пожаловался и ушёл.

Настоящая жесть начинается, когда система **пытается “умно” на него отреагировать**:

* клиентские приложения настроены на ретраи,
* API-шлюзы и ingress’ы тоже что-то ретраят “для надёжности”,
* внутри наших сервисов мы ещё поверх этого делаем свои попытки повторить запрос.

И вот тут один единственный медленный запрос начинает **размножаться**.
Одиночный хвост превращается в лавину запросов, которая добивает и без того уставший компонент.

Давай разберём этот эффект по шагам.

---

#### 3. Как ретраи превращают один хвост в retry storm (8:00–12:00)

Теперь сценарий:

1. Один запрос стал медленным: сеть потеряла пакет, БД подумала дольше обычного.
2. Клиент (мобильное приложение) не дождался и сделал retry.
3. Между клиентом и сервисом стоит API-гейтвей или ingress, который тоже умеет ретраить.
4. Сам сервис на PHP при обращении к следующему сервису тоже делает retry по таймауту.

В итоге:

* один “хвостовой” запрос порождает **несколько параллельных запросов**,
* нагрузка на и так притормозивший узел растёт,
* всё это превращается в **RETRY STORM** — лавину ретраев.

Характерный симптом:

* p50 почти не меняется,
* **p99 улетает в космос**,
* а нагрузка по CPU/сети на некоторых нодах начинает резко прыгать.

Ретраи нужны, но не бесплатно.
Их надо:

* **ограничивать по количеству** — и на каждом слое, и в сумме по цепочке;
* **договориться, кто именно ретраит** — клиент, гейтвей или сервис, а не все сразу;
* **привязать ко времени** — делать экспоненциальный backoff с джиттером и уважать общий дедлайн запроса, чтобы не долбить мёртвый сервис до бесконечности.

---

#### 4. Два класса техник: уменьшить хвост vs спрятать хвост (12:00–14:00)

Итак, у нас есть факт: хвосты неизбежны, а ретраи при неправильной настройке только усиливают боль. Выкинуть и хвосты, и ретраи нельзя — это часть жизни распределённой системы.

Поэтому давай разложим инструменты по полочкам. Всё, что мы делаем с tail latency, обычно попадает в одну из двух корзин:

1. **Уменьшение хвоста**
   Сделать так, чтобы медленных запросов **становилось меньше**.

2. **Маскировка/обход хвоста**
   Сделать так, чтобы редкие медленные запросы **не ломали всю систему** и пользователь видел более стабильное поведение.

Чаще всего в продакшене нужны обе группы.

---

#### 5. Уменьшение хвоста (14:00–23:00)

##### **Hedged Requests**

Идея hedged-запросов в том, чтобы не ждать бесконечно “вдруг повезёт”, а **дать себе второй шанс на другой ноде**, если первый запрос подозрительно затянулся.

Рабочая схема (та, что используют Google и не только):

1. Сначала **меряем латентность** и строим распределение.
2. Выбираем порог, например **p95**.
3. Когда приходит запрос:

    * шлём его на обычную реплику;
    * если он **не успел завершиться за p95** — отправляем **вторую копию** на другую реплику;
    * берём **первый успешный ответ**, второй считаем лишним и стараемся отменить.
4. Число параллельных копий ограничиваем: обычно **2**, иногда 3, но не “по всем серверам”.

Это важно:

* Hedged имеет смысл в первую очередь для **идемпотентных операций** — чтений и безопасных повторяемых обновлений.
  Для “создать заказ” или “списать деньги” нужен вообще другой уровень защиты от дублей.
* Мы платим **дополнительными запросами** за более стабильный p95/p99.
  Если всё настроено аккуратно, прирост нагрузки получается небольшой, а хвост заметно подрезается.

Минусы:

* увеличивает нагрузку (мы сознательно иногда делаем два запроса вместо одного);
* требует калибровки порога (если порог слишком маленький — будем спамить вторыми копиями почти всегда; если слишком большой — толку не будет);
* требует аккуратной интеграции с ретраями, чтобы не получилось: “сначала мы хеджим запросы, а потом ещё и три раза ретраим поверх”.

##### **Tied Requests (связанные запросы)**

Tied-запросы — это эволюция hedged.
Мы всё ещё даём системе **несколько кандидатов** на выполнение одной операции, но стараемся сделать так, чтобы **тяжёлую работу реально делал только один**.

Идея по шагам:

1. Клиент отправляет запрос на первую ноду.
2. Через **очень маленькую задержку** (порядка одного-двух сетевых RTT, условные 5–10 ms) отправляет копию на вторую.
3. Ноды координируются через общий быстрый стор/lease (Redis/etcd/метаданные в storage):

    * первая, взяв задачу, отмечает `jobId` как “в работе”;
    * вторая, стартуя, смотрит в стор: если задача уже помечена, она **не запускает тяжёлую работу**, а либо сразу отказывается, либо ждёт результат первой.

То есть мы всё ещё хеджим запрос, но:

* вторая попытка не дублирует вычисление, если первая уже что-то делает;
* задержка маленькая: нам важно только успеть записать “я взял задачу”, а не досидеть до p95.

Использовать это имеет смысл:

* для относительно тяжёлых, но **идемпотентных** операций (чтобы дубликаты не ломали данные);
* там, где простой hedged слишком расточителен — много CPU/IO тратится на дублирование одной и той же работы.

По сравнению с обычными hedged:

* **плюс** — меньше лишней работы, меньше риск добить уставший ресурс;
* **минус** — нужна координация (общий стор, lease, TTL, обработка сбоев), то есть реализация заметно сложнее.

##### **Request Coalescing**

Если кэш прогрет — запросы быстрые.
Но если кэш промахнулся, 100 одинаковых запросов могут одновременно пойти в БД.

Request coalescing:

* узнаёт, что “у нас уже есть один запрос за этим ключом”;
* новые запросы **подвешивает** и ждёт результат первого;
* результат раздаёт всем.

Итог:

* уменьшаем load на нижние уровни (БД, внешние API);
* хвосты становятся короче, меньше шанс устроить себе шторма.

##### **Latency-aware Load Balancing**

Обычный round-robin считает, что все ноды одинаковы.
На практике:

* часть нод может быть подогружена;
* часть попала на более медленный диск/соседей.

Latency-aware LB:

* смотрит на **историческую задержку** по нодам;
* реже шлёт запросы тем, у кого latency выше;
* даёт “отдохнуть” перегруженным или деградировавшим нодам.

Это напрямую уменьшает хвост:
мы уменьшаем шанс попасть на самую медленную ноду.

##### **Background jittering**

Thundering herd:

* крон-задачи запускаются по расписанию “ровно в минуту”;
* воркеры просыпаются все разом;
* массово бьют один ресурс.

Jitter:

* добавляем случайную компоненту в расписания и ретраи;
* вместо 1000 задач в 00:00 получаем 1000 задач, размазанных по минуте.

Хвосты становятся меньше, пики — сглаживаются.

**Частично: Backpressure / ограничение конкуренции**

Полноценный backpressure мы разберём в throughput-части, но в контексте хвоста важная мысль:

> Лучше **быстро отказать** части запросов, чем медленно убить вообще всё.

Ограничения:

* max concurrency на один ресурс;
* лимиты на количество параллельных запросов в БД;
* HTTP-ответы типа 429 вместо того, чтобы держать очередь до бесконечности.

---

#### 6. Маскировка/обход хвоста (23:00–30:00)

##### **Deadline Propagation**

Вместо “таймаутов по месту” — общая идея:

* запрос приходит с дедлайном: “у меня есть 700 ms на весь путь”.
* каждый сервис умеет:

    * посмотреть на оставшееся время;
    * либо успеть ответить;
    * либо честно сказать “не успею”.

Дедлайн нужно **проталкивать вниз по цепочке** — в контекст, в HTTP-заголовки, в gRPC-метаданные.

Плюсы:

* хвостовые запросы **не зависают бесконечно**;
* система тратит меньше ресурсов на заведомо проигрышные попытки.

##### **Circuit Breaker**

Механизм:

* следим за процентом ошибок / таймаутов на внешнем ресурсе;
* если он превышает порог — **размыкаем цепь**:

    * новые запросы сразу падают с ошибкой (или fallback);
    * система не долбится в мёртвый ресурс.

Пока breaker открыт:

* периодически делаем “пробные” запросы;
* если стало лучше — закрываем и снова даём трафик.

В контексте хвоста:

* вместо того, чтобы каждый запрос висел до таймаута,
  мы **быстро** отдаём контролируемую ошибку или деградированный ответ.

##### **Graceful Degradation / Fallback**

Иногда сделать быстро “хоть что-то” лучше, чем долго — “идеально”.

Примеры:

* не удаётся получить свежие рекомендации — отдают дефолтный список бестселлеров;
* не могут посчитать цену с учётом всех скидок и купонов — показывают приблизительную цену или сообщают “утончили стоимость чуть позже”.

В плане хвоста:

* пользователю не приходится ждать до p99–p999;
* система не держит на себе висящие до таймаута запросы.

---

#### 7. Метрики и наблюдаемость (30:00–35:00)

Чтобы всё это работало, нужны метрики:

* **Перцентили по latency**: p50, p95, p99, p999.
* **Rate of retries**:

    * сколько процентов запросов были ретраями;
    * какие сервисы ретраят чаще всего.
* **Длина очередей**:

    * запросов в очереди перед воркерами;
    * размер connection pool’ов.

Distributed tracing:

* помогает увидеть, **какое именно звено в цепочке даёт хвост**;
* позволяет отличить “хвост в БД” от “хвоста в очереди” или “хвоста в сети”.

Alerting:

* алерты по p99, а не по среднему;
* алерты по резким всплескам ретраев;
* алерты по росту длины очередей.

Финальная мысль:

> Tail latency — это не баг, а естественное свойство сложных систем.
> Задача инженера — не сделать хвост нулевым (это невозможно),
> а **держать его под контролем** и не давать ему ломать общий опыт пользователя.

Следом мы перейдём к второй части:
почему добавление серверов не решает проблему throughput и откуда берутся потолки масштабирования.

---

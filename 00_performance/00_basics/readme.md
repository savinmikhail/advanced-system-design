## Часть 1. Производительность
### 1.1 Базовая механика (latency, throughput, хвосты, Little)

### 1.1.0 Зачем вообще говорить про производительность

> Представьте, что вы открыли приложение, нажали на кнопку — и оно думает.  
> Один раз — ладно.  
> Второй — раздражает.  
> На третий вы закрываете приложение и идёте к конкуренту.

Разработчики идут смотреть метрики, и видят что среднее время запроса равно 200 ms, всё отлично

Но пользователям безразлична наша средняя по больнице. 
ользователю важно его конкретное действие — уложилось оно в комфортные 200 ms или застряло в редком хвосте на 3 секунды.

Таким образом медленно работающие приложение приводит нас к потере клиентов, а значит и денег, поэтому такую ситуацию мы игнорировать не можем

Чтобы понять что с этим можно сделать, сначала поговорим о базовой терминологии.

### 1.1.1 Термины

У нас по сути три величины:

- Latency — сколько времени один запрос живёт внутри системы:
  от момента, когда он туда зашёл, до момента, когда вышел (например, 200 ms или 2 s).

- Throughput — пропускная способность системы:
  сколько запросов в секунду мы способны стабильно завершать (например, 1000 RPS).

- Concurrency — сколько запросов одновременно находятся внутри:
  кто-то уже обрабатывается, кто-то стоит в очереди — всё это “висящие” запросы.

Если представить это как магазин:

- throughput — сколько людей в минуту пробивают на кассе;
- latency — сколько один человек проводит “от входа до выхода”;
- concurrency — сколько людей сейчас внутри (в зале + в очереди).

### 1.1.2 закон Литтла

Теория очередей говорит очень простую вещь:

> Latency = Concurrency / Throughput

Например:

- система стабильно держит 1000 RPS;
- в каждый момент времени внутри 3000 запросов.

Тогда:

```text
Latency = Concurrency / Throughput
Latency = 3000 / 1000 = 3 секунды
```

То есть:

- при таких входных данных мы имеем 3 секунды среднего ожидания;
- если concurrency вырастет до 6000 при том же throughput → latency будет уже 6 секунд.

#### Где тут проблема

Теперь добавим поток входящих запросов:

- входящий поток — 1500 RPS;
- система успевает стабильно обрабатывать только 1000 RPS.

Разница 500 RPS превращается в рост очереди:

- каждую секунду внутри становится на 500 “висящих” запросов больше;
- concurrency растёт, а через формулу Latency = Concurrency / Throughput
  автоматически растёт и latency.

Если так работать достаточно долго, получаем:

- очередь растёт без верхней границы;
- latency стремится к бесконечности;
- пользователи видят “всё умерло”, хотя код и железо формально живы.

Таким образом, если поток входящих запросов дольше, чем несколько секунд превышает реальный throughput —
очередь будет раздуваться, а latency будет улетать, плюс система может начать деградировать под такой нагрузкой и все станет еще хуже

#### Зачем нам это в реальной жизни

- Смотришь на графики:
  “внутри системы сейчас 10 000 запросов, throughput 2000 RPS” → по формуле видно, что средняя latency уже ≈ 5 секунд,
  даже если каждый отдельный обработчик сам по себе быстрый.

- Понимаешь, что:

  - пока throughput не вырастет
    (горизонтальное масштабирование, оптимизация тяжёлых участков),
  - или пока ты не ограничишь входящий поток
    (rate limiting, очереди, backpressure),

  любые шаманства с таймаутами и ретраями — косметика.
  Очередь всё равно будет копиться чисто по Литтлу.

- На собесах это как раз то, чего ждут:
  не “давайте добавим серверов”, а “при таком входящем потоке и таком throughput’е очередь будет расти,
  значит latency закономерно поедет вверх, пока мы не изменим баланс между arrival rate и обслуживанием”.

### 1.1.3 Почему среднее время ответа — плохой ориентир

Допустим, у нас есть 100 запросов:

- 90 обрабатываются за 100 ms;
- 10 — за 5 секунд.

Среднее время ответа:

```text
(90 - 0.1 + 10 - 5) / 100
= (9 + 50) / 100
= 0.59 секунды
```

“В среднем” у вас 590 ms.
На отчёте выглядит терпимо.

По факту:

- 90 человек получают быстрый ответ и думают “нормально работает”;
- 10 человек сидят по 5 секунд и считают ваш сервис мусором.

Бизнесу и пользователям важны не абстрактные 590 ms,
а:

- какая доля людей попадает в плохой опыт;
- насколько плох этот опыт по времени.

Для этого вводят перцентили.

### 1.1.4 Перцентили и хвост (tail latency)

Перцентиль — это “время, быстрее которого обслуживается X% запросов”.

Примеры:

- p50 — половина запросов быстрее этого времени (по сути медиана).
- p95 — 95% запросов быстрее этого времени.
- p99 — 99% запросов быстрее.

Представим что для нашего прода метрики следующие:

- p50 = 150 ms,
- p95 = 400 ms,
- p99 = 2 s.

Что мы из этого читаем:

- половина запросов улетает за 150 ms — всё приятно;
- ещё 45% укладываются до 400 ms — уже не вау, но терпимо;
- последний 1% сидит до 2 секунд — и именно эти ребята потом пишут в отзывы “ничего не работает”.

Хвост распределения — это как раз этот небольшой процент очень медленных запросов.
Tail latency — это любые метрики, которые смотрят именно на хвост: p95, p99, p999.

На собесах и в реальной жизни разговор про “производительность” почти всегда должен быть в терминах p95/p99, а не “среднее у нас норм”.

### 1.1.5 Почему в распределённых системах хвост удлиняется

В монолите один HTTP-запрос часто выглядит просто:

- веб-сервер,
- один поход в базу,
- немного логики,
- ответ.

В распределённой системе цепочка превращается в:

- API-шлюз;
- сервис A;
- сервис B;
- очередь;
- сервис C;
- один-два похода в БД;
- кэш;
- иногда ещё внешнее API.

Теперь представим, что у каждого звена:

- p50 = 50 ms,
- p99 = 200 ms.

Типичный запрос, который проходит по 8–10 звеньям, в среднем всё равно будет быстрым.
Но вероятность, что хотя бы одно звено из цепочки попадёт в свой p99, уже совсем другая.

Грубо:

- одно звено иногда тормозит — ну ладно;
- десяток звеньев с ретраями и сетевыми задержками
  → ваш общий p99 легко вылезает в секунды, даже если отдельно каждый сервис “нормальный”.

Это и есть причина, почему в распределённых системах бороться приходится не только за “быстрый средний ответ”,
но и за укрощение хвоста: ограничивать fan-out, убирать лишние ретраи, ставить таймауты, делать кеширование и деградацию.

Дальше в главе мы как раз и будем разбирать:

- как уменьшать этот хвост и не дать ему убить систему;
- почему простой совет “давайте добавим серверов” не всегда спасает throughput и где нас упирают фундаментальные ограничения.

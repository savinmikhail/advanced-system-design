### Часть 4.0. Базовая механика: latency, throughput, Little, хвосты

#### 1. Хук (0:00–1:30)


> Представьте, что вы открыли приложение, нажали на кнопку, и… оно думает.
> Один раз — ладно.
> Второй раз — начинает раздражать.
> На третьем вы просто закрываете приложение и идёте к конкуренту.

Разработчики при этом очень любят говорить:
“Средняя задержка у нас 200 ms, всё отлично”.

Проблема в том, что пользователю на среднее плевать.
Пользователю важен **его конкретный запрос**, и попал он в “нормальные” 200 ms или в редкий хвост на 3 секунды.

Отсюда связка: **frustration → деньги**.
Задержка → раздражение → брошенные корзины → меньше выручка.

Чтобы этим управлять, нам нужно договориться о двух базовых величинах: latency и throughput.

---

#### 2. Что такое Latency и Throughput (1:30–4:00)

Первое понятие — **latency**.
Это **время ответа**: сколько проходит от входа запроса в систему до отправки ответа пользователю.

Примеры:

* HTTP-запрос: от получения пакета до отправки HTTP-ответа.
* Запрос к базе: от отправки SQL до получения результата.
* Сообщение в очереди: от попадания в очередь до обработки.

Второе понятие — **throughput**.
Это **скорость обработки запросов**, обычно в RPS (requests per second), сообщений/сек, операций/сек.

* Веб-сервер: 5000 RPS.
* Очередь: 20 000 сообщений/сек.
* БД: 2000 транзакций/сек.

Интуитивно:
latency — “сколько ждёт один человек”,
throughput — “сколько людей мы обслуживаем в единицу времени”.

---

#### 3. Связь между ними: закон Литтла (4:00–7:00)

Есть очень простой, но мощный закон теории очередей: **закон Литтла**.

В удобной для нас форме:

> **Concurrency = Throughput × Latency**

Отсюда:

> **Latency = Concurrency / Throughput**

Где:

* **Concurrency** — сколько запросов “висят” в системе одновременно.
* **Throughput** — сколько запросов в секунду мы реально обрабатываем.
* **Latency** — среднее время, которое запрос проводит в системе.

Пример:

* у нас в системе одновременно висит **3000** запросов;
* реальный throughput — **1000 RPS**.

Тогда:

```text
Latency = Concurrency / Throughput
Latency = 3000 / 1000 = 3 секунды
```

То есть:

* при 1000 RPS 3000 параллельных запросов — это 3 секунды ожидания;
* если concurrency вырастет до 6000 при том же throughput, latency улетит до 6 секунд.

Мораль:

* Как только **очередь растёт**, **время ответа растёт**.
* Если вы увеличили нагрузку, а throughput не вырос — latency полезет вверх просто по математике, даже без “багов”.

---

#### 4. Почему среднее время ответа — фиговый показатель (7:00–10:00)

Допустим, у нас есть 100 запросов:

* 90 штук обрабатываются за 100 ms,
* 10 штук — за 5 секунд.

Среднее время ответа:

```text
(90 * 0.1 + 10 * 5) / 100
= (9 + 50) / 100
= 0.59 секунды
```

С точки зрения “среднего” всё как будто норм: **590 ms**.

Но реальность другая:

* 90 людей получают быстрый ответ и думают “вау, как шустро”.
* 10 человек сидят по **5 секунд** и ненавидят ваш продукт.

Бизнесу важны не абстрактные средние, а:

* сколько людей попадают в плохой опыт;
* насколько плох этот опыт.

Для этого вводят **перцентили**.

---

#### 5. Перцентили и “хвост” (10:00–14:00)

Перцентиль — это “время, быстрее которого обслуживается X% запросов”.

Примеры:

* **p50** — 50% запросов уложились быстрее этого времени. Это по сути медиана.
* **p95** — 95% запросов быстрее этого времени.
* **p99** — 99% запросов быстрее.

Типичная картинка:

* p50 = 150 ms,
* p95 = 400 ms,
* p99 = 2 s.

Что она говорит:

* половина запросов улетают за 150 ms — всё классно;
* 5% запросов — до 400 ms — чуть хуже, но терпимо;
* **1% запросов попадает в 2 секунды** — и именно эти люди портят вам репутацию.

Хвост распределения — это как раз эти редкие, но очень медленные запросы.

**Tail latency** — это метрика, которая смотрит на хвост: p95, p99, p999.

---

#### 6. Почему в распределённых системах хвост раздувается (14:00–17:00)

В монолитике один HTTP-запрос обычно:

* сходил в БД,
* что-то посчитал,
* вернул ответ.

В распределённой системе:

* API-шлюз,
* сервис А,
* сервис B,
* очередь,
* сервис C,
* ещё раз БД,
* кэш,
* внешнее API.

Если у каждого звена p99 = 200 ms, а ваш запрос трогает 10 звеньев, то:

* “типичный” запрос идёт быстро,
* а **вероятность, что хоть одно звено попадёт в хвост**, растёт.

Совсем грубо: чем больше звеньев и ретраев, тем больше шанс, что конкретный запрос выстрелит в **секунды, а то и минуты**.

далее мы:

* сначала разберёмся, **как уменьшать этот хвост и как не дать ему убить систему**;
* потом — **почему простое “давайте добавим серверов” не спасает throughput**, и где нас ограничивают фундаментальные законы.


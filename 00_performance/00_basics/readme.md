## Часть 1. Производительность: базовая механика (latency, throughput, хвосты, Little)

### 1. Зачем вообще говорить про производительность

> Представьте, что вы открыли приложение, нажали на кнопку — и оно думает.  
> Один раз — ладно.  
> Второй — раздражает.  
> На третий вы закрываете приложение и идёте к конкуренту.

Разработчики в этот момент любят говорить:
“Средняя задержка у нас 200 ms, всё отлично”.

Проблема в том, что пользователю на среднее плевать.  
Пользователю важно **его конкретное действие** — уложилось оно в комфортные 200 ms
или застряло в редком хвосте на 3 секунды.

Отсюда связка: **frustration → деньги**.  
Задержка → раздражение → брошенные корзины → меньше выручка.

Чтобы этим управлять, надо договориться о двух базовых величинах: **latency** и **throughput**.

### 3. термины

У нас по сути три величины:

* **Latency** — **сколько времени один запрос живёт внутри системы**:
  от момента, когда он туда зашёл, до момента, когда вышел
  (например, 200 ms или 2 s).

* **Throughput** — пропускная способность системы:
  сколько запросов в секунду мы **способны** стабильно завершать
  (например, 1000 RPS).

* **Concurrency** — **сколько запросов одновременно находятся внутри**:
  кто-то уже обрабатывается, кто-то стоит в очереди — всё это “висящие” запросы.

Если представить это как магазин:

* throughput — сколько людей в минуту пробивают на кассе;
* latency — сколько один человек проводит “от входа до выхода”;
* concurrency — сколько людей сейчас внутри (в зале + в очереди).

### 4. закон Литтла

Теория очередей говорит очень простую вещь:

> **Latency = Concurrency / Throughput**

Например:

* система стабильно держит **1000 RPS**;
* в каждый момент времени внутри **3000** запросов.

Тогда:

```text
Latency = Concurrency / Throughput
Latency = 3000 / 1000 = 3 секунды
```

То есть:

* при таких входных данных мы имеем **3 секунды среднего ожидания**;
* если concurrency вырастет до 6000 при том же throughput → latency будет уже **6 секунд**.

#### Где тут проблема

Теперь добавим поток входящих запросов:

* входящий поток — **1500 RPS**;
* система успевает стабильно обрабатывать только **1000 RPS**.

Разница **500 RPS** превращается в **рост очереди**:

* каждую секунду внутри становится на 500 “висящих” запросов больше;
* concurrency растёт, а через формулу **Latency = Concurrency / Throughput**
  автоматически растёт и latency.

Если так работать достаточно долго, получаем:

* очередь растёт без верхней границы;
* latency стремится к бесконечности;
* пользователи видят “всё умерло”, хотя код и железо формально живы.

Таким образом, если поток входящих запросов **дольше, чем несколько секунд** превышает реальный throughput —
очередь будет раздуваться, а latency будет улетать, плюс система может начать деградировать под такой нагрузкой и все станет еще хуже

#### Зачем нам это в реальной жизни

* Смотришь на графики:
  “внутри системы сейчас 10 000 запросов, throughput 2000 RPS” → по формуле видно, что **средняя latency уже ≈ 5 секунд**,
  даже если каждый отдельный обработчик сам по себе быстрый.

* Понимаешь, что:

  * пока **throughput не вырастет**
    (горизонтальное масштабирование, оптимизация тяжёлых участков),
  * или пока ты **не ограничишь входящий поток**
    (rate limiting, очереди, backpressure),

  любые шаманства с таймаутами и ретраями — косметика.
  Очередь всё равно будет копиться чисто по Литтлу.

* На собесах это как раз то, чего ждут:
  не “давайте добавим серверов”, а “при таком входящем потоке и таком throughput’е очередь будет расти,
  значит latency закономерно поедет вверх, пока мы не изменим баланс между arrival rate и обслуживанием”.

### 4. Почему среднее время ответа — плохой ориентир

Допустим, у нас есть 100 запросов:

* 90 обрабатываются за 100 ms;
* 10 — за 5 секунд.

Среднее время ответа:

```text
(90 * 0.1 + 10 * 5) / 100
= (9 + 50) / 100
= 0.59 секунды
```

“В среднем” у вас **590 ms**.
На отчёте выглядит терпимо.

По факту:

* 90 человек получают быстрый ответ и думают “нормально работает”;
* 10 человек сидят по **5 секунд** и считают ваш сервис мусором.

Бизнесу и пользователям важны не абстрактные 590 ms,
а:

* **какая доля** людей попадает в плохой опыт;
* **насколько плох** этот опыт по времени.

Для этого вводят **перцентили**.

### 5. Перцентили и хвост (tail latency)

Перцентиль — это “время, быстрее которого обслуживается X% запросов”.

Примеры:

* **p50** — половина запросов быстрее этого времени (по сути медиана).
* **p95** — 95% запросов быстрее этого времени.
* **p99** — 99% запросов быстрее.

Представим что для нашего прода метрики следующие:

* p50 = 150 ms,
* p95 = 400 ms,
* p99 = 2 s.

Что мы из этого читаем:

* половина запросов улетает за 150 ms — всё приятно;
* ещё 45% укладываются до 400 ms — уже не вау, но терпимо;
* **последний 1%** сидит до 2 секунд — и именно эти ребята потом пишут в отзывы “ничего не работает”.

Хвост распределения — это как раз этот небольшой процент очень медленных запросов.
**Tail latency** — это любые метрики, которые смотрят именно на хвост: p95, p99, p999.

На собесах и в реальной жизни разговор про “производительность” почти всегда должен быть в терминах p95/p99, а не “среднее у нас норм”.

### 6. Почему в распределённых системах хвост удлиняется

В монолите один HTTP-запрос часто выглядит просто:

* веб-сервер,
* один поход в базу,
* немного логики,
* ответ.

В распределённой системе цепочка превращается в:

* API-шлюз;
* сервис A;
* сервис B;
* очередь;
* сервис C;
* один-два похода в БД;
* кэш;
* иногда ещё внешнее API.

Теперь представим, что у **каждого** звена:

* p50 = 50 ms,
* p99 = 200 ms.

Типичный запрос, который проходит по 8–10 звеньям, в среднем всё равно будет быстрым.
Но **вероятность**, что хотя бы одно звено из цепочки попадёт в свой p99, уже совсем другая.

Грубо:

* одно звено иногда тормозит — ну ладно;
* десяток звеньев с ретраями и сетевыми задержками
  → ваш общий p99 легко вылезает в секунды, даже если отдельно каждый сервис “нормальный”.

Это и есть причина, почему в распределённых системах бороться приходится не только за “быстрый средний ответ”,
но и за **укрощение хвоста**: ограничивать fan-out, убирать лишние ретраи, ставить таймауты, делать кеширование и деградацию.

Дальше в главе мы как раз и будем разбирать:

* как уменьшать этот хвост и не дать ему убить систему;
* почему простой совет “давайте добавим серверов” не всегда спасает throughput и где нас упирают фундаментальные ограничения.

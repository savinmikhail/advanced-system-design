## Часть 1. Производительность

### 1.1 Базовая механика (latency, throughput, хвосты, Little)

### 1.1.0 Зачем вообще говорить про производительность

> Представьте, что вы открыли приложение, нажали на кнопку — и оно думает.  
> Один раз — ладно.  
> Второй — раздражает.  
> На третий вы закрываете приложение и идёте к конкуренту.

Разработчики идут смотреть метрики, и видят что среднее время запроса равно 200 ms, всё отлично

Но пользователям безразлично наше среднее по больнице.
Его волнует только одно: его конкретный запрос — уложился он в комфортные сотни миллисекунд или провисел пару секунд.

Таким образом медленно работающие приложение приводит нас к потере клиентов, а значит и денег, поэтому такую ситуацию мы игнорировать не можем

Игнорировать это нельзя, особенно когда вы обсуждаете архитектуру на собеседовании или проектируете систему под рост.

Чтобы понять что с этим можно сделать, сначала поговорим о базовой терминологии.

### 1.1.1 Термины

У нас есть три понятия Latency, Throughput и Concurrency:

- Latency — сколько времени один запрос живёт внутри системы: от момента, когда он туда зашёл, 
  до момента, когда вышел (например, 200 ms или 2 s).

- Throughput — пропускная способность системы: сколько запросов в секунду мы способны стабильно завершать (например, 1000 RPS).

- Concurrency — сколько запросов одновременно находятся внутри:
  кто-то уже обрабатывается, кто-то стоит в очереди — всё это “висящие” запросы.

Если представить это как магазин:

- throughput — сколько людей в минуту проходит через все кассы;
- latency — сколько один человек проводит от входа до выхода
- concurrency — сколько людей сейчас в очереди

### 1.1.2 закон Литтла

Очевидно это понятия взаимосвязаны - если перед вами уже есть очередь, то время вашего ожидания будет больше. Равно как и если бы throughput был выше, то очередь бы накапливалась меньше

Эта закономерность описывается законом Литтла:

> Latency = Concurrency / Throughput

~~на экран~~

Например:
- система стабильно держит 1000 RPS;
- в каждый момент времени внутри 3000 запросов.

Тогда:
```text
Latency = Concurrency / Throughput
Latency = 3000 / 1000 = 3 секунды
```

~~на экран~~

То есть при таких входных данных мы имеем 3 секунды среднего ожидания;

#### Где тут проблема

Теперь добавим поток входящих запросов:

- входящий поток — 1500 RPS;
- система успевает стабильно обрабатывать только 1000 RPS.

Разница 500 RPS превращается в рост очереди:

- каждую секунду внутри становится на 500 “висящих” запросов больше;
- concurrency растёт, а через формулу Latency = Concurrency / Throughput
  автоматически растёт и latency.

Если так работать достаточно долго, получаем:

- очередь растёт без верхней границы;
- latency стремится к бесконечности;
- пользователи видят “всё умерло”, хотя код и железо формально живы, но нескоро приступят к выполнению запроса пользователя

Таким образом, если поток входящих запросов дольше, чем несколько секунд превышает реальный throughput —
очередь будет раздуваться, а latency будет сильно увеличиваться, вдобавок система может начать деградировать под такой нагрузкой и все станет еще хуже

#### Зачем нам это знать

На практике это понимание помогает принимать решения, к примеру:

Ты смотришь на метрики и видишь что 
- внутри системы сейчас 10 000 запросов, 
- throughput 2000 RPS
- → по формуле видно, что средняя latency уже ≈ 5 секунд, даже если каждый отдельный обработчик сам по себе быстрый.

Из этих соображений следует что

  - пока throughput не вырастет (горизонтальное масштабирование, оптимизация тяжёлых участков),
  - или пока ты не ограничишь входящий поток (rate limiting, очереди, backpressure),

  Очередь будет только увеличиваться

На собесах как раз так и надо рассуждать, к примеру
не “давайте добавим серверов”, а “при таком входящем потоке запросов и таком throughput’е очередь будет расти,
значит latency закономерно поедет вверх, пока мы не изменим баланс между concurrency и throughput

### 1.1.3 Почему среднее время ответа — нерепрезентативная метрика?

Значит мы можем мерить latency и жить спокойно? на самом деле нет, этого недостаточно

Разберем такой пример:
- у нас есть 100 запросов
- 90 обрабатываются за 100 ms;
- 10 — за 5 секунд.

Среднее время ответа:

```text
(90 - 0.1 + 10 - 5) / 100
= (9 + 50) / 100
= 0.59 секунды
```

“В среднем” у вас 590 ms.
На отчёте выглядит терпимо.

По факту:

- 90 человек получают быстрый ответ и думают “нормально работает”;
- 10 человек сидят по 5 секунд и оставляют негативные отзывы

Бизнесу и пользователям важны не абстрактные 590 ms, а:

- какая доля людей попадает в плохой опыт;
- насколько плох этот опыт по времени.

Для этого вводят перцентили.

### 1.1.4 Перцентили и хвост (tail latency)

Перцентиль — это “время, быстрее которого обслуживается соответствующий процент запросов”.

Например:

- при p50 — половина запросов быстрее этого времени (по сути медиана).
- при p95 — 95% запросов быстрее этого времени.
- при p99 — 99% запросов быстрее.

Представим что для нашего прода метрики следующие:

- p50 = 150 ms,
- p95 = 400 ms,
- p99 = 2 s.

Что это значит?

- половина запросов улетает за 150 ms — прекрасно;
- ещё 45% укладываются до 400 ms — терпимо;
- последний 1% сидит до 2 секунд - те самые расстроенные пользователи из наших примеров выше

Тут возникает новый термин:
Tail latency (Хвост задержки) — это как раз этот небольшой процент очень медленных запросов.

На собесах и в реальной жизни разговор про “производительность” почти всегда должен быть в терминах p95/p99, а не “среднее у нас норм”.

### 1.1.5 Почему в распределённых системах хвост удлиняется

При этом в распределенных системах хвост склонен удлиняться. Почему так происходит?

В монолите запрос на создание заказа может состоять из 5 звеньев:

- веб-сервер,
- один поход в базу,
- один поход в кеш,
- немного логики внутри монолита

В микросервисной же архитектуре в цепочке становиться больше звеньев:

- веб-сервер,
- сервис A;
- сервис B;
- сервис C;
- каждый из них в свою очередь ходит в базу и кеш;

Теперь представим, что у каждого звена:

- p50 = 50 ms,
- p99 = 200 ms.

Большинство запросов будут быстрыми, но вероятность попадания в tail latency растет по теории вероятности с увеличением числа звеньев

В итоге:

- по отдельности все сервисы “нормальные” (p99 ≈ 200 ms),
- а p99 по нашему запросу легко может быть 2 секунды, если мы поймаем хвост на каждом звене

Дальше в главе мы как раз и будем разбирать:

- как уменьшать этот хвост и не дать ему убить систему;
- почему простой совет “давайте добавим серверов/лучше утилизируем процессор” может сделать только хуже

## Блок 1.2 Tail Latency

### 1.2.0. Откуда берутся хвосты

Но откуда берутся хвосты? Они же по определению случаются лишь иногда, значит причина плавающая и вряд ли в коде, верно?

На то есть три группы причин - Аппаратные, Сетевые и Программные

1) Аппаратные, например throttling на диске из-за лимита записи/чтения или thermal throttling на процессоре

2) Сетевые, например Congestion из-за соседнего сервера, изредка долгие DNS-запросы

3) Программные

   - GC-паузы
   - Locks и мьютексы: на горячем ресурсе возникает очередь.
   - Hot key в Redis: один ключ бьют сотни запросов.
   - Медленные запросы в БД: хоп по всем шардам к примеру

Важно: для хвоста не нужно, чтобы что-то было сломано.
Достаточно маленькой вероятности “плохого сценария”, которая при большом трафике проявляется постоянно.

Если посмотреть на этот список причин, видно что это закон протекающих абстракций в действии
на уровне кода у нас абстракция “сделать HTTP-запрос / сходить в БД / положить в очередь”.
Но как только мы смотрим на tail latency, абстракция начинает протекать:
видно TCP-ретрансляции, задержки DNS, GC-паузы, lock-и, медленные диски.

В одном из моих проектов сервера стояли в московском ЦОДе. Как-то летом выдался особенно жаркий день, охлаждение не вывезло, часть стоек вообще начали отключать в попытках охладить.
С нашей стороны не было никакого релиза, но сайт сильно деградировал по показателям, едва ли что-то вообще грузилось часть запросов падала со всякими 502 504 ошибками

Тоже пример аппаратных причин tail latency

Вдобавок на виртуалках или в облаках есть эффект noisy neighbor — когда другой сервис на той же ноде начинает потреблять выделенные ресурсы, ваубивает вам p99

### 1.2.1 Retry storm

Одиночный хвост сам по себе неприятен, но систему не убивает. Гораздо хуже, когда она начинает “умно” на него реагировать.

Типичный паттерн:

1. Один запрос внезапно становится медленным (сеть, диск, БД, GC — неважно).
2. Клиент (мобильное приложение) не дождался — делает retry.
3. API-шлюз / ingress тоже умеет ретраить “для надёжности”.
4. Наш сервис, обращаясь к следующему сервису, сверху добавляет свои retry по таймауту.

В результате один проблемный запрос размножается на несколько параллельных:

- тот же самый горячий endpoint начинает получать *кратно больше* запросов;
- компонент, который и так притормозил, добивается лавиной повторов;
- хвост из единичного эпизода превращается в retry storm.

Характерные симптомы:

- p50 почти не меняется;
- p95–p99 улетают в космос;
- на отдельных нодах скачет нагрузка по CPU/сети, хотя бизнес-трафик не сильно вырос.

Ретраи сами по себе нужны, но их надо держать на поводке:

- ограничивать количество попыток — и на каждом слое, и по цепочке в целом;
- разделить ответственность: кто именно ретраит — клиент, гейтвей или сервис, а не все сразу;
- привязывать к времени: экспоненциальный backoff с джиттером и уважение общего дедлайна запроса, чтобы не долбить мёртвый сервис до победного.

Иначе “надёжность через ретраи” легко превращается в механизм добивания и без того больных компонентов.

### 1.2.2 Два класса техник: уменьшить хвост и спрятать хвост

Однако хвосты никуда не денутся, ретраи тоже — это нормальная жизнь распределённой системы.
Поэтому все приёмы борьбы с tail latency по сути делятся на две группы:

1. **Уменьшить хвост** — сделать так, чтобы медленных запросов реально стало меньше
   (hedged/tied requests, request coalescing, latency-aware балансировка, backpressure и т.п.).

2. **Спрятать/обойти хвост** — сделать так, чтобы редкие медленные запросы не ломали всю систему и опыт пользователя
   (deadlines, circuit breaker, graceful degradation, кеш/преагрегация).

В нормальном проде почти всегда нужны обе группы сразу: и подрезать хвост, и не давать ему тянуть за собой весь сервис.

### 1.2.3 Уменьшение хвоста

#### 1.2.3.1 Hedged Requests

Идея hedged-запросов в том, чтобы не ждать бесконечно “вдруг повезёт”, а дать себе второй шанс на другой ноде, если первый запрос подозрительно затянулся.

Рабочая схема (та, что используют Google и не только):

1. Сначала меряем латентность и строим распределение.
2. Выбираем порог, например p95.
3. Когда приходит запрос:

    - шлём его на обычную реплику;
    - если он не успел завершиться за p95 — отправляем вторую копию на другую реплику;
    - берём первый успешный ответ, второй считаем лишним и стараемся отменить.
4. Число параллельных копий ограничиваем: обычно 2, иногда 3, но не “по всем серверам”.

Это важно:

- Hedged имеет смысл в первую очередь для идемпотентных операций — чтений и безопасных повторяемых обновлений.
  Для “создать заказ” или “списать деньги” нужен вообще другой уровень защиты от дублей.
- Мы платим дополнительными запросами за более стабильный p95/p99.
  Если всё настроено аккуратно, прирост нагрузки получается небольшой, а хвост заметно подрезается.

Минусы:

- увеличивает нагрузку (мы сознательно иногда делаем два запроса вместо одного);
- требует калибровки порога (если порог слишком маленький — будем спамить вторыми копиями почти всегда; если слишком большой — толку не будет);
- требует аккуратной интеграции с ретраями, чтобы не получилось: “сначала мы хеджим запросы, а потом ещё и три раза ретраим поверх”.

#### 1.2.3.2 Tied Requests (связанные запросы)

Tied-запросы — это эволюция hedged.
Мы всё ещё даём системе несколько кандидатов на выполнение одной операции, но стараемся сделать так, чтобы тяжёлую работу реально делал только один.

Идея по шагам:

1. Клиент отправляет запрос на первую ноду.
2. Через очень маленькую задержку (порядка одного-двух сетевых RTT, условные 5–10 ms) отправляет копию на вторую.
3. Ноды координируются через общий быстрый стор/lease (Redis/etcd/метаданные в storage):

    - первая, взяв задачу, отмечает `jobId` как “в работе”;
    - вторая, стартуя, смотрит в стор: если задача уже помечена, она не запускает тяжёлую работу, а либо сразу отказывается, либо ждёт результат первой.

То есть мы всё ещё хеджим запрос, но:

- вторая попытка не дублирует вычисление, если первая уже что-то делает;
- задержка маленькая: нам важно только успеть записать “я взял задачу”, а не досидеть до p95.

Использовать это имеет смысл:

- для относительно тяжёлых, но идемпотентных операций (чтобы дубликаты не ломали данные);
- там, где простой hedged слишком расточителен — много CPU/IO тратится на дублирование одной и той же работы.

По сравнению с обычными hedged:

- плюс — меньше лишней работы, меньше риск добить уставший ресурс;
- минус — нужна координация (общий стор, lease, TTL, обработка сбоев), то есть реализация заметно сложнее.

#### 1.2.3.3 Request Coalescing

Когда кэш промахнулся, сотни одинаковых запросов могут одновременно упасть на один и тот же ресурс (БД, внешний API) и устроить шторм, резко раздув хвост.

Request coalescing делает вместо этого:

- первый запрос реально идёт вниз;
- остальные, которые пришли “с тем же ключом”, мы временно подвешиваем;
- когда первый вернулся – раздаём его результат всем ожидающим.

Мы уменьшаем concurrency на нижнем уровне → меньше нагрузка на БД/внешний сервис → ниже шанс, что p99 превратится в секунды.

#### 1.2.3.4 Latency-aware Load Balancing

Обычный round-robin считает, что все ноды одинаковы.
На практике:

- часть нод может быть подогружена;
- часть попала на более медленный диск/соседей.

Latency-aware LB:

- смотрит на историческую задержку по нодам;
- реже шлёт запросы тем, у кого latency выше;
- даёт “отдохнуть” перегруженным или деградировавшим нодам.

Это напрямую уменьшает хвост:
мы уменьшаем шанс попасть на самую медленную ноду.

#### 1.2.3.5 Background jittering

Thundering herd:

- крон-задачи запускаются по расписанию “ровно в минуту”;
- воркеры просыпаются все разом;
- массово бьют один ресурс.

Jitter:

- добавляем случайную компоненту в расписания и ретраи;
- вместо 1000 задач в 00:00 получаем 1000 задач, размазанных по минуте.

Хвосты становятся меньше, пики — сглаживаются.

#### 1.2.3.6 Backpressure

Полноценный backpressure мы разберём в throughput-части, но в контексте хвоста важная мысль:

> Лучше быстро отказать части запросов, чем медленно убить вообще всё.

Ограничения:

- max concurrency на один ресурс;
- лимиты на количество параллельных запросов в БД;
- HTTP-ответы типа 429 вместо того, чтобы держать очередь до бесконечности.

### 1.2.4 Маскировка/обход хвоста

#### 1.2.4.1 Deadline Propagation

Вместо “таймаутов по месту” — общая идея:

- запрос приходит с дедлайном: “у меня есть 700 ms на весь путь”.
- каждый сервис умеет:

    - посмотреть на оставшееся время;
    - либо успеть ответить;
    - либо честно сказать “не успею”.

Дедлайн нужно проталкивать вниз по цепочке — в контекст, в HTTP-заголовки, в запросы к базе и тп.

Плюсы:

- хвостовые запросы не зависают бесконечно;
- система тратит меньше ресурсов на заведомо проигрышные попытки.

#### 1.2.4.2 Circuit Breaker

Механизм:

- следим за процентом ошибок / таймаутов на внешнем ресурсе;
- если он превышает порог — размыкаем цепь:

    - новые запросы сразу падают с ошибкой (или fallback);
    - система не долбится в мёртвый ресурс.

Пока breaker открыт:

- периодически делаем “пробные” запросы;
- если стало лучше — закрываем и снова даём трафик.

В контексте хвоста:

- вместо того, чтобы каждый запрос висел до таймаута,
  мы быстро отдаём контролируемую ошибку или деградированный ответ.

#### 1.2.4.3 Graceful Degradation / Fallback

Иногда сделать быстро “хоть что-то” лучше, чем долго — “идеально”.

Примеры:

- не удаётся получить свежие рекомендации — отдают дефолтный список бестселлеров;
- не могут посчитать цену с учётом всех скидок и купонов — показывают приблизительную цену или сообщают “утончили стоимость чуть позже”.

В плане хвоста:

- пользователю не приходится ждать до p99–p999;
- система не держит на себе висящие до таймаута запросы.

### 1.2.5 Метрики и наблюдаемость

Чтобы всё это работало, нужны метрики:

- Перцентили по latency: p50, p95, p99, p999.
- Rate of retries:

    - сколько процентов запросов были ретраями;
    - какие сервисы ретраят чаще всего.
- Длина очередей:

    - запросов в очереди перед воркерами;
    - размер connection pool’ов.

Distributed tracing:

- помогает увидеть, какое именно звено в цепочке даёт хвост;
- позволяет отличить “хвост в БД” от “хвоста в очереди” или “хвоста в сети”.

Alerting:

- алерты по p99, а не по среднему;
- алерты по резким всплескам ретраев;
- алерты по росту длины очередей.

Финальная мысль:

> Tail latency — это не баг, а естественное свойство сложных систем.
> Задача инженера — не сделать хвост нулевым (это невозможно),
> а держать его под контролем и не давать ему ломать общий опыт пользователя.

Следом мы перейдём к второй части:
почему добавление серверов не решает проблему throughput и откуда берутся потолки масштабирования.

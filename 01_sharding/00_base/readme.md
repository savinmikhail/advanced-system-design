### 2.0. Базовый ликбез: шардирование и партиционирование

#### 2.0.1. От какой боли вообще стартуем

Типичная картина:

* есть одна большая таблица `events` / `orders` / `logs`;
* она растёт годами;
* в ней уже десятки/сотни миллионов строк;
* индексы раздулись, VACUUM жрёт время, бэкапы занимают ночь;

Плюс операционные штуки:

* таблица приближается к физическим ограничениям:

    * диск под ней забит;
    * на одной файловой системе жить уже стремно;
* хотим:

    * “горячие” данные держать на SSD;
    * “холодные” архивы — на дешёвом HDD;
* иногда просто: “на одном сервере уже страшно держать такую тушу которая может занимать терабайты”.

Вот из этой боли рождаются:

* **партиционирование** — разрезать таблицу на части;
* **шардирование** — разнести эти части по разным машинам/кластерам.

---

#### 2.0.2. Вертикальное и горизонтальное разрезание

Есть два базовых направления, в которых можно резать данные:

1. **Вертикальное разрезание** (vertical partitioning)

   Делим таблицу **по столбцам**.

   Примеры:

* выносим жирные, редко используемые поля (JSON-поля, большие текстовые комментарии) в отдельную таблицу;
* разделяем сущность:

    * `users_core(id, email, password_hash, state)`
    * `users_profile(user_id, name, avatar_url, bio, …)`

Зачем:

* уменьшить “ширину” критичного индекса;
* ускорить типичные выборки, которые не тащат за собой гигантский payload;
* иногда — развести по разным подсистемам (core-данные и всё остальное).

Это **не совсем про распределённые системы**, но важно понимать:
часто архитектурный разговор “про шардирование” начинается с того, что люди даже вертикально таблицу не разрезали.

2. **Горизонтальное разрезание** (horizontal partitioning)

   Делим таблицу **по строкам**.

   Примеры:

* `events` по дате: “декабрь 2025”, “январь 2026”;
* `users` по регионам: “EU”, “US”, “APAC”;
* `orders` по диапазонам ID.

Это как раз тот тип разрезания, который нас интересует в этой главе: из него вырастают и **партиции**, и **шарды**.

---

#### 2.0.3. Партиционирование: одна таблица, много кусков

**Партиционирование (partitioning)** — это когда одна логическая таблица разбита на несколько физических кусков по какому-то признаку.

Например, в Postgres:

```sql
CREATE TABLE events (
    id         bigserial PRIMARY KEY,
    user_id    bigint      NOT NULL,
    created_at timestamptz NOT NULL,
    payload    jsonb       NOT NULL
) PARTITION BY RANGE (created_at);
```

Создадим две партиции:

```sql
CREATE TABLE events_2025_12
    PARTITION OF events
    FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');

CREATE TABLE events_2026_01
    PARTITION OF events
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');
```

Для приложения это всё ещё **одна таблица** `events`:

```sql
SELECT * FROM events
WHERE created_at >= now() - interval '1 day';
```

Postgres сам решает:

* в какие партиции лезть (partition pruning);
* какие партиции можно вообще не трогать.

Зачем это нам по боли:

* свежие партиции можно держать на SSD, старые — на другом tablespace/диске;
* можно **быстро дропать архивы**: `DROP TABLE events_2024_01` — и год данных исчез;
* VACUUM/индексы работают по кускам, а не по монстру на миллиард строк.

Это уже **горизонтальное партиционирование**, но **пока на одном кластере**.

---

#### 2.0.4. Горизонтальное партиционирование по ключу: RANGE, LIST, HASH

Где-то здесь можно перечислить основные способы деления:

1. **RANGE** — по диапазонам.

* по дате (`created_at`),
* по ID (0–1M, 1M–2M).

Хорошо, когда:

* данные естественно упорядочены (временные серии, логи);
* удобно архивировать “старые хвосты”.

Боль:

* если все запросы идут только к “последней” партиции — именно она становится горячей;
* легко получить **hot partition**.

2. **LIST** — по конечному набору значений.

* по `region`: `EU`, `US`, `APAC`;
* по `tenant_id`, если их немного и они стабильны.

3. **HASH** — равномерное разбрасывание по ключу.

   В Postgres:

   ```sql
   CREATE TABLE users (
       id    bigint PRIMARY KEY,
       email text NOT NULL
   ) PARTITION BY HASH (id);

   CREATE TABLE users_p0
       PARTITION OF users
       FOR VALUES WITH (MODULUS 4, REMAINDER 0);

   CREATE TABLE users_p1
       PARTITION OF users
       FOR VALUES WITH (MODULUS 4, REMAINDER 1);
   -- и так далее
   ```

   Под капотом — тот же самый `user_id % N`, который мы потом будем обсуждать в Dynamic Sharding.

   Плюс:

* равномерное распределение нагрузки и объёма (если ключ нормальный);
* нет одной “горячей” партиции.

Минус:

* сложно делать операции “по диапазону”;
* сложнее мигрировать/менять количество партиций.

Все эти штуки — **про то, как горизонтально разрезать таблицу**.
Пока это **partitioning**: всё может жить в одном Postgres-кластере.

---

#### 2.0.5. Когда partitioning превращается в sharding

Теперь уточняем терминологию.

**Шардинг (sharding)** — это когда эти куски **разнесены по разным узлам/кластерам** и нужен **routing**:

* `users_shard_1`: Postgres #1, данные по `user_id % 4 = 0`;
* `users_shard_2`: Postgres #2, `user_id % 4 = 1`;
* `users_shard_3`: Postgres #3, `user_id % 4 = 2`;
* `users_shard_4`: Postgres #4, `user_id % 4 = 3`.

Грубо:

> Партиционирование — это про то, как порезать стол.
> Шардирование — про то, как эти куски развезти по разным складам.

Почему приходится идти до сюда:

* таблица/индексы уже **физически не лезут** на один сервер/в один диск нормально (или очень скоро не влезут);
* мы хотим:

    * нагрузку по запросам **размазать по разным машинам**;
    * для горячих шардов использовать SSD, для холодных — что-то попроще;
    * отделить регионы:

        * пользователи из EU → кластер в Европе,
        * пользователи из US → кластер в США.

Простейший “на коленке” routing-код:

```php
function getUserShardConnection(int $userId): PDO
{
    $shard = $userId % 4;

    return match ($shard) {
        0 => $this->connShard0,
        1 => $this->connShard1,
        2 => $this->connShard2,
        3 => $this->connShard3,
    };
}
```

Это уже шардирование:

* нам нужно **знать, куда идти** за конкретным пользователем;
* при rebalancing’е это всё превратится в боль, и ровно об этом будет следующая секция.

---

#### 2.0.6. Шардирование по регионам и мапа-шардов

Пример из реальной жизни: гео-шардинг.

* Пользователи живут в разных регионах.
* Хотим:

    * чтобы европейцев обслуживал европейский кластер (меньше RTT);
    * американцев — американский;
    * азиатских — азиатский.

Наивный вариант: зашить mapping в код:

```php
function getClusterForRegion(string $region): string
{
    return match ($region) {
        'EU' => 'pg-eu-cluster',
        'US' => 'pg-us-cluster',
        'APAC' => 'pg-apac-cluster',
    };
}
```

Более взрослый вариант: **таблица-шардмапа**:

```sql
CREATE TABLE shard_map (
    region      text PRIMARY KEY,
    dsn         text NOT NULL,
    is_readonly boolean NOT NULL DEFAULT false
);
```

Теперь приложение:

* по `region` сначала идёт в `shard_map`,
* получает DSN/кластер,
* и уже туда лезет за данными.

Это важный мостик к **Dynamic Sharding**:

> Как только вы вынесли “куда идти” из `% N` в явную конфигурацию/таблицу,
> вы можете начинать **переезжать** между шардами без перекомпиляции всего мира.

И именно это мы будем разбирать дальше.

---

#### 2.0.7. Шардирование на уровне БД: Citus и друзья

Чтобы человек не думал, что всё делается только через “PHP + 4 коннекшена”, кратко отмечаем:

* в Postgres есть расширения, которые делают распределённую БД:

    * Citus — популярный вариант;
* они:

    * сами создают шарды;
    * рассовывают по воркерам;
    * держат routing и cross-shard запросы под капотом;

Сценарий для восприятия:

> То, что мы обсуждаем: “по какому ключу резать”, “как маршрутизировать”, “как жить с cross-shard” —
> это не фантазии. Это те же вопросы, на которые отвечают Citus, Vitess, Cockroach, Yugabyte и прочие “взрослые” движки.

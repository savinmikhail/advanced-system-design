### 2.0. Базовый ликбез: шардирование и партиционирование

#### 2.0.1. От какой боли вообще стартуем

Типичная картина:

* есть одна большая таблица `events` / `orders`;
* она растёт годами;
* в ней уже десятки/сотни миллионов строк;
* индексы раздулись, VACUUM жрёт время, бэкапы занимают ночь;
* seq scan по этой таблице начинает занимать секунды и жрёт диск

Плюс операционные штуки:

* таблица приближается к физическим ограничениям:

    * диск под ней забит;
    * на одной файловой системе жить уже стремно;
* хотим:

    * “горячие” данные держать на SSD;
    * “холодные” архивы — на дешёвом HDD;

    1. На уровне ОС ты монтируешь разные диски в разные директории:

       * `/mnt/ssd_fast`
       * `/mnt/hdd_slow`

    2. В Postgres создаёшь таблспейсы, которые указывают на эти директории:

        ```sql
        CREATE TABLESPACE fast_ssd LOCATION '/mnt/ssd_fast';
        CREATE TABLESPACE slow_hdd LOCATION '/mnt/hdd_slow';
        ```

    3. Дальше можешь:
    
         * при создании таблицы:
    
        ```sql
        CREATE TABLE events_hot (
            ...
        ) TABLESPACE fast_ssd;
        ```
    
        * при создании индекса:
    
        ```sql
        CREATE INDEX idx_events_hot_created_at
          ON events_hot(created_at)
          TABLESPACE fast_ssd;
        ```
    
        * перенести существующую таблицу:
    
        ```sql
        ALTER TABLE events_archive SET TABLESPACE slow_hdd;
        ```

На этом фоне появляются два базовых инструмента:

* **партиционирование** — разрезать таблицу на части внутри одного инстанса БД;
* **шардирование** — разнести эти части по разным машинам/кластерам.

И тут важное честное предупреждение:

> **Шардирование — тяжёлая артиллерия.
> К нему идут, когда остальные варианты уже упёрлись в стену.**

Пока можно, делаем по порядку:

1. Нормализуем схему, чиним индексы, переписываем самые тупые запросы.
2. Выжимаем **вертикальное масштабирование** (CPU, RAM, диск, быстрые SSD).
3. Вводим **партиционирование** на одном кластере.
4. И только когда всё это уже не спасает по объёму/нагрузке —
   **лезем в шардирование**, с пониманием, что оно привезёт с собой кросс-шардовые транзакции, миграции и прочую боль.

---

#### 2.0.2. Вертикальное и горизонтальное разрезание

Есть два базовых направления, в которых можно резать данные.

##### 1) Вертикальное разрезание (vertical partitioning)

Это не магическая фича Postgres, а просто **разбиение одной толстой таблицы на две по колонкам**.

Типичный жизненный сценарий:

* когда-то была аккуратная таблица `users`;
* потом туда по чуть-чуть навешивали поля:

    * `middle_name`,
    * `google_id`, `github_id`, `vk_id`,
    * куча флажков `is_blocked`, `is_beta`, `is_whatever`,
    * `bio`, `avatar_url`, `settings_json`, и т.п.;
* через пару лет это уже “стройка века” на 40+ колонок.

Пример до:

```text
users
──────────────────────────────────────────────────────────────────────
id (bigint, PK)
email (text, unique)
password_hash (text)
state (smallint)
middle_name (text)
google_id (text)
github_id (text)
vk_id (text)
avatar_url (text)
bio (text)
settings_json (jsonb)
... ещё пачка флагов/полей
```

90% запросов по факту делают:

```sql
SELECT id, email, state
FROM users
WHERE email = ?;
```

Но читают с диска **здоровенную строку**, потому что там теперь всё на свете.

Vertical partitioning в таком случае — это осознанный шаг:

```text
users_core
────────────────────────────────────────
id (bigint, PK)
email (text, unique)
password_hash (text)
state (smallint)
google_id (text)
github_id (text)
vk_id (text)

users_profile
────────────────────────────────────────
user_id (bigint, PK, FK → users_core.id)
avatar_url (text)
bio (text)
settings_json (jsonb)
... всё “редкое и жирное”
```

Дальше:

* все горячие штуки (логин, списки юзеров, проверки прав) ходят только в `users_core`;
* “страница профиля”, настройки и прочий обвес — в `users_profile`.

Зачем это вообще делать:

* **меньше байт на горячем пути**:

    * в кэш Postgres влезает больше строк ядра;
    * `Seq Scan`/`Index Scan` по `users_core` реально трогает меньше страниц;
* можно чуть уменьшить блоат и нагрузку на индексы ядра, если “мясо” меняется чаще, чем core-поля.

И важная оговорка:

> В 80–90% случаев вместо vertical partitioning достаточно:
>
> * перестать делать `SELECT *`;
> * пересмотреть индексы;
> * в крайнем случае — мигрировать данные в **нормально спроектированную** новую таблицу.
>
> Вертикальное разрезание — это уже хирургия для конкретно распухших, очень горячих сущностей, а не стандартный приём “на каждый чих”.

##### 2) Горизонтальное разрезание (horizontal partitioning)

Делим таблицу **по строкам**.

Примеры:

* `events` по дате: “декабрь 2025”, “январь 2026”;
* `users` по регионам: “EU”, “US”, “APAC”;
* `orders` по диапазонам ID.

И вот **горизонтальное разрезание** — это как раз то,
из чего вырастают и **партиции**, и **шарды**.

---

#### 2.0.3. Партиционирование: одна таблица, много кусков

**Партиционирование (partitioning)** — это когда одна логическая таблица разбита на несколько физических кусков по какому-то признаку, 
но всё это живёт **в одном инстансе БД** и по возможности **прозрачно для приложения**:

* приложение всё так же делает `INSERT/SELECT` в `events`;
* БД сама решает, в какую партицию складывать и откуда читать.

Например, в Postgres:

```sql
CREATE TABLE events (
    id         bigserial PRIMARY KEY,
    user_id    bigint      NOT NULL,
    created_at timestamptz NOT NULL,
    payload    jsonb       NOT NULL
) PARTITION BY RANGE (created_at);
```

Создадим две партиции:

```sql
CREATE TABLE events_2025_12
    PARTITION OF events
    FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');

CREATE TABLE events_2026_01
    PARTITION OF events
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');
```

Для приложения это всё ещё **одна таблица** `events`:

```sql
SELECT * FROM events
WHERE created_at >= now() - interval '1 day';
```

Postgres сам решает:

* в какие партиции лезть (partition pruning);
* какие партиции можно вообще не трогать.

Зачем это нам по боли:

* **Seq Scan и Index Scan перестают чесать всю вселенную.**
  Вместо одной таблицы на миллиард строк у нас десятки/сотни партиций.
  Запрос `WHERE created_at >= now() - interval '1 day'` лезет **только в свежие партиции**, а не бежит по всему истории проекта.

* **Индексы меньше и предсказуемее.**
  Вместо одного индекса размером в десятки гигабайт у нас куча меньших индексов на партициях.
  Планировщику проще принимать решения, а тебе проще:

    * пересоздавать/перестраивать отдельные индексы на нужных партициях,
    * не убивая диск и не упираясь в “один огромный btree”.

* **VACUUM/ANALYZE работают по кускам, а не по монстру.**
  Автовакуум и анализ статистики крутятся на конкретных партициях:

    * можно отдельно тюнить параметры для “горячих” и “холодных” партиций;
    * не бывает ситуации, когда один жирный `VACUUM` по всей таблице кладёт I/O.

* **Архивирование и retention-политики превращаются в DROP TABLE.**
  Старый год логов не нужен?
  `DROP TABLE events_2023_01` — и он исчез:

    * без `DELETE` по миллиарду строк,
    * без чудовищного блоата.

* **Только потом — разные диски/tablespace’ы для разных слоёв.**
  Когда уже есть партиции, можно:

    * свежие (горячие) партиции держать на более быстрых дисках/таблспейсах;
    * старые (холодные) — на более дешёвом/медленном storage;
    * или вообще выносить архивные партиции в отдельный кластер / offload.


Это уже **горизонтальное партиционирование**, но **пока на одном кластере**.

---

#### 2.0.4. Горизонтальное партиционирование по ключу: RANGE, LIST, HASH

Основные способы деления:

##### RANGE — по диапазонам

* по дате (`created_at`);
* по ID (0–1M, 1M–2M).

Хорошо, когда:

* данные естественно упорядочены (временные серии, логи);
* удобно архивировать “старые хвосты”.

Боль:

* если все запросы идут только к “последней” партиции — именно она становится горячей;
* легко получить **hot partition**.

##### LIST — по конкретным значениям

* по `region`: `EU`, `US`, `APAC`;
* по `tenant_id`, если их немного и они стабильны.

Полезно, когда есть небольшой фиксированный набор категорий.

##### HASH — равномерное разбрасывание по ключу

В Postgres:

```sql
CREATE TABLE users (
    id    bigint PRIMARY KEY,
    email text NOT NULL
) PARTITION BY HASH (id);

CREATE TABLE users_p0
    PARTITION OF users
    FOR VALUES WITH (MODULUS 4, REMAINDER 0);

CREATE TABLE users_p1
    PARTITION OF users
    FOR VALUES WITH (MODULUS 4, REMAINDER 1);
-- и так далее
```

Под капотом — тот же самый `user_id % N`, который потом встретится в Dynamic Sharding.

Плюсы:

* равномерное распределение нагрузки и объёма (если ключ нормальный);
* нет одной “горячей” партиции.

Минусы:

* сложнее делать операции “по диапазону”;
* сложнее мигрировать/менять количество партиций.

Все эти штуки — **про то, как горизонтально разрезать таблицу**.
Пока это **partitioning**: всё может жить в одном Postgres-кластере.

---

#### 2.0.5. Когда partitioning превращается в sharding

Теперь аккуратно фиксируем терминологию.

**Партиционирование**:

* режем таблицу на куски (партиции);
* **всё остаётся на одном инстансе** БД;
* приложение по идее не знает, что внутри что-то порезано.

**Шардирование (sharding)**:

* режем данные на куски (шарды);
* **разносим эти куски по разным инстансам/серверам**;
* появляется задача **routing’а** — куда идти за этим конкретным ключом.

Простейший кейс:

* `users_shard_1`: Postgres #1, данные по `user_id % 4 = 0`;
* `users_shard_2`: Postgres #2, `user_id % 4 = 1`;
* `users_shard_3`: Postgres #3, `user_id % 4 = 2`;
* `users_shard_4`: Postgres #4, `user_id % 4 = 3`.

Грубо:

> Партиционирование — это про то, как порезать стол.
> Шардирование — про то, как эти куски столешницы развезти по разным складам.

**Зачем вообще идти до шардов, если есть партиции?**

* таблица/индексы уже **физически не лезут** на один сервер/в один диск нормально (или скоро не влезут);
* хотим увеличить **пропускную способность записи**:

    * один HDD даёт 100–200 MB/s, SSD — 500–600 MB/s,
    * несколько шардов на нескольких машинах могут писать **параллельно**;
* хотим распределить **CPU-нагрузку**;
* хотим геораспределение:

    * пользователи из EU → кластер в Европе;
    * US → кластер в США и т.д.

При этом:

* почти всегда шардирование идёт **в паре с репликацией**:

    * каждый шард имеет свои реплики;
    * иначе падение одной машины = смерть куска данных/сервиса.

Простейший “на коленке” routing-код:

```php
function getUserShardConnection(int $userId): PDO
{
    $shard = $userId % 4;

    return match ($shard) {
        0 => $this->connShard0,
        1 => $this->connShard1,
        2 => $this->connShard2,
        3 => $this->connShard3,
    };
}
```

Это уже шардирование:

* нам нужно **знать, куда идти** за конкретным пользователем;
* при rebalancing’е такая схема быстро превратится в ад — ровно об этом дальше в Dynamic Sharding.

---

#### 2.0.6. Шардирование по регионам и мапа-шардов

Пример из реальной жизни: гео-шардинг.

* Пользователи живут в разных регионах.
* Хотим:

    * чтобы европейцев обслуживал европейский кластер (меньше RTT);
    * американцев — американский;
    * азиатских — азиатский.

Наивный вариант — захардкодить mapping в коде:

```php
function getClusterForRegion(string $region): string
{
    return match ($region) {
        'EU'   => 'pg-eu-cluster',
        'US'   => 'pg-us-cluster',
        'APAC' => 'pg-apac-cluster',
    };
}
```

Более взрослый вариант: **таблица-шардмапа**:

```sql
CREATE TABLE shard_map (
    region      text PRIMARY KEY,
    dsn         text NOT NULL,
    is_readonly boolean NOT NULL DEFAULT false
);
```

Теперь приложение:

* по `region` сначала идёт в `shard_map`;
* получает DSN/кластер;
* и уже туда лезет за данными.

Это важный мостик к **Dynamic Sharding**:

> Как только вы вынесли “куда идти” из `% N` в явную конфигурацию/таблицу,
> вы можете начинать **переезжать** между шардами без перекомпиляции всего мира.

И ровно это мы будем разбирать в следующей части.

---

#### 2.0.7. Шардирование на уровне БД: Citus и друзья

Чтобы не казалось, что всё делается только через “PHP + 4 коннекшена”, фиксируем:

* в Postgres есть расширения, которые делают распределённую БД:

    * Citus — популярный пример;

* они:

    * сами создают шарды и распределяют их по воркерам;
    * держат у себя routing;
    * умеют выполнять **cross-shard** запросы (join/агрегации) под капотом.

Важно донести идею:

> Всё, что мы обсуждаем — “по какому ключу резать”, “как маршрутизировать”,
> “как жить с кросс-шардовыми операциями” — это не теоретический онанизм.
> Это реальные вопросы, на которые отвечают Citus, Vitess, Cockroach, Yugabyte и прочие движки.

---

#### 2.0.8. Мини-резюме перед углублением

Чтобы не потерять картину:

* Вертикальное разрезание — по столбцам, для оптимизации схемы и индексов.
* Горизонтальное разрезание — по строкам; наш основной фокус.
* Partitioning:

    * одна логическая таблица;
    * много партиций;
    * всё ещё один кластер/инстанс.
* Sharding:

    * те же куски, но разнесённые по разным инстансам;
    * нужен routing и отдельный разговор про миграции и согласованность.
* Ключи разрезания:

    * RANGE / LIST / HASH;
    * региональные мапы, user_id, tenant_id, временные диапазоны.
* Боль, от которой стартуем:

    * таблица перестаёт влезать на одну машину/диск/индекс;
    * хотим разделить горячее/холодное;
    * хотим разнести пользователей по регионам;
    * хотим увеличить пропускную способность, а один инстанс уже выжат.

И сверху ещё раз:

> Шардирование — не модная фича, а крайняя мера.
> Сначала выжимаем всё из одной БД и партиционирования,
> потом уже тащим в жизнь шардирование, rebalancing и все связанные приключения.

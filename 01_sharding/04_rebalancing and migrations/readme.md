## 2.4.5. Rebalancing и миграции без downtime

### 2.4.5.1. Зачем нужен rebalancing (1 минута)

Мы уже частично затронули проблему перемещения данных между шардами в главе о динамическом шардировании и почему статическое шардирование нас не устраивает,
теперь поговорим об этом подробнее

По сути, rebalancing — это ответ на один простой вопрос:

> “Как сделать так, чтобы **распределение данных и нагрузки** снова стало здоровым?”

Основные причины "нездоровья" это:

1. **Неравномерный рост данных**
   Один шард распух до терабайта, остальные по 100 ГБ.

2. **Горячие пользователи / категории / сущности**
   Один шард получает 80–90% RPS.

3. **Добавление новых шардов (upscaling)**
   Ноды добавили → данные как-то надо перекинуть на них.

4. **Удаление шардов (downscaling)**
   Убрали лишнее железо → данные надо слить обратно.

5. **Изменение shard key**
   Самое неприятное — когда бизнес понял, что резать по `user_id` было ошибкой, а жить надо по `tenant_id` или `region`.

> Миграция — это не “переложить данные”.
> Миграция — это “переключить сервис на **новую карту мира** так, чтобы пользователи не заметили”.

И всё это происходит **в продакшене**, под нагрузкой, с пользователями, которые вообще не в курсе, что вы там перелопачиваете.

---

## 2.4.5.2. Виды миграций (2 минуты)

Чтобы не путаться в словах, введём базовую типологию.

1. **Resharding внутри того же кластера**
   Распределяем ключи по-новому между теми же нодами:
   например, дробим диапазон `[0..1М]` на два: `[0..500k]` и `[500k..1М]`.

2. **Upscaling**
   Было 4 шарда, стало 8.
   Нужно часть ключей “снять” со старых и положить на новые.

3. **Downscaling**
   Было 16 шардов, хотим 8.
   Данные с “лишних” шардов надо аккуратно слить на остальные.

4. **Online relocation**
   Точечно переносим отдельные диапазоны / tenant’ов / регионов:
   например, “всех EU-клиентов вывезти из US-кластера в EU-кластер”.

5. **Shard key migration**
   Самый тяжёлый случай: меняем саму идею шардирования.
   Пример: было `hash(user_id)`, стало `tenant_id` → дальнейший rebalancing уже живёт в другой модели.

Эта типология нужна, чтобы дальше понимать:
какой из подходов вообще уместен для какой задачи.

---

## 2.4.5.3. Основные проблемы при миграции (3 минуты)

Прежде чем обсуждать техники, честно перечислим, что **обязательно сломается**, если не подумать.

1. **Неконсистентность во время переноса**

   Если просто “копировать строки”:

    * часть данных может оказаться только на старом;
    * часть — только на новом;
    * что-то задублируется.

2. **In-flight writes**

   Пока вы копируете 500 ГБ, в базу продолжают прилетать новые записи:

    * кто-то обновляет профиль;
    * кто-то создаёт заказ;
    * кто-то меняет статус.

   Если эти изменения не учесть — новый шард при переключении будет отставать.

3. **Гонки (race conditions)**

   Одна часть кода читает из старого, другая — уже из нового.
   Клиент делает два запроса подряд — и вдруг видит разные данные.

4. **Atomic cutover**

   Момент “теперь правда читаем и пишем в новый шард” должен быть:

    * быстрым,
    * атомарным в пределах системы,
    * откатываемым.

5. **Нагрузка на сеть и базу**

   Прямолинейный backfill (переливка) типа:

   ```sql
   INSERT INTO new_shard.table
   SELECT * FROM old_shard.table;
   ```

   под нагрузкой легко кладёт:

    * старый шард (I/O, CPU),
    * новый шард,
    * сеть.

6. **Кеши**

   L1/L2-кеш могут продолжить держать старые данные:

    * старые shard map’ы;
    * старые значения по ключам;
    * стейт в приложении.

   В итоге часть запросов идёт уже по новой схеме, часть — по старой.

Все это надо учитывать, иначе будет проще уволиться чем пофиксить (шутка)

---

## 6. Общая структура online-миграции (1–2 минуты)

Почти все взрослые схемы rebalancing’а раскладываются на три шага:

1. **Перенос bulk-данных (backfill)**
   Перекачиваем основной объём из старого layout’а в новый.

2. **Синхронизация изменений (CDC / журнал изменений)**
   Пока идёт backfill, фиксируем все новые изменения и доносим их до нового шарда.

3. **Переключение маршрутизации (cutover)**
   Переводим чтение/запись на новый layout, минимизируя окно гонок.

Примерная схема:

![Схема миграции между шарадами](assets/01-migration-flow.png)

Дальше всё, что мы будем обсуждать — вариации на тему этих трёх шагов.

[//]: # (cdc объяснить)

---

## 7. Подход 1 (простая, но опасная штука): Dual Write (двойная запись) (2 минуты)

Первое, что приходит в голову (и что часто предлагают на собесах):

> “Давайте просто будем писать и туда, и туда, а потом аккуратно выключим старое”.

Алгоритм:

1. При каждом изменении пишем **одновременно** в старый и в новый шард.
2. Чтения какое-то время идут всё ещё из старого.
3. Когда уверены, что новый шард догнал — переворачиваем чтения на новый.
4. Старый шард либо выключаем, либо оставляем как бэкап и затем чистим.

Звучит просто. На практике:

* расхождение во времени:

    * запись в старый прошла, в новый отвалилась → данные расходятся;
* двойные инкременты / side-effects:

    * если апдейты не идемпотентные — легко сделать “+2” вместо “+1”;
* падение одной из сторон:

    * старый шард жив, новый нет / наоборот — консистентности нет.

Итог:

> Dual write иногда сгодится, но как базовое решение для серьёзной миграции — так себе идея.

Для собеса от вас обычно ждут не кода dual write, а объяснения, **почему это больно**:
расхождение времён записи, неидемпотентные апдейты, падение одной из сторон и отсутствие единого источника истины.

---

## 8–9. Другие базовые схемы (коротко)

Есть ещё несколько “полегче, чем CDC” подходов, которые часто всплывают в обсуждениях.

**“Write to new, read from old” + мигратор.**

* Все новые записи сразу идут в новый шард.
* Чтения пока выполняем из старого, чтобы “истина” была в одном месте.
* Фоновый мигратор по диапазонам/tenant’ам догоняет данные в новом шарде, после чего чтения переключаем.
* Плюс: нет двойной записи, понятный rollback.
* Минус: нужно аккуратно чистить расхождения, больше ручной логики.

**“Routing First”.**

* Сначала меняем shard-map/routing: новые запросы сразу летят в новый layout.
* Старый шард держит хвост (холодные данные), фоновый процесс постепенно выносит их.
* Плюс: почти нет downtime, горячие ключи переезжают сразу.
* Минус: сложнее рассуждать о консистентности, нужен fallback “если на новом не нашли — смотрим на старом”.

Для собеса обычно достаточно уметь на словах сравнить эти схемы с CDC+backfill и объяснить, где вы бы выбрали простую миграцию с мигратором, а где уже нужен тяжёлый артиллерийский паттерн.

---

## 2.4.5.4. Канонический сценарий: CDC + Backfill (3–4 минуты)

[//]: # (переместить блок)

Это “взрослый” паттерн, на котором живут большие ребята (Uber, Pinterest, Shopify и т.п.),
когда мигрируют сотни гигабайт и миллиарды записей без даунтайма и потерь.

Алгоритм:

1. **Backfill (snapshot)**
   Берём большой “снимок” данных:

    * читаем старый шард по чанкам;
    * заливаем всё в новый layout;
    * при этом согласовываем нагрузку, чтобы не убить базу.

2. **CDC (Change Data Capture)**
   Параллельно включаем слежение за изменениями:

    * из журнала транзакций / Debezium / logical replication;
    * каждое изменение (`INSERT/UPDATE/DELETE`) превращается в событие.

3. **Migrator**
   Отдельный воркер читает эти события и **повторяет** изменения на новом шарде:

    * гарантированная доставка;
    * корректный порядок для каждого ключа (по offset/LSN).

4. **Дожидаемся, пока лаг станет ≈ 0**
   То есть все изменения со старого уже догнаны в новый.

5. **Переключаем чтения и записи**
   Роутер/фичефлаг переводит трафик на новый layout.

6. Старый шард превращается в архив / временный бэкап, позже чистится.

Плюсы:

* отлично работает на больших объёмах;
* чёткая модель консистентности;
* нет потерь данных при in-flight writes.

Минусы:

* инфраструктурно сложнее:

    * нужен брокер (Kafka/Pulsar) или логическая репликация;
    * нужен надёжный мигратор;
    * нужно мониторить лаг/отставание.

Это, по сути, общий паттерн, который используют крупные компании для “миграций на миллиарды записей”.

Здесь хорошо работает одна эволюционирующая картинка (её можно рисовать/анимировать по шагам):

1. **Состояние “до”** — только старый шард/layout, все стрелки чтения/записи идут туда.
2. **Backfill** — появляется новый шард, от старого толстая стрелка “копирования” чанками данных.
3. **CDC stream** — к backfill добавляется стрелка “stream изменений” (журнал, Debezium, logical replication) от старого к новому.
4. **Cutover** — стрелки чтения/записи разворачиваются на новый layout, старый становится “подписью” как архив/backup.

---

## 2.4.5.5. Live Traffic Replay — просто знать идею (1 минута)

Есть ещё подход **Live Traffic Replay** — это уже hardcore уровня Amazon/Yandex.
Суть в том, что вы реплеите реальный продовый трафик в новый кластер параллельно со старым,
сравниваете состояние/ответы, и только после этого переключаете пользователей.

Для собеса достаточно понимать идею:

> “Мы гоняем реальный трафик в новый layout, проверяем, что он ведёт себя так же, как старый,
> и используем это как последнюю проверку перед cutover”.

В деталях это нужно только тем, кто реально строит свои базы/хранилища, а не “просто” шардирует Postgres.

---

## 2.4.5.6. Atomic cutover (3 минуты)

Как бы мы ни переносили данные, всегда останется один ключевой момент:

> “С какой секунды мы считаем новым источником правды **новый layout**?”

Этот момент должен быть:

* коротким;
* контролируемым;
* откатываемым.

Приёмы:

1. **Feature flag**

    * В коде есть переключатель `useNewShardLayout`.
    * Пока он `false` — все чтения/записи идут по старой схеме.
    * Переключаем флаг — и весь трафик идёт по новой карте.

2. **Поколения (generation / epoch)**

    * У shard-map’а есть версия: `generation = 17`.
    * Клиент/роутер помечает запросы `X-Shard-Generation: 17`.
    * Когда переключаем layout на `generation = 18`,
      старый layout продолжает жить какое-то время, но новые запросы уже идут по 18-й версии.
    * Это помогает избежать гонок между разными экземплярами приложения.

[//]: # (как мы эпоху то прокинем?)

3. **Короткий freeze писателей**

    * На очень маленькое окно (десятки миллисекунд) можно остановить запись:

        * через блокировку в БД;
        * через флаг в роутере.
    * За это время меняем shard-map / переключаем координатор.
    * Пользователь на таком окне обычно ничего не замечает, если это <100ms.

4. **Write fencing**

    * Запись проходит только если версия совпадает:

      ```sql
      UPDATE users
      SET ...
      WHERE id = :id AND generation = 18;
      ```

    * Всё, что прилетело “не в ту эпоху” — либо отклоняется, либо переповторяется.

Общая идея:

> Cutover — это не “нажать одну волшебную кнопку”,
> а аккуратно продуманный переход с возможностью быстро откатиться назад.

---

## 2.4.5.7. In-flight writes (1 минута)

Отдельно проговорим проблему:

* пока идёт миграция, система не стоит на паузе;
* новые записи и апдейты продолжают прилетать.

Если их не учитывать, получаем:

* “дырки” в новых шардах;
* дубли;
* странные расхождения, которые вылезут через неделю в отчётах.

Техники, которые помогают:

* **идемпотентные операции**:

    * чтобы повторное применение “того же” события не ломало данные;
* **write fencing / generation checks**:

    * писать только в правильный layout;
* **drain очередей**:

    * перед финальным cutover вычищаем очереди задач/сообщений;
* **короткие freeze-окна для критичных операций**:

    * лучше 50ms легкого фриза, чем вечный бардак в состояниях.

---

## 2.4.5.8. Post-migration cleanup (1 минута)

После успешного cutover работа не заканчивается.

Нужно:

* почистить старые данные:

    * либо сразу дропнуть старый шард,
    * либо повесить на него TTL / GC-воркеры;
* выбросить старые shard-map’ы и конфиги,
  чтобы к ним не откатывались случайно;
* убедиться, что кеши не держат старые маршруты и значения;
* собрать метрики до/после:

    * latency,
    * error rate,
    * нагрузка на новые шарды.

И очень желательно:

* оставить фонового уборщика “мусора” на время:

    * данные с устаревшими generation’ами;
    * странные хвосты, которые могли появиться на границах.

---

## 2.4.5.9. Анти-паттерны (1 минута)

Короткий список того, что почти всегда заканчивается плохо.

1. **Полная остановка сервиса “на время миграции”**

    * “Мы ночью всё выключим, покрутим SQL, утром включим”.
    * В реальном бизнесе — это час+ простоя, с неясным исходом.

2. **Один гигантский SQL-скрипт**

    * `ALTER TABLE`, `INSERT INTO ... SELECT ...` на сотни гигабайт одним махом.
    * Любая ошибка — и ты откатываешься не к “минус 5 секунд”, а к “минус ночь”.

3. **Клиент знает, на какой шард идти**

    * Шардирование зашито в клиентские приложения / мобильники.
    * Любой rebalancing теперь = обновить весь зоопарк клиентов.

4. **“Сначала вырубим старый шард, потом перенесём данные”**

    * В этот момент у тебя нет “источника правды”.
    * Любой косяк в переносе → данные ушли навсегда.

---

## 2.4.5.10. Итоги и мостик (1 минута)

Собираем всё в одну мысль:

* Шардирование без rebalancing — это бомба с таймером.
* Неизбежно наступит момент, когда layout надо будет менять:

    * из-за роста,
    * из-за ошибок в shard key,
    * из-за новых регионов и требований бизнеса.
* Важно не “надеяться, что обойдётся”,
  а заранее иметь понятную схему:

    * backfill,
    * CDC/журнал изменений,
    * аккуратный cutover,
    * и план уборки.

И как только у нас появляются несколько шардов, миграции данных и свои layout’ы для чтения/записи,
следующий естественный вопрос — **а как жить с транзакциями и консистентностью в такой системе?**

Мы уже несколько раз упоминали боль “транзакций между шардами”,  
в следующей части как раз разберём, какие подходы к этому вообще существуют и зачем в реальных системах почти везде всплывают Saga, outbox и eventual consistency.

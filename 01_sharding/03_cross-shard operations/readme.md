# Cross-Shard операции и оптимизации — сценарий

## 0. Вступление (1 минута)

Мы научились шардировать данные, выбирать shard key, бороться с hot keys и даже динамически перестраивать layout шардинга так, чтобы кластер не умирал при росте нагрузки.

Но у этого праздника есть обратная сторона.

Как только мы разрезали данные по шардам — по `userId`, `tenantId`, диапазонам, регионам — выясняется неприятный факт: **бизнес-логика вообще не обязана жить по тем же границам**.

Продукт говорит:
“покажи все заказы в статусе `pending`”,
“считай выручку по всем продавцам”,
“переведи деньги с этого аккаунта на тот” —
а данные лежат по разным шардам.

И тогда появляется штука, через которую проходят все большие системы —
**cross-shard операции**.

Это, по сути, ещё одна “самая неприятная часть шардирования”.

---

## 1. Почему cross-shard операции неизбежны (2 минуты)

Формулировка простая:

> Шардирование помогает БД выдерживать нагрузку,
> но ломает привычную “монолитную” модель данных в голове.

Разберёмся на живых кейсах.

### 1) Сущности живут на разных шардах

Мы режем по `userId` или `tenantId`, а бизнес-кейсы — между ними:

* пользователь покупает у продавца:

    * покупатель на одном шарде, продавец на другом;
* комментарий хранится рядом с постом,

    * а профиль автора — в другом шардe, где лежит сам пользователь;
* чат между двумя пользователями:

    * один участник на шарде 3, второй на шарде 7 — а чат логически один.

Любой “join между сущностями” легко превращается в cross-shard.

### 2) Запрос без shard key

```http
GET /orders?status=pending
```

В запросе **нет** `userId` / `tenantId` / `region`.

Никакой роутер по этому запросу не узнает, какой шард трогать.
Ответ один: **идти по всем**.

### 3) Фильтр / поиск по атрибуту

Возраст, категория, цена, рейтинг, “все активные пользователи из РФ” —
почти всегда **размазаны по шардам**.

Если нет отдельного индекса/каталога, это снова fan-out на все шарды.

### 4) Глобальные агрегаты и пагинация

“Покажи количество заказов за неделю”,
“топ-10 товаров по продаже”,
“сколько активных пользователей сегодня заходили”.

Это по определению **агрегаты по всему датасету**, значит — cross-shard.

### 5) Денежные транзакции

Перевод денег с одного аккаунта на другой:

* аккаунты легко могут оказаться на разных шардах;
* хочется сделать “списать здесь, зачислить там” **атомарно**.

Добро пожаловать в мир cross-shard транзакций.

### 6) Глобальная сортировка

“Покажи последние заказы”,
“последние сообщения в системе”,
“общий топ-10 рейтинга”.

Чтобы отсортировать “по времени / по рейтингу” глобально, придётся смотреть **во все шарды**, а потом мержить результаты.

---

## 2. Каталог cross-shard операций (2 минуты)

Чтобы не тонуть в хаосе, заведём каталог.

Большинство проблем упираются в комбинации следующих типов операций:

* **Cross-shard JOIN** — связываем данные с разных шардов.
* **Cross-shard Aggregation** — `SUM` / `COUNT` / `GROUP BY` по всему кластеру.
* **Cross-shard Filtering / Search** — фильтр/поиск по атрибутам, не совпадающим с shard key.
* **Cross-shard Transactions** — атомарные изменения на нескольких шардах сразу.
* **Cross-shard Ordering / Pagination** — глобальный сортированный список/лента.
* **Scatter-Gather Queries** — fan-out на N шардов и merge результата.

Дальше будем не просто “бороться с болью”, а обсуждать,
**какими техниками снижать количество и цену таких операций.**

---

## 3. Практические стратегии оптимизации (7–10 минут)

### 3.1. Data Locality — правильный shard key

Начнём с неприятной правды:

> 90% болезненных cross-shard операций —
> из-за того, что шардирование сделали “по приколу”, а не по домену.

Система нормальная, но выбрали shard key, который не отражает **границы бизнес-кейсов**.

Правильный подход:

* соцсеть — резать по `userId`:

    * лента, лайки, комментарии пользователя можно хранить “рядом”.
* SaaS — по `tenantId`:

    * данные одного клиента держим на одном шарде.
* маркетплейс — по `sellerId`:

    * все заказы и товары продавца локализованы.
* мессенджер — по `chatId`:

    * сообщения одного чата не размазываем по кластеру.

Если ключ выбран правильно:

* большая часть запросов **локальны**;
* JOIN происходит “внутри шарда”, а не через сеть;
* cross-shard остаются только там, где это **реально глобальная логика**.

Формула:

> “Выбор shard key — лучшая оптимизация cross-shard.
> Всё остальное — уже последствия.”

---

### 3.2. Денормализация — убить JOIN любой ценой (3 минуты)

В монолите нас учили:

> “Нормализация — это хорошо, 3НФ, чтобы не дублировать данные”.

В распределённой системе цена JOIN другая:

> `JOIN` = сетевой вызов, потенциально cross-shard.

Идея:

**дублируем** критичные поля в “место, где реально нужен ответ”.

Примеры:

* `username` в таблице `orders`:

    * чтобы не ходить отдельно к `users`.
* `price` в `order_items`:

    * чтобы считать сумму по позиции без похода в `products`.
* `sellerName` в `product_listing`:

    * чтобы отдавать карточку товара без объединения с таблицей продавцов.

Да, это **денормализация**, и да, данные могут временно расходиться.

Как жить:

* вводим события:

    * `UserUpdated`, `ProductUpdated`, `SellerRenamed`;
* на их основе обновляем денормализованные копии асинхронно.

Плюсы:

* запросы становятся **локальными**;
* меньше cross-shard JOIN;
* latency сильно падает.

Минусы:

* **eventual consistency**:
  аватар сменили, а в заказе старый, пока не доедет событие;
* нужен pipeline событий / CDC / worker’ы, которые всё это пересчитывают.

Коротко:

> Нормализация — хорошо.
> Но масштабируемость и latency — лучше.

---

### 3.3. Scatter-Gather (Fan-Out) — когда избежать нельзя (2 минуты)

Иногда никак:

* запрос реально глобальный;
* shard key не помогает.

Тогда делаем классический **fan-out / scatter-gather**:

1. Роутер/координатор получает запрос.
2. Рассылает его **на все (или часть) шардов**.
3. Каждый шард считает **частичный результат**:

    * свою сумму, свой `COUNT`, свой топ-10.
4. Роутер собирает всё, мержит и отдаёт клиенту.

Схема простая, но есть подстава:

> Один медленный шард делает медленным **весь запрос**.
> Tail latency растёт примерно пропорционально количеству шардов.

Что можно улучшить:

* **shard filtering** — ходить не на все шарды, а только на те, где точно есть кандидаты:

    * через routing index / метаданные;
* **кэширование**:

    * кешировать partial-результаты на шардах;
    * кешировать итоговый merge для популярных запросов;
* **early termination**:

    * при поиске топ-10 можно остановиться, когда уже набрали достаточно кандидатов и “хвост” мало что изменит;
* **пропуск пустых шардов**:

    * если шард “знает, что у него по этому фильтру результата нет”, он может отвечать мгновенно “ничего”.

Fan-out — это не зло само по себе.
Зло — это бездумный fan-out “на все шарды всегда”.

---

### 3.4. Routing Index / Directory Sharding — как не ходить везде (2 минуты)

Чтобы не бегать по всем шардам каждый раз,
можно завести **отдельный индекс маршрутизации**.

Идея:

> маленькая таблица/индекс, которая отвечает на вопрос:
> “какие шарды вообще имеют смысл для такого запроса?”

Примеры:

* `age_index`:

  ```text
  age 18–25 → [shard2, shard5, shard7]
  age 26–35 → [shard1, shard3]
  ```

* `premium_index`:

  ```text
  premium=true → [shard3]
  ```

Тогда запрос:

```http
GET /users?age=18-25&premium=true
```

раскладывается не на 64 шарда, а, например, на `[shard3, shard5]`.

Где это используется:

* Cassandra / Elastic / Solr — в виде вторичных индексов / inverted index’а;
* крупные соцсети и маркетплейсы — в виде вспомогательных “routing таблиц”.

Эффект:

* уменьшаем fan-out;
* хвостовая latency падает;
* нагрузка на кластер становится более предсказуемой.

Цена:

* нужно **обновлять этот индекс** при изменении данных;
* это ещё одна сущность, которую надо мониторить и бэкапить.

---

### 3.5. Cross-Shard Transactions → Saga / Outbox (3 минуты)

Фантазия junior’а:

> “Сделаем `BEGIN` на двух шардах и закоммитим. Что может пойти не так?”

Ответ: всё.

Классический 2PC (two-phase commit) в распределённой системе:

* глобальные блокировки;
* coordinator, смерть которого ломает всё;
* latency, завязанная на самый медленный участник;
* системная сложность, к которой ты не готов, если ты не Google/Spanner.

Реалистичный подход:

> **Не пытаться сделать “одну большую атомарную транзакцию на весь кластер”**,
> а превратить её в **workflow**.

Это и есть **Saga pattern**:

* составная операция разбивается на шаги:

    * `reserveMoney`, `reserveInventory`, `createOrder`…
* у каждого шага есть **компенсация**:

    * `releaseMoney`, `releaseInventory`, `cancelOrder`…
* если на шаге 3 всё падает — мы не “откатываем весь кластер”,
  а вызываем компенсирующие шаги к уже выполненным.

Оркестрация бывает:

* централизованной (отдельный saga orchestrator / workflow engine);
* событийной (каждый сервис реагирует на события и сам решает, что делать).

Важный вспомогательный паттерн — **Outbox + CDC**:

* записываем событие в локальную таблицу вместе с бизнес-изменением;
* отдельный процесс/CDC-пайплайн надёжно вываливает события в очередь/шину;
* никакой “потерянной записи в Kafka”.

Ключевая мысль:

> ACID между шардами — это почти всегда миф.
> В реальной жизни — Saga, компенсации и хороший мониторинг.

И да, **компенсация всегда сложнее, чем кажется на доске**.

---

### 3.6. Pre-Aggregation (MapReduce стиль) (2 минуты)

Если глобальные агрегаты неизбежны —
самый дешёвый запрос — тот, который уже посчитан.

Идея:

* вместо того чтобы каждый раз бегать по всем шардам,
  мы считаем **частичные агрегаты заранее**.

Примеры:

* суммарные лайки поста;
* количество просмотров за день;
* оборот по продавцу за час;
* топ-100 товаров в категории.

Паттерн:

1. На каждом шарде считаем свои маленькие счётчики / топы:

    * `likes_per_post_per_hour`,
    * `sales_per_seller_per_day`.
2. Центральный сервис/джоба периодически склеивает это в глобальную картину.

Плюсы:

* нагрузка плавно размазана по времени;
* запрос “покажи метрику” становится обычным чтением из маленькой таблицы/кеша;
* легко кэшировать и обновлять.

Минусы:

* данные чуть отстают от реального времени;
* нужен pipeline, который всё это считает и не падает.

Но в 99% продовых сценариев “агрегаты, обновляемые раз в минуту/5 минут”, —
это более чем нормально.

---

### 3.7. Кэширование cross-shard запросов (1–2 минуты)

И, конечно, **кеш**.

Где он особенно полезен:

* глобальные / тяжёлые cross-shard запросы;
* популярные фильтры и топы;
* отчётность.

Техники:

* кешировать **partial-результаты** на шардe:

    * каждый шард хранит свои агрегаты / свои “топы”,
      а merge-узел только их склеивает;
* кешировать **уже склеенный результат** на уровне API:

    * “топ-100 товаров по продажам” → обновляем раз в N секунд;
* использовать **materialized views** / precomputed buckets:

    * ~“данные за 1 минуту”, “за 1 час”, “за день”.

Пример:

> Топ-100 товаров в категории вполне можно обновлять раз в минуту.
> Никто из пользователей не заметит разницы между “3 секунды назад” и “30 секунд назад”,
> а база — заметит.

---

## 4. Анти-паттерны (1 минута)

Быстрый список того, что гарантированно стреляет в ногу.

### JOIN через REST другого сервиса

“Нет JOIN’ов, мы же микросервисы, мы просто ходим REST-запросами.”

Реально это:

* distributed N+1;
* куча сетевых hop’ов;
* кошмарная latency и дебаг.

Часто хуже, чем один честный cross-shard в базе.

### Scatter-gather на все шарды всегда

“Нам лень думать, просто делаем fan-out на 64 шарда каждый раз.”

Результат:

* tail latency улетает в космос;
* любой проблемный шард делает больно всем запросам.

### ACID между шардами “как в монолите”

“Мы же можем сделать 2PC / XA, что может пойти не так?”

Ответ: **всё**.
Это уровень Spanner/Cockroach с атомными часовыми и мозгами ядра.

### Клиент знает шард

“Мы закопаем логику маршрутизации в клиента:
он сам знает, на какой шард ходить.”

Это убивает:

* динамическое шардирование;
* rebalancing;
* любые изменения layout’а.

Поменять схему → обновить всех клиентов → удачи.

---

## 5. Hot keys — отдельная категория боли (1 минута)

Отдельно стоит напомнить про **hot keys**:

* даже если шардирование хорошее,
  один безумно горячий ключ (или tenant) может перегреть один шард;
* любой cross-shard, зависящий от этого шарда, начнёт страдать.

Лечится уже знакомыми техниками:

* **microsharding** — дробим ответственность по этому ключу/tenant’у на несколько микрошардов;
* вынесение в **отдельный tier** — специальный кластер под горячие ключи;
* агрессивный **кеш** и локальные счетчики;
* **throttling / token bucket** — один клиент не должен иметь возможности сжечь весь кластер.

Идея проста:

> Hot key — это не “просто очередная нагрузка”,
> это отдельная планета с собственными правилами.

---

## 6. Закрытие и мостик к следующей теме (30 секунд)

Собираем всё в одну картинку:

* Cross-shard операции — **не баг**, а нормальная цена масштабируемости.
* Задача инженера — не “убить их всех”,
  а **минимизировать их количество и стоимость**:

    * правильным shard key,
    * денормализацией,
    * routing-индексами,
    * pre-aggregation,
    * кешированием,
    * и в крайнем случае — fan-out’ом по уму.

И важный вывод:

> Даже если вы идеально оптимизировали cross-shard операции,
> всё развалится, если один шард будет хронически перегружен.

Поэтому следующая тема логична:

**Rebalancing & Online Migrations** —
как таскать данные между шардами без даунтайма и без фейерверка из падений.

# Блок 2.4. Cross-Shard операции и оптимизации

## 0. Вступление (1 минута)

Мы научились шардировать данные, выбирать shard key, бороться с hot keys и даже динамически перестраивать layout шардинга так, чтобы кластер не умирал при росте нагрузки.

Но у этого праздника есть обратная сторона.

Как только мы разрезали данные по шардам — по `userId`, `tenantId`, диапазонам, регионам — выясняется неприятный факт: **бизнес-логика вообще не обязана жить по тем же границам**.

Продукт говорит:
“покажи все заказы в статусе `pending`”,
“считай выручку по всем продавцам”,
“переведи деньги с этого аккаунта на тот” —
а данные лежат по разным шардам.

И тогда появляется штука, через которую проходят все большие системы —
**cross-shard операции**.

Это, по сути, ещё одна “самая неприятная часть шардирования”.

Для примеров в этой части будем держать в голове **маркетплейс**.
Типичные задачи там:

* глобальный топ товаров (по продажам/просмотрам);
* поиск по атрибутам (“кроссовки, чёрные, до 5000₽, с бесплатной доставкой”);
* отчёт по продавцам (оборот, количество заказов, конверсия).

И везде одно и то же: данные уже разрезаны по шардам (по `userId`, `sellerId`, регионам),
а бизнес‑запрос идёт “поверх всего этого зоопарка”.

---

## 2.4.1. Почему cross-shard операции неизбежны (2 минуты)

Формулировка простая:

> Шардирование помогает БД выдерживать нагрузку,
> но ломает привычную “монолитную” модель данных в голове.

Разберёмся на живых кейсах.

### 1) Сущности живут на разных шардах

Мы режем по `userId` или `tenantId`, а бизнес-кейсы — между ними.

В маркетплейсе это выглядит так:

* покупатель оформляет заказ у продавца:

    * покупатель на одном шарде, продавец и его витрина — на другом;
* страница заказа должна показать:

    * и позиции заказа,
    * и данные товара,
    * и имя продавца,
    * всё это потенциально с разных шардов.

В более общем виде:

* комментарий хранится рядом с постом,

    * а профиль автора — в другом шарде, где лежит сам пользователь;
* чат между двумя пользователями:

    * один участник на шарде 3, второй на шарде 7 — а чат логически один.

Любой “join между сущностями” легко превращается в cross-shard.

### 2) Запрос без shard key

```http
GET /orders?status=pending
```

В запросе **нет** `userId` / `tenantId` / `region`.

Никакой роутер по этому запросу не узнает, какой шард трогать.
Ответ один: **идти по всем**.

### 3) Фильтр / поиск по атрибуту

Возраст, категория, цена, рейтинг, “все активные пользователи из РФ” —
почти всегда **размазаны по шардам**.

Если нет отдельного индекса/каталога, это снова fan-out на все шарды.

### 4) Глобальные агрегаты и пагинация

В маркетплейсе это классические “аналитические” запросы:

* “покажи количество заказов за неделю по всем продавцам”;
* “глобальный топ‑10 товаров по выручке”;
* “сколько активных покупателей сегодня заходили”.

Это по определению **агрегаты по всему датасету**, значит — cross-shard.

### 5) Денежные транзакции

В маркетплейсе это перевод денег:

* с баланса покупателя;
* на баланс продавца;
* плюс комиссию площадки и, возможно, кешбек/бонусы.

Баланс покупателя, баланс продавца и бухгалтерские проводки легко оказываются на разных шардов.
Хочется сделать “списать здесь, зачислить там” **атомарно**.

Добро пожаловать в мир cross-shard транзакций.

### 6) Глобальная сортировка

“Покажи последние заказы”,
“последние сообщения в системе”,
“общий топ-10 рейтинга”.

Чтобы отсортировать “по времени / по рейтингу” глобально, придётся смотреть **во все шарды**, а потом мержить результаты.

---

## 2.4.2. Каталог cross-shard операций (2 минуты)

Чтобы не тонуть в хаосе, заведём каталог.

Большинство проблем упираются в комбинации следующих типов операций:

* **Cross-shard JOIN** — связываем данные с разных шардов.
* **Cross-shard Aggregation** — `SUM` / `COUNT` / `GROUP BY` по всему кластеру.
* **Cross-shard Filtering / Search** — фильтр/поиск по атрибутам, не совпадающим с shard key.
* **Cross-shard Transactions** — атомарные изменения на нескольких шардах сразу.
* **Cross-shard Ordering / Pagination** — глобальный сортированный список/лента.
* **Scatter-Gather Queries** — fan-out на N шардов и merge результата.

Если снова посмотреть на маркетплейс:

* Cross-shard JOIN — страница заказа, где на одном экране нужны данные заказа, товара и продавца.
* Cross-shard Aggregation — отчёт “оборот по продавцам за неделю”, когда `orders` уже разрезаны по `userId` или регионам.
* Cross-shard Filtering / Search — поиск товара по атрибутам (цвет, размер, бренд, цена), размазанным по шардированным индексам.
* Cross-shard Transactions — списать деньги у покупателя, зачислить продавцу, применить комиссию и кешбек.
* Cross-shard Ordering / Pagination — глобальный “топ‑продажи” или “последние заказы по всей платформе”.
* Scatter-Gather Queries — любой глобальный поиск/отчёт, который не привязан к одному shard key.

Дальше будем не просто “бороться с болью”, а обсуждать,
**какими техниками снижать количество и цену таких операций именно в таких сценариях.**

---

## 2.4.3. Практические стратегии оптимизации (7–10 минут)

### 2.4.3.1. Data Locality — правильный shard key

Начнём с неприятной правды:

> 90% болезненных cross-shard операций —
> из-за того, что шардирование сделали не по домену.

Система нормальная, но выбрали shard key, который не отражает **границы бизнес-кейсов**.

Правильный подход:

* соцсеть — резать по `userId`:

    * лента, лайки, комментарии пользователя можно хранить “рядом”.
* SaaS — по `tenantId`:

    * данные одного клиента держим на одном шарде.
* маркетплейс — по `sellerId`:

    * все заказы и товары продавца локализованы.
* мессенджер — по `chatId`:

    * сообщения одного чата не размазываем по кластеру.

Если ключ выбран правильно:

* большая часть запросов **локальны**;
* JOIN происходит “внутри шарда”, а не через сеть;
* cross-shard остаются только там, где это **реально глобальная логика**.

Формула:

> “Выбор shard key — лучшая оптимизация cross-shard.
> Всё остальное — уже последствия.”

---

### 2.4.3.2. Денормализация — убить JOIN любой ценой (3 минуты)

В маркетплейсе это очень приземлённая задача:
страница “Мой заказ” должна показать список позиций, названия товаров, аватарку продавца и итоговую сумму —
желательно за один локальный запрос, а не через цепочку JOIN’ов по разным шадам.

В монолите мы следуем правилу:

> “Нормализация — это хорошо, 3НФ, чтобы не дублировать данные”.

В распределённой системе цена JOIN другая:

> `JOIN` = сетевой вызов, потенциально cross-shard.

Идея:

**дублируем** критичные поля в “место, где реально нужен ответ”.

Примеры в том же маркетплейсе:

* `username` в таблице `orders`:

    * чтобы страница заказов не ходила отдельно в `users` за именем покупателя.
* `price` в `order_items`:

    * чтобы считать сумму по позиции без похода в `products`.
* `sellerName` и рейтинг в `product_listing`:

    * чтобы отдавать карточку товара без отдельного JOIN с таблицей продавцов.

Да, это **денормализация**, и да, данные могут временно расходиться.

Как жить:

* вводим события:

    * `UserUpdated`, `ProductUpdated`, `SellerRenamed`;
* на их основе обновляем денормализованные копии асинхронно.

Плюсы:

* запросы становятся **локальными**;
* меньше cross-shard JOIN;
* latency сильно падает.

Минусы:

* **eventual consistency**:
  аватар сменили, а в заказе старый, пока не доедет событие;
* нужен pipeline событий / CDC / worker’ы, которые всё это пересчитывают.

Коротко:

> Нормализация — хорошо.
> Но масштабируемость и latency — лучше.

---

### 2.4.3.3. Scatter-Gather (Fan-Out) — когда избежать нельзя (2 минуты)

Иногда никак:

* запрос реально глобальный (глобальный топ товаров, отчёт по всем продавцам);
* shard key не помогает.

Тогда делаем классический **fan-out / scatter-gather**:

1. Роутер/координатор получает запрос.
2. Рассылает его **на все (или часть) шардов**.
3. Каждый шард считает **частичный результат**:

    * свою сумму, свой `COUNT`, свой топ-10.
4. Роутер собирает всё, мержит и отдаёт клиенту.

Схема простая, но есть подстава:

> Один медленный шард делает медленным **весь запрос**.
> Tail latency растёт примерно пропорционально количеству шардов.

Что можно улучшить:

* **shard filtering** — ходить не на все шарды, а только на те, где точно есть кандидаты:

    * через routing index / метаданные;
* **кэширование**:

    * кешировать partial-результаты на шардах;
    * кешировать итоговый merge для популярных запросов;
* **early termination**:

    * при поиске топ-10 можно остановиться, когда уже набрали достаточно кандидатов и “хвост” мало что изменит - good enough
* **пропуск заведомо пустых шардов**:
    * на уровне роутера — если по routing index видно, что нужный фильтр попадает только в часть шардов, остальные даже не дёргаем;
    * на уровне движка — partition pruning / min-max статистика / BRIN: движок понимает, что конкретная партиция/кусок “по этому условию точно пуст”, и даже не читает её.

Fan-out — это не зло само по себе.
Зло — это бездумный fan-out “на все шарды всегда”.

---

### 2.4.3.4. Routing Index / Directory Sharding — как не ходить везде (2 минуты)

В маркетплейсе так выглядит “поиск по каталогу”:
пользователь задаёт фильтры по цене, бренду, цвету, размеру, наличию на складе —
ни один из этих атрибутов не совпадает с тем, по чему мы режем шарды.

Когда запрос не содержит shard key (`age`, `status`, `premium=true`),
наивный путь — fan-out на все 64 шарда.

Чтобы так не делать каждый раз, заводим **дополнительный routing index** —
не второе шардирование, а слой, который отвечает на вопрос:

> “Какие шарды вообще имеет смысл трогать для такого фильтра?”

Важно не путать:

* **shard-map** — по `userId` / `tenantId` выбираем один шард.
* **routing index** — по `age/status/...` выбираем **набор кандидатов-шардов**,
  остальные считаем заведомо пустыми.

#### Пример 1: грубый индекс по бакетам

Для каждого шарда ведём статистику по возрастным диапазонам:

![Пример routing index по age buckets](assets/01-routing-age-buckets.png)

При запросе:

```http
GET /users?age=18-25
```

роутер:

* видит, что `18–25` есть только на Shard1 и Shard3;
* вообще не дёргает Shard2 и остальные.

Мы **не утверждаем**, что “все 18–25 только на Shard1 и Shard3”,
мы говорим: “на других шардах точно никого 18–25 нет”.

#### Пример 2: глобальный secondary index

Отдельный сервис/таблица хранит:

```text
(age bucket, premium flag) → список user_id, сгруппированных по шардам
```

Для фильтра `age=18-25&premium=true`:

```text
→ Shard1: [u123, u456]
→ Shard3: [u789, u101112]
```

Роутер сначала идёт в индекс, получает:

* какие шарды трогать,
* какие ключи на них искать,

и уже делает точечные запросы на эти шарды.

Так устроены:

* secondary indexes в Cassandra;
* Elastic / Solr (inverted index);

Для postgresql с citus придется руками прикручивать

#### Эффект

* уменьшаем fan-out (не 64 шарда, а, скажем, 2–5);
* хвостовая latency падает;
* нагрузка на кластер предсказуемее.

#### Цена

* routing index нужно **обновлять** при изменении данных;
* это отдельный кусок инфраструктуры:

  * репликация,
  * мониторинг,
  * бэкапы,
  * деградация (что делать, если индекс временно отстаёт).

Это не замена шардированию, а **надстройка**:
шарды всё так же режутся по одному ключу,
а routing index подсказывает, какие из них стоит дергать для сложных фильтров.

### 2.4.3.5. Cross-Shard Transactions: 2PC vs Saga (1–1.5 минуты)

Инстинкт такой:

> “Ну ладно, шарды-шмарды, сейчас сделаем `BEGIN` на двух шардах и закоммитим. Что может пойти не так?”

Ответ: **всё**.

Классический 2PC (two-phase commit) в распределённой системе:

* вешает **глобальные блокировки**;
* вводит **координатор**, смерть которого превращает кластер в кирпич;
* тянет latency до самого медленного участника;
* резко усложняет систему — это уровень Spanner/Cockroach, а не обычного продуктового проекта.

Поэтому в реальной жизни почти всегда делают наоборот:

> **Не пытаются сделать одну большую атомарную транзакцию на весь кластер,
> а превращают её в цепочку шагов — workflow.**

Если снова вернуться к маркетплейсу:

* покупатель оплачивает заказ;
* деньги должны списаться с его платёжного инструмента;
* часть уйти на баланс продавца, часть — в комиссию площадки;
* возможно, нужно начислить кешбек.

Баланс покупателя, баланс продавца и бухгалтерский учёт легко разъезжаются по разным шардам/сервисам.
Это и есть типичный cross-shard кейс, где мы хотим “ощущение одной транзакции” поверх нескольких шардов.

Это и есть идея **Saga**:

* сложная операция режется на шаги (примерно: `reserveMoney`, `reserveInventory`, `createOrder`…);
* у каждого шага есть **компенсация** (`releaseMoney`, `releaseInventory`, `cancelOrder`);
* если где-то посередине всё падает — мы не “откатываем весь мир”,
  а вызываем компенсирующие действия к уже выполненным шагам.

Часто рядом живёт паттерн **Outbox + CDC**:

* бизнес-изменение и “событие” пишутся в одну локальную транзакцию;
* отдельный процесс/CDC гарантированно вываливает это событие в очередь/шину;
* так мы не теряем сообщения между шардами/сервисами.

**Детально Saga, orchestration vs choreography, Outbox/CDC, 2PC
мы разберём уже в следующей части — про eventual consistency и распределённые транзакции.**

[//]: # (citus умеет в кроссшард транзакции. или в между базами тоже пойдет?)

---

### 2.4.3.6. Pre-Aggregation (MapReduce стиль) (2 минуты)

Если глобальные агрегаты неизбежны —
самый дешёвый запрос — тот, который уже посчитан.

Идея:

* вместо того чтобы каждый раз бегать по всем шардам,
  мы считаем **частичные агрегаты заранее**.

Примеры для нашего маркетплейса:

* суммарные лайки/просмотры товара;
* количество заказов в день по продавцу;
* оборот по продавцу за час/день;
* топ-100 товаров в категории по продажам.

Как это обычно делают:

1. **На каждом шарде** есть свои маленькие агрегатные таблицы:

   * `post_views_hourly(post_id, bucket_start, views)`
   * `sales_per_seller_per_day(seller_id, day, amount)`

   Они обновляются:
   * либо прямо на write-path’е (`INSERT ... ON CONFLICT DO UPDATE`),
   * либо периодической джобой, которая раз в N минут делает `INSERT ... SELECT ... GROUP BY`,
   * или через **стрим-процессор**:
     * события (`PostViewed`, `OrderCreated`) летят в Kafka/стрим;
     * отдельный воркер / Flink / KStreams крутит по ним счётчики и заливает в эти агрегатные таблицы.

2. **Центральный сервис/джоба** периодически склеивает это в глобальную картину:
   * забирает агрегаты со всех шардов,
   * делает `SUM/COUNT` по ним,
   * пишет результат в одну “глобальную” таблицу / кеш / ClickHouse.

Плюсы:

* нагрузка плавно размазана по времени;
* глобальный запрос “покажи метрику” превращается в чтение из маленькой таблицы/кеша;
* это хорошо комбинируется с кешированием и материализованными вьюхами.

Минусы:

* данные чуть отстают от реального времени;
* нужен pipeline, который всё это считает и не падает.

Но в 99% продовых сценариев
“глобальные агрегаты, обновляемые раз в минуту/5 минут”
— это нормальный и очень выгодный компромисс.

---

### 2.4.3.7. Кэширование cross-shard запросов (1–2 минуты)

И, конечно, **кеш**.

Где он особенно полезен:

* глобальные / тяжёлые cross-shard запросы;
* популярные фильтры и топы;
* отчётность.

Техники:

* кешировать **partial-результаты** на шардe:

    * каждый шард хранит свои агрегаты / свои “топы”,
      а merge-узел только их склеивает;
* кешировать **уже склеенный результат** на уровне API:

    * “топ-100 товаров по продажам” → обновляем раз в N секунд;
* использовать **materialized views** / precomputed buckets:

    * ~“данные за 1 минуту”, “за 1 час”, “за день”.

Пример:

> Топ-100 товаров в категории вполне можно обновлять раз в минуту.
> Никто из пользователей не заметит разницы между “3 секунды назад” и “30 секунд назад”,
> а база — заметит.

---

## 2.4.4. Анти-паттерны (1 минута)

Быстрый список того, что гарантированно стреляет в ногу.

### JOIN через REST другого сервиса

“Нет JOIN’ов, мы же микросервисы, мы просто ходим REST-запросами.”

Реально это:

* distributed N+1;
* куча сетевых hop’ов;
* очень высокая latency и сложный дебаг.

Часто хуже, чем один честный cross-shard в базе.

### Scatter-gather на все шарды всегда

“Нам лень думать, просто делаем fan-out на 64 шарда каждый раз.”

Результат:

* tail latency улетает в космос;
* любой проблемный шард делает больно всем запросам.

### ACID между шардами “как в монолите”

“Мы же можем сделать 2PC / XA, что может пойти не так?”

Ответ: **всё**.
Это уровень Spanner/Cockroach с атомными часовыми и мозгами ядра.

### Клиент знает шард

“Мы закопаем логику маршрутизации в клиента:
он сам знает, на какой шард ходить.”

Это убивает:

* динамическое шардирование;
* rebalancing;
* любые изменения layout’а.

Поменять схему → обновить всех клиентов → удачи.

---

## 5. Hot keys — отдельная категория боли (1 минута)

Отдельно стоит напомнить про **hot keys**:

* даже если шардирование хорошее,
  один безумно горячий ключ (или tenant) может перегреть один шард;
* любой cross-shard, зависящий от этого шарда, начнёт страдать.

Лечится уже знакомыми техниками:

* **microsharding** — дробим ответственность по этому ключу/tenant’у на несколько микрошардов;
* вынесение в **отдельный tier** — специальный кластер под горячие ключи;
* агрессивный **кеш** и локальные счетчики;
* **throttling / token bucket** — один клиент не должен иметь возможности сжечь весь кластер.

[//]: # (много раз встречается)

---

## 6. Закрытие и мостик к следующей теме (30 секунд)

Собираем всё в одну картинку:

* Cross-shard операции — **не баг**, а нормальная цена масштабируемости.
* Задача инженера — не “убить их всех”,
  а **минимизировать их количество и стоимость**:

    * правильным shard key,
    * денормализацией,
    * routing-индексами,
    * pre-aggregation,
    * кешированием,
    * и в крайнем случае — fan-out’ом по уму.

И важный вывод:

> Даже если вы идеально оптимизировали cross-shard операции,
> всё развалится, если один шард будет хронически перегружен.

Поэтому следующая тема логична:

**Rebalancing & Online Migrations** —
как таскать данные между шардами без даунтайма и без фейерверка из падений.

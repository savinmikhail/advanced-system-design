## 2.4.5. Rebalancing и миграции без downtime

### 2.4.5.1. Зачем вообще нужен rebalancing

Мы уже частично затронули проблему перемещения данных между шардами в главе о динамическом шардировании и почему статическое шардирование нас не устраивает,
теперь поговорим об этом подробнее

Rebalancing — это ответ на один вопрос:

> “Как вернуть **здоровое распределение данных и нагрузки** по кластерам, не роняя прод?”

Почему layout со временем перестаёт быть адекватным?:

1. **Неравномерный рост данных (data skew)**  
   Один шард распух до терабайта, остальные по 100 ГБ.  
   → **Resharding внутри кластера**: переразбиваем диапазоны / бакеты между теми же нодами  
   (например, `[0..1M]` → `[0..500k]` + `[500k..1M]`).

2. **Горячие пользователи / категории / tenant’ы**  
   Один шард получает 80–90% RPS.  
   → **Targeted relocation / hot-tenant migration**: вывозим конкретные диапазоны или tenant’ов  
   на отдельные ноды / кластеры.

3. **Рост кластера (upscaling)**  
   Добавили новые шарды → их нужно заполнить частью существующих данных.  
   → **Upscaling rebalancing**: переносим часть диапазонов / бакетов на новые ноды.

4. **Сжатие кластера (downscaling)**  
   Убрали железо / хотим сократить расходы.  
   → **Downscaling rebalancing**: сливаем данные с “лишних” шардов на оставшиеся.

5. **Смена shard key (меняется доменная модель)**  
   Резали по `userId`, поняли, что правильнее по `sellerId` или `tenantId`.  
   → **Shard-key migration**: переносим данные в новую схему шардирования  
   (старый layout больше не нужен, дальше живём в новой модели).

Во всех этих сценариях миграции идут **на живой нагрузке** — без длительного downtime и без потерь/дубликатов,  
и именно это делает тему rebalancing одной из сложных частей шардирования.

## 2.4.5.3. Основные проблемы при миграции

Прежде чем обсуждать техники ребаланса, обозначим какие проблемы нас могут встретить в этом процессе

1. **Неконсистентность данных во время переноса**

   Если будем просто копировать строки, то

    * часть данных может оказаться только на старом;
    * часть — только на новом;
    * что-то задублируется.

2. **In-flight writes**

   Пока вы копируете 500 ГБ, в базу продолжают прилетать новые записи:

    * кто-то обновляет профиль;
    * кто-то создаёт заказ;
    * кто-то меняет статус.

   Если эти изменения не учесть — новый шард при переключении будет отставать.

3. **race conditions**

   Одна часть кода читает из старого, другая — уже из нового.
   Клиент делает два запроса подряд — и вдруг видит разные данные.

4. **Atomic cutover**

   Момент переключения со старого на новый шард должен быть

    * быстрым,
    * атомарным в пределах системы,
    * откатываемым.

5. **Нагрузка на сеть и базу**

   Неконтролируемый backfill (переливка) под нагрузкой может положить:

    * старый шард по диску/процессору
    * новый шард,
    * сеть

6. **Кеши**

   кеш может продолжить держать старые данные:

    * старые shard map’ы;
    * старые значения по ключам;
    * стейт в приложении.

   В итоге часть запросов идёт уже по новой схеме, часть — по старой.

Все это надо учитывать заранее, пофиксить будет проблематично

## 6. Общая структура online-миграции

схемы rebalancing’а обычно состоят из трех этапов

1. **Перенос bulk-данных (backfill)**
   Перекачиваем основной объём из старого layout’а в новый.

2. **Синхронизация изменений (CDC / журнал изменений)**
   Пока идёт backfill, фиксируем все новые изменения и доносим их до нового шарда.

3. **Переключение маршрутизации (cutover)**
   Переводим чтение/запись на новый layout, минимизируя окно гонок.

Примерная схема:

![Схема миграции между шарадами](assets/01-migration-flow.png)

## 7. Dual Write

Первое, что приходит в голову (и что часто предлагают на собесах):

> “Давайте просто будем писать и туда, и туда, а потом аккуратно выключим старое”.

Алгоритм:

1. При каждом изменении пишем **одновременно** в старый и в новый шард.
2. Чтения какое-то время идут всё ещё из старого.
3. Когда уверены, что новый шард догнал — переворачиваем чтения на новый.
4. Старый шард либо выключаем, либо оставляем как бэкап и затем чистим.

Звучит просто. На практике:

* расхождение во времени:

    * запись в старый прошла, в новый отвалилась → данные расходятся;
* двойные инкременты / side-effects:

    * если апдейты не идемпотентные — легко сделать “+2” вместо “+1”;
* падение одной из сторон:

    * старый шард жив, новый нет / наоборот — консистентности нет.

Итог:

> Dual write иногда сгодится, но как базовое решение для серьёзной миграции — так себе идея.

Для собеса от вас обычно ждут не кода dual write, а объяснения, **почему это больно**:
расхождение времён записи, неидемпотентные апдейты, падение одной из сторон и отсутствие единого источника истины.

---

## 8–9. Другие базовые схемы

Есть ещё несколько “полегче, чем CDC” подходов, которые часто всплывают в обсуждениях.

**“Write to new, read from old” + мигратор.**

* Все новые записи сразу идут в новый шард.
* Чтения пока выполняем из старого, чтобы “истина” была в одном месте.
* Фоновый мигратор по диапазонам/tenant’ам догоняет данные в новом шарде, после чего чтения переключаем.
* Плюс: нет двойной записи, понятный rollback.
* Минус: нужно аккуратно чистить расхождения, больше ручной логики.

**“Routing First”.**

* Сначала меняем shard-map/routing: новые запросы сразу летят в новый layout.
* Старый шард держит хвост (холодные данные), фоновый процесс постепенно выносит их.
* Плюс: почти нет downtime, горячие ключи переезжают сразу.
* Минус: сложнее рассуждать о консистентности, нужен fallback “если на новом не нашли — смотрим на старом”.

Для собеса обычно достаточно уметь на словах сравнить эти схемы с CDC+backfill и объяснить, где вы бы выбрали простую миграцию с мигратором, а где уже нужен тяжёлый артиллерийский паттерн.

---

## 2.4.5.4. Канонический сценарий: CDC + Backfill

[//]: # (переместить блок)

Это “взрослый” паттерн, на котором живут большие ребята (Uber, Pinterest, Shopify и т.п.),
когда мигрируют сотни гигабайт и миллиарды записей без даунтайма и потерь.

Алгоритм:

1. **Backfill (snapshot)**
   Берём большой “снимок” данных:

    * читаем старый шард по чанкам;
    * заливаем всё в новый layout;
    * при этом согласовываем нагрузку, чтобы не убить базу.

2. **CDC (Change Data Capture)**
   Параллельно включаем слежение за изменениями:

    * из журнала транзакций / Debezium / logical replication;
    * каждое изменение (`INSERT/UPDATE/DELETE`) превращается в событие.

3. **Migrator**
   Отдельный воркер читает эти события и **повторяет** изменения на новом шарде:

    * гарантированная доставка;
    * корректный порядок для каждого ключа (по offset/LSN).

4. **Дожидаемся, пока лаг станет ≈ 0**
   То есть все изменения со старого уже догнаны в новый.

5. **Переключаем чтения и записи**
   Роутер/фичефлаг переводит трафик на новый layout.

6. Старый шард превращается в архив / временный бэкап, позже чистится.

Плюсы:

* отлично работает на больших объёмах;
* чёткая модель консистентности;
* нет потерь данных при in-flight writes.

Минусы:

* инфраструктурно сложнее:

    * нужен брокер (Kafka/Pulsar) или логическая репликация;
    * нужен надёжный мигратор;
    * нужно мониторить лаг/отставание.

Это, по сути, общий паттерн, который используют крупные компании для “миграций на миллиарды записей”.

Здесь хорошо работает одна эволюционирующая картинка (её можно рисовать/анимировать по шагам):

1. **Состояние “до”** — только старый шард/layout, все стрелки чтения/записи идут туда.
2. **Backfill** — появляется новый шард, от старого толстая стрелка “копирования” чанками данных.
3. **CDC stream** — к backfill добавляется стрелка “stream изменений” (журнал, Debezium, logical replication) от старого к новому.
4. **Cutover** — стрелки чтения/записи разворачиваются на новый layout, старый становится “подписью” как архив/backup.

---

## 2.4.5.5. Live Traffic Replay 

Есть ещё подход **Live Traffic Replay** — это уже hardcore уровня Amazon/Yandex.
Суть в том, что вы реплеите реальный продовый трафик в новый кластер параллельно со старым,
сравниваете состояние/ответы, и только после этого переключаете пользователей.

Для собеса достаточно понимать идею:

> “Мы гоняем реальный трафик в новый layout, проверяем, что он ведёт себя так же, как старый,
> и используем это как последнюю проверку перед cutover”.

В деталях это нужно только тем, кто реально строит свои базы/хранилища, а не “просто” шардирует Postgres.

---

## 2.4.5.6. Atomic cutover

Как бы мы ни переносили данные, всегда останется один ключевой момент:

> “С какой секунды мы считаем новым источником правды **новый layout**?”

Этот момент должен быть:

* коротким;
* контролируемым;
* откатываемым.

Приёмы:

1. **Feature flag**

    * В коде есть переключатель `useNewShardLayout`.
    * Пока он `false` — все чтения/записи идут по старой схеме.
    * Переключаем флаг — и весь трафик идёт по новой карте.

2. **Поколения (generation / epoch)**

    * У shard-map’а есть версия: `generation = 17`.
    * Клиент/роутер помечает запросы `X-Shard-Generation: 17`.
    * Когда переключаем layout на `generation = 18`,
      старый layout продолжает жить какое-то время, но новые запросы уже идут по 18-й версии.
    * Это помогает избежать гонок между разными экземплярами приложения.

[//]: # (как мы эпоху то прокинем?)

3. **Короткий freeze писателей**

    * На очень маленькое окно (десятки миллисекунд) можно остановить запись:

        * через блокировку в БД;
        * через флаг в роутере.
    * За это время меняем shard-map / переключаем координатор.
    * Пользователь на таком окне обычно ничего не замечает, если это <100ms.

4. **Write fencing**

    * Запись проходит только если версия совпадает:

      ```sql
      UPDATE users
      SET ...
      WHERE id = :id AND generation = 18;
      ```

    * Всё, что прилетело “не в ту эпоху” — либо отклоняется, либо переповторяется.

Общая идея:

> Cutover — это не “нажать одну волшебную кнопку”,
> а аккуратно продуманный переход с возможностью быстро откатиться назад.

---

## 2.4.5.7. In-flight writes

Отдельно проговорим проблему:

* пока идёт миграция, система не стоит на паузе;
* новые записи и апдейты продолжают прилетать.

Если их не учитывать, получаем:

* “дырки” в новых шардах;
* дубли;
* странные расхождения, которые вылезут через неделю в отчётах.

Техники, которые помогают:

* **идемпотентные операции**:

    * чтобы повторное применение “того же” события не ломало данные;
* **write fencing / generation checks**:

    * писать только в правильный layout;
* **drain очередей**:

    * перед финальным cutover вычищаем очереди задач/сообщений;
* **короткие freeze-окна для критичных операций**:

    * лучше 50ms легкого фриза, чем вечный бардак в состояниях.

---

## 2.4.5.8. Post-migration cleanup

После успешного cutover работа не заканчивается.

Нужно:

* либо сразу убрать старый шард, либо через некоторое время когда убедимся что все нормально работает
* убедиться, что кеши не держат старые маршруты и значения;
* собрать метрики до/после:
    * latency,
    * error rate,
    * нагрузка на новые шарды.

## 2.4.5.9. Анти-паттерны

Короткий список того, что почти всегда заканчивается плохо.

1. **Полная остановка сервиса “на время миграции”**

    * “Мы ночью всё выключим, покрутим SQL, утром включим”.
    * В реальном бизнесе — это час+ простоя, с неясным исходом.

2. **Один гигантский SQL-скрипт**

    * `ALTER TABLE`, `INSERT INTO ... SELECT ...` на сотни гигабайт одним махом.
    * Любая ошибка — и ты откатываешься не к “минус 5 секунд”, а к “минус ночь”.

3. **Клиент знает, на какой шард идти**

    * Шардирование зашито в клиентские приложения / мобильники.
    * Любой rebalancing теперь = обновить весь зоопарк клиентов.

4. **“Сначала вырубим старый шард, потом перенесём данные”**

    * В этот момент у тебя нет “источника правды”.
    * Любой косяк в переносе → данные ушли навсегда.

---

## 2.4.5.10. Итоги и мостик

В итоге

* Неизбежно наступит момент, когда layout надо будет менять:

    * из-за роста,
    * из-за ошибок в shard key,
    * из-за новых требований бизнеса.

Поэтому важно знать как это делать и быть готовым

## 2.4.5. Rebalancing и миграции без downtime

### 2.4.5.1. Зачем вообще нужен rebalancing

Мы уже частично затронули проблему перемещения данных между шардами в главе о динамическом шардировании и почему статическое шардирование нас не устраивает,
теперь поговорим об этом подробнее

Rebalancing — это ответ на один вопрос:

> “Как вернуть **здоровое распределение данных и нагрузки** по кластерам, не роняя прод?”

Сначала добавим новый термин
Layout шардирования — это текущая схема “ключ → шард”:
выбор shard key, способ резки (range/hash/buckets) и shard-map, который говорит, какие данные лежат на каких нодах.

Так вот, почему layout со временем перестаёт быть актуальным?:

1. **Неравномерный рост данных (data skew)**  
   Один шард разросся до терабайта, остальные по 100 ГБ.  
   → **Resharding внутри кластера**: переразбиваем диапазоны / бакеты между теми же нодами  
   (например, `[0..1M]` → `[0..500k]` + `[500k..1M]`).

2. **Горячие пользователи / категории / tenant’ы**  
   Один шард получает 80–90% RPS.  
   → **Targeted relocation / hot-tenant migration**: вывозим конкретные диапазоны или tenant’ов  
   на отдельные ноды / кластеры.

3. **Рост кластера (upscaling)**  
   Добавили новые шарды → их нужно заполнить частью существующих данных.  
   → **Upscaling rebalancing**: переносим часть диапазонов / бакетов на новые ноды.

4. **Сжатие кластера (downscaling)**  
   Убрали железо / хотим сократить расходы.  
   → **Downscaling rebalancing**: сливаем данные с “лишних” шардов на оставшиеся.

5. **Смена shard key (меняется доменная модель)**  
   Резали по `userId`, поняли, что правильнее по `sellerId` или `tenantId`.  
   → **Shard-key migration**: переносим данные в новую схему шардирования  
   (старый layout больше не нужен, дальше живём в новой модели).

Во всех этих сценариях миграции идут **на живой нагрузке** — без длительного downtime и без потерь/дубликатов,  
и именно это делает тему rebalancing одной из сложных частей шардирования.

## 2.4.5.3. Основные проблемы при миграции

Прежде чем обсуждать техники ребаланса, обозначим какие проблемы нас могут встретить в этом процессе

1. **Неконсистентность данных во время переноса**

   Если будем просто копировать строки, то

    * часть данных может оказаться только на старом;
    * часть — только на новом;
    * что-то задублируется.

2. **In-flight writes**

   Пока вы копируете 500 ГБ, в базу продолжают прилетать новые записи:

    * кто-то обновляет профиль;
    * кто-то создаёт заказ;
    * кто-то меняет статус.

   Если эти изменения не учесть — новый шард при переключении будет отставать.

3. **race conditions**

   Одна часть кода читает из старого, другая — уже из нового.
   Клиент делает два запроса подряд — и вдруг видит разные данные.

4. **Atomic cutover**

   Момент переключения со старого на новый шард должен быть

    * быстрым,
    * атомарным в пределах системы,
    * откатываемым.

5. **Нагрузка на сеть и базу**

   Неконтролируемый backfill (переливка) под нагрузкой может положить:

    * старый шард по диску/процессору
    * новый шард,
    * сеть

6. **Кеши**

   кеш может продолжить держать старые данные:

    * старые shard map’ы;
    * старые значения по ключам;
    * стейт в приложении.

   В итоге часть запросов идёт уже по новой схеме, часть — по старой.

Все это надо учитывать заранее, пофиксить будет проблематично

## 6. Общая структура online-миграции

схемы rebalancing’а обычно состоят из трех этапов

1. **Перенос bulk-данных (backfill)**
   Перекачиваем основной объём из старого layout’а в новый.

2. **Синхронизация изменений (CDC / журнал изменений)**
   Пока идёт backfill, фиксируем все новые изменения и доносим их до нового шарда.

3. **Переключение маршрутизации (cutover)**
   Переводим чтение/запись на новый layout, минимизируя окно гонок.

Примерная схема:

![Схема миграции между шарадами](assets/01-migration-flow.png)

Разберём по шагам.

1. Первый шаг - **Backfill (переливание)**  
   * читаем данные из старого layout чанками;
   * заливаем их в новый;
   * ограничиваем скорость, чтобы не уронить прод.
   * логируем прогресс (offsет/LSN/последний id).
   
   Результат: новый layout содержит всю историю до момента начала миграции.

2. Пока идёт backfill, в старую базу продолжают прилетать изменения.
   Чтобы их не потерять, включаем CDC(Change Data Capture), например Debezium, читаем WAL-логи, каждое `INSERT/UPDATE/DELETE` превращаем в событие.

3. В это время отдельный воркер (Migrator) читает CDC-стрим и **реплеит** изменения в новый layout.
    Благодаря такому подходу у нас есть
   * гарантированная доставка;
   * правильный порядок событий для каждого ключа, например по LSN(Log Sequence Number) из  WAL-журнала ;
   Но нужна idempotent-обработка, чтобы повторы не ломали данные.
   постепенно новый layout  догоняет старый по всем in-flight writes.

4. Благодаря LSN мы можем сверять отставание в записях между layout, и когда оно примерно равно нулю
    
5. drain очередей - перед cutover вычищаем очереди задач/сообщений, которые пишут в старый layout;

6. Мы переходим к стадии **Cutover**. Для этого роутер/координатор переводит чтения/записи на новый layout;

   * feature-flag `useNewShardLayout`:

       * `false` — все запросы идут по старой схеме,
       * `true` — все запросы идут по новой;
   * поколения (generation/epoch) shard-map’а:

       * у карты layout’а есть версия,
       * клиенты/роутеры работают с конкретной версией,
       * при смене поколения можно держать оба layout’а короткое время;
   * короткий freeze на запись:

       * на десятки миллисекунд блокируем writes,
       * обновляем shard-map, снимаем блокировку;
   * write fencing:

       * запись проходит только при совпадении версии (`WHERE generation = 18`),
       * всё “из старой эпохи” либо отклоняется, либо переповторяется.

7. Через некоторое время после стабилизации мы
   * отключаем мигратор и CDC для данного сценария;
   * чистим старый layout или оставляем как холодный архив.

Плюсы такого подхода:

* работает на сотнях гигабайт и миллиардах строк;
* модель консистентности прозрачная;
* in-flight writes не теряются.

Минусы:

* инфраструктурно тяжелее:

   * нужен CDC/репликация или брокер;
   * нужен мигратор с retry/идемпотентностью;
   * нужно мониторить лаг.

### Упрощённые варианты, которые вы услышите

Иногда вместо полноценного CDC используют более простые схемы:

* **Dual write**
  Пишем в старый и новый layout после backfill.
  Проблемы: нет журнала, нет гарантии порядка, легко получить рассинхрон и двойные апдейты.
  Годится только для маленьких, не критичных данных.

* **Write to new, read from old + мигратор**
  Новые записи идут в новый layout, читаем ещё из старого, фоновый мигратор подтягивает историю.
  Работает, если объёмы умеренные и можно пережить мелкие расхождения.

* **Routing first**
  Сначала меняем shard-map, горячие ключи сразу начинают жить в новом layout,
  хвост медленно вывозим фоном. Удобно для разгрузки горячих шардов, но сложнее reasoning по консистентности.

Ключевая мысль:  
эти техники — не “другие стратегии миграции”, а упрощённые реализации отдельных шагов
в общей схеме `backfill → догоняем изменения → cutover`.

### 2.4.5.10. Итоги

Рано или поздно layout придётся менять:

* из-за роста;
* из-за косяков в выборе shard key;
* из-за смены продуктовых требований.

Нормальная стратегия:

* иметь базовое понимание **CDC + backfill** как канона;
* осознанно пользоваться упрощёнными схемами (dual write, write-to-new, routing-first) там, где масштаб и риск это позволяют;
* проектировать cutover и работу с in-flight writes так же серьёзно, как и саму схему шардирования.

Шардирование — это не только “как порезать”, но и “как потом это перерезать и не убить прод”.

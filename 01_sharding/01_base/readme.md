# 2.2. Основы шардирования: shard key, hot keys, skew, routing

## Часть 1: Базовые понятия (1–2 минуты)

Чтобы понимать стратегии, определимся с базовой терминологией.

### 1. Shard Key

Shard key — это ответ на вопрос:
**“по какому признаку мы режем данные и решаем, на какой шард пойдёт запрос?”**

Примеры:

* `user_id`
* `tenant_id` (идентификатор клиента/компании в мульти-арендном SaaS)
* `(country, city)`
* временной диапазон (`created_at`)

От выбора shard key зависит почти всё: баланс нагрузки, сложность запросов, цена миграций.

Что делает shard key **хорошим**:

* он более-менее равномерно размазывает нагрузку и объём данных;
* типичные бизнес-операции влезают **в один шард** (или в небольшое число);
* ключ стабильный (редко меняется), его легко пробросить через сервисы.

Что ломает **плохой shard key**:

1. **Перекос нагрузки (skew) и горячие шарды**

* Режем по `tenant_id`, а один enterprise-клиент даёт 50% всего трафика.  
  → этот tenant сидит на одном шарде и жарит его, остальные скучают.
* Режем по диапазону `user_id`, а “старые” активные пользователи оказались в одном диапазоне.  
  → один “горячий” диапазон/шард + куча холодного хлама.

2. **Кросс-шард там, где по смыслу должно быть локально**

* Режем по `user_id`, а бизнес-операции крутятся вокруг `shop_id` или `project_id`.  
  → любой отчёт по магазину собирает данные со **всех** шардов (scatter-gather).
* Режем по `country`, а в рамках одной компании (tenant) есть команды из разных стран.  
  → простая операция “показать все данные клиента” превращается в квест по шардам.

3. **Невозможность эволюции**

* Шардимся по полю, которое меняется: `region`, `tariff`, `status`.  
  → каждый “переезд” пользователя между регионами/тарифами — это физическая миграция между шардами.
* Используем составной shard key из трёх полей, половина сервисов его не может нормально прокинуть.  
  → начинаются костыли вида “у нас тут отдельная табличка/сервис, чтобы узнать, куда вообще идти”.

4. **Запросы не попадают в шард**

* Популярные запросы **не содержат** shard key.  
  Например, шардимся по `user_id`, а бегаем по `email` или `phone`.  
  → чтобы ответить на запрос, приходится трогать все шарды: либо seq scan, либо index scan в каждом.

Итого:

> Shard key — это не “рандомное поле из таблицы”.  
> Это компромисс между балансом нагрузки, локальностью бизнес-операций и тем, какие запросы вы реально будете выполнять.

Дальше, когда будем разбирать стратегии (consistent hashing, directory-based схемы, microsharding, hot-key isolation), мы всё время будем возвращаться к этому вопросу:  
**выбранный shard key помогает системе жить или гарантирует вам постоянные кросс-шардовые проблемы?**

### 2. Hot keys и skew нагрузки

**Hot key** — это ключ, который генерирует **несоразмерно большой** трафик по сравнению с остальными.

Примеры:

* видео в трендах → 100k RPS на один `video_id`;
* “звёздный” продавец, по которому постоянно считают метрики;
* топовый игровой сервер / гильдия / чат-канал.

Даже если shard key в целом выбран нормально, один такой ключ может фактически
**превратить целый шард в bottleneck**:

* CPU, диск, коннекты — всё упирается в обработку одного `video_id` / `tenant_id`;
* остальные шарды живут спокойно, но пользователям от этого не легче — всё упирается в горячий шард.

Это подводит нас к следующему понятию.

**Skew (перекос нагрузки)** — это когда нагрузка и данные по шардам распределились криво:

* shard1 — 80% RPS,
* shard2–4 — по 5–7% и скучают.

Hot key — частный случай причины skew’а: один ключ перегревает один шард.  
Но skew может возникать и без явно выраженного hot key:

* один крупный `tenant_id` даёт половину всех запросов;
* диапазон ID или регион, на который внезапно приходится почти весь трафик;
* маркетинговая кампания, которая цепляет только часть шардов.

**Почему это беда:**

* масштабируешь кластер “симметрично”, добавляя узлы, а основная боль остаётся на одном–двух шардах;
* средний RPS на кластер выглядит красиво, а p95/p99 latency для запросов,
  попадающих на перегретые шарды, улетает в космос;
* любое падение горячего шарда может спровоцировать каскадный отказ:
  часть трафика переезжает на соседей → они тоже начинают задыхаться.

**Что с этим обычно делают (намётки, детали дальше по сценарию):**

* **hot-key isolation** — вынос особо горячих ключей (или tenants) на отдельные шарды/кластера;
* **microsharding** — дробим большие шарды на много мелких логических кусочков, чтобы нагрузку можно было перераспределять тонко;
* **умный routing** (latency-aware, load-aware) — чтобы не забивать уже страдающие шарды;
* **дополнительный кеш / fan-out через очередь** — не бить по базе/ядру на каждый запрос к hot key;
* **rate limiting / backpressure** — один шумный клиент не должен иметь возможности сжечь кластер.

Важно: ни hot keys, ни skew не лечатся “ещё одним `% N`”.  
Их нужно **видеть по метрикам** и целенаправленно разгружать архитектурными решениями.

---

## Часть 2.5. Где живёт роутинг (1–2 минуты)

В прошлой секции мы показали shard map / таблицу маршрутизации.

Сейчас зафиксируем, **где вообще может жить логика “куда идти за этим ключом”**.

Есть три верхнеуровневых варианта.

---

### 1) Routing в приложении (самый честный и самый частый)

Самый приземлённый вариант:

* приложение держит **несколько DSN** (`db_shard_1`, `db_shard_2`, …);
* есть слой `ShardRouter`, который по `userId` / `tenantId` выбирает нужный коннект;
* вся логика:
    * какой ключ на каком шарде,
    * как делать кросс-шардовые операции,
    * как мигрировать между шардами —
      лежит в коде приложения.

С точки зрения Symfony это выглядит так:

* в конфиге — несколько коннектов к разным базам;
* в сервисе — что-то вроде:

```php
  $conn = $router->forUser($userId); // выбираем нужный shard
  $conn->executeQuery('SELECT ...');
```

Плюсы:

* полный контроль, никаких чёрных ящиков;
* проще дебажить и понимать, что реально происходит.

Минусы:

* весь “мозг” шардирования — это ваш код;
* кросс-шардовые запросы и миграции придётся продумывать самим.

---

### 2) Pooler / простой proxy (pgbouncer, Odyssey, ProxySQL и т.п.)

Следующий слой — **инфраструктурные прокси/пулер**ы.

* Говорят на том же протоколе, что и клиент (Postgres / MySQL wire).
* Сидят между приложением и реальными базами.
* Умеют:

    * пулить коннекты;
    * балансировать по нескольким одинаковым backend’ам;
    * иногда разводить чтение/запись (master/replica) или разных пользователей по разным серверам.

Но важно:

* они **не знают ни про shard key, ни про layout данных**;
* не строят распределённый план запросов;
* не джойнят и не агрегируют данные с разных шардов;
* key-based шардинг (`user_id`, `tenant_id`) всё равно делает **либо приложение**, либо более умный слой.

Исторически pgpool-II пытался играть в “шардирование по колонке” (hash по `shard_key`),
но с кучей ограничений и боли. Сейчас это скорее legacy-подход, чем база для нового продового дизайна.

---

### 3) Координатор (распределённый движок: Citus, Vitess и т.п.)

Отдельный класс решений — **координаторы**, по сути распределённые движки поверх обычной СУБД.

* Для приложения это выглядит как **одна база и один DSN**.
* Координатор:

    * понимает схему: какие таблицы распределённые, какой shard key;
    * сам решает, какие шарды трогать для конкретного запроса;
    * строит **распределённый план**:

        * какие части запроса отправить на шарды;
        * где делать фильтрацию/агрегации;
        * где собирать и мёржить результат;
    * умеет работать с транзакциями в распределённом мире (в рамках своих ограничений).

Примеры:

* **Citus** поверх Postgres — де-факто лидер в Postgres-мире для шардинга и распределённых запросов;
* **Vitess** поверх MySQL — координатор, на котором живут большие кластера (включая YouTube).

Это уже **не “просто прокси”**, а по сути распределённая СУБД с координатором.

---

В этой главе, когда я говорю “routing-слой”, речь не про конкретный продукт, а про принцип:

> клиент знает один “вход”,
> а “куда на самом деле пойдут данные” решает отдельный слой —
> будь то application-level роутер, pooler + своя логика, или полноценный координатор типа Citus/Vitess.

А какой именно стек выбрать — это уже отдельный инженерный выбор с кучей trade-off’ов.

[//]: # (здесь бы ссылку на репозиторий с примером приложения с  динамическим шардированием чтоб можно было легко посмотреть/потрогать руками чтоб не оставалось магией)

# Блок 2.2. Основы шардирования: shard key, hot keys, skew, routing

## 2.2.1. Базовые понятия (1–2 минуты)

Прежде чем двигаться дальше, определимся с базовой терминологией - что такое shard key, hot key, skew, routing

### 1. Shard Key

Shard key — это ответ на вопрос:
**“по какому признаку мы режем данные и решаем, на какой шард пойдёт запрос?”**

Например:

* `user_id`
* `created_at`
* `region`

Выбор правильного shard key очень важен, от него будет зависеть баланс нагрузки, сложность запросов, цена миграций.

Что делает shard key хорошим?:

* он более-менее равномерно размазывает нагрузку и объём данных;
* типичные бизнес-операции выполняются **в одном шарде** (или в небольшом числе шардов);

К каким проблемам приводит неудачный выбор shard key?

1. **Перекос нагрузки (skew) и горячие шарды**

   * Режем по `tenant_id`, а один enterprise-клиент даёт 50% всего трафика → шард с этим ключом перегревается
   * Режем по диапазону `created_at` заказы, из-за чего шард с самым свежим диапазоном больше всего получает запросов и перегревается

2. **Кросс-шард там, где по смыслу должно быть локально**

   * Режем по `user_id`, а бизнес-операции крутятся вокруг `shop_id` или `project_id`.  
     → любой отчёт по магазину собирает данные со **всех** шардов (scatter-gather).

3. **Запросы не попадают в шард**

   * Популярные запросы **не содержат** shard key.  
     Например, шардимся по `user_id`, а бегаем по `email` или `phone`.  
     → чтобы ответить на запрос, приходится трогать все шарды

Таким образом:

> Shard key Это компромисс между балансом нагрузки, локальностью бизнес-операций и тем, какие запросы вы реально будете выполнять.

### 2. Hot keys и skew нагрузки

**Hot key** — это ключ, который генерирует **несоразмерно большой** трафик по сравнению с остальными.

Примеры:

* топовый продавец, по которому постоянно смотрят отзывы/товары
* популярный товар - лабубу какая-нибудь
* видео в трендах → 100k RPS на один `video_id`;

Даже если shard key в целом выбран нормально, один такой ключ может фактически
**превратить целый шард в bottleneck**

Такая ситуация называется Skew (перекос нагрузки),  когда нагрузка и данные по шардам распределились криво:

* shard1 — 80% RPS,
* shard2–4 — по 5–7% 

Hot key — частный случай причины skew’а
Но skew может возникать и без явно выраженного hot key:

Например, если мы имеем регион, на который внезапно приходится почти весь трафик

Почему skew это проблема?

* масштабируем кластер “симметрично”, добавляя узлы, а основная нагрузка остаётся на одном–двух шардах;
* средний RPS на кластер выглядит красиво, а p95/p99 latency для запросов,
  попадающих на перегретые шарды, сильно вырастают;
* любое падение горячего шарда может спровоцировать каскадный отказ:
  часть трафика переезжает на соседей → они тоже начинают отказывать.

**Что с этим можно сделать? есть несколько техник, сейчас кратко скажу, подробнее разберем позже**

* **hot-key isolation** — вынос особо горячих ключей (или tenants) на отдельные шарды/кластера;
* **microsharding** — дробим большие шарды на много мелких логических кусочков, чтобы нагрузку можно было перераспределять тонко;
* **умный routing** (latency-aware, load-aware) — чтобы не забивать уже страдающие шарды;
* **дополнительный кеш / fan-out через очередь** — не бить по базе/ядру на каждый запрос к hot key;
* **rate limiting / backpressure** — один шумный клиент не должен иметь возможности сжечь кластер.

Важно: hot keys, skew нужно **видеть по метрикам** и целенаправленно разгружать архитектурными решениями.

## 2.2.2. Где живёт роутинг (1–2 минуты)

Так как данные живут на разных серверах, логичный вопрос - кто и как поймет на какой сервер отправлять наш sql запрос

Есть три варианта: routing в приложении, прокси и координатор

---

### 1) Routing в приложении

* приложение знает **несколько DSN** (`db_shard_1`, `db_shard_2`, …);
* есть слой `ShardRouter`, который по `userId` / `tenantId` выбирает нужный коннект;
* вся логика лежит в коде приложения.:
    * какой ключ на каком шарде,
    * как делать кросс-шардовые операции,
    * как мигрировать между шардами

Плюсы:

* полный контроль, никаких чёрных ящиков;
* проще дебажить и понимать, что реально происходит.

Минусы:

* кросс-шардовые запросы и миграции придётся продумывать самим.

---

### 2) простой proxy 

Идея в том, что есть прослойка между приложением и базой которое умеет маршрутизировать запрос в тот или иной шард, но не умеет в кросс-шардовые запросы и другие сложные вещи
На деле же прокси часто используются для балансировки, но не для рутинга шардирования

Примерами могут быть pgbouncer, pgpool. второй даже чуть-чуть умел в шардирование

### 3) Координатор

Если же мы хотим чтобы наша прокся умела сама работать с шардированием, а приложение бы ничего о нем не знало, то нам нужен координатор
Также их называют распределенными 

Координатор:

* понимает схему: какие таблицы распределённые, какой shard key;
* сам решает, какие шарды трогать для конкретного запроса;
* строит **распределённый план**:

    * какие части запроса отправить на шарды;
    * где делать фильтрацию/агрегации;
    * где собирать и мёржить результат;
* умеет работать с транзакциями в распределённом мире (в рамках своих ограничений).

Примеры:

* **Citus** для Postgres 
* **Vitess** поверх MySQL

В этой главе, когда я говорю “routing-слой”, речь не про конкретный продукт, а про принцип:

> клиент знает один “вход”,
> а “куда на самом деле пойдут данные” решает отдельный слой —
> будь то application-level роутер, pooler + своя логика, или полноценный координатор типа Citus/Vitess.

Домашней работой вам будет на своем стеке реализовать шардирование. Шардируйте таблицу например заказов по хешу, разбор будет на бусти. там же я покажу пример directory map шардинга

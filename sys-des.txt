Оглавление
Глава 1. Что такое системный дизайн и кому он нужен?
Часть 1. Кому будет полезен курс?
Часть 2. Где и как используется системный дизайн?
Глава 2. Основы проектирования систем
Часть 1. Свойства систем
Часть 2. Требования к системам
Часть 3. Архитектура дизайн-систем
Часть 4. Масштабирование и нагрузка: базовая часть
Часть 5. Проектирование API
Глава 3. Продвинутое проектирование систем
1. Введение
2. Часть 1: Продвинутое кэширование и консистентность кэша
a. Cache Coherence в распределенных системах
b. Многоуровневое кэширование и оптимизации
c. Решение сложных проблем кэширования
3. Часть 2: Распределенные транзакции и консистентность данных
a. Distributed Transactions: от 2PC до Saga
b. Eventual Consistency
c. Strong Consistency без блокировок
4. Часть 3: Продвинутое шардирование и партиционирование
a. Dynamic Sharding стратегии
b. Cross-shard операции и оптимизации
c. Rebalancing и Migration без downtime
5. Часть 4: Производительность в распределенных системах
a. Tail Latency оптимизация
b. Throughput Scaling законы
c. Resource Optimization trade-offs
Глава 4. Системный дизайн в аналитике
1. Введение: От сборщика мусора требований к архитектору. Зачем вам это нужно? (5 минут)
2. Требования — фундамент для проектирования (10 минут)
3. Проектирование решения (20 мин)
4. Собеседования (5 мин)
Заключение (2 мин)
Глава 5. Системный дизайн в backend-разработке
Часть 1. Системный дизайн на собеседованиях и в работе backend-разработчика
Роль бэкенд-разработчика в системном дизайне
Основные зоны ответственности (это можно раскрывать построчно, останавливаясь на каждом пункте)
Собеседования по системному дизайну для бэкенд-разработчиков
По каким критериям оценивают вас интервьюеры?
Как готовиться и вести себя на собеседовании
Часть 2. Масштабирование и нагрузка: продвинутая часть
Измерения масштабируемости
Продвинутые паттерны горизонтального масштабирования
Декомпозиция сервисов
Владение данными
Безопасность состояния (Stateless Architecture)
Управление сессиями
Обработка запросов
Масштабирование баз данных
Горизонтальное масштабирование (Sharding)
Репликация чтения
Кэширование в масштабах системы
Архитектуры для асинхронной обработки
Взаимодействие микросервисов
Работа под нагрузкой
Глобальные системы
Наблюдаемость и мониторинг
Оптимизация стоимости системы
Часть 3. Балансировка нагрузки
Роль балансировки в архитектуре системы
Стратегии размещения балансировщиков
Алгоритмы балансировки и их применение
Round Robin и Weighted Round Robin
Least Connections и Weighted Least Connections
Алгоритмы по времени отклика и адаптивные
Consistent Hashing
Паттерны работы с сессиями
Health-checking и механизмы failover
Балансировка нагрузки в микросервисах
Географическая балансировка
Балансировка и базы данных
Оптимизация производительности и мониторинг
Расширенные паттерны
Диагностика и устранение проблем
Выводы
Часть 4. Хранение данных
1. Основные требования к системам хранения данных
2. Шаги проектирования системы хранения данных
Практические советы при проектировании хранения данных
Выводы
Глава 6. Системный дизайн во frontend-разработке
Часть 1. Системный дизайн на собеседованиях и в работе frontend-разработчика  6.1.1. Про SD и какую задачу вам могут дать   6.1.2. Зачем проводят секцию SD у фронтенд разработчиков?  6.1.3. Как проходит собеседование у фронтенд-разработчиков  Понимание поставленной задачи  Высокоуровневый дизайн  Погружение
Часть 2. Архитектура  6.2.1. Что такое архитектура и зачем она нужна?  6.2.2. Что можно отнести к архитектуре фронтенда?  6.2.3. Выбор стэка  6.2.4. Выбор режима рендеринга  6.2.4. Выбор архитектуры и методологий  6.2.5. Готовая дизайн-система или пишем свою?  6.2.6. Архитектурные подходы к разделению проекта  6.2.7. BFF (Backend For Frontend)  6.2.8. UI/UX и Доступность
Часть 3. Оптимизация производительности  6.3.0. О чем эта часть?  6.3.1. Оптимизация изображений  6.3.2. Оптимизация пагинации  6.3.3. Бесконечный скролл  6.3.4. Виртуализация  6.3.5. Optimistic Update  6.3.6. Нормализация данных  6.3.7. Нормализация данных  6.3.8. Debounce  6.3.9. Graceful Degradation
Часть 4. Тесты и метрики  6.4.0. О чем эта часть?
6.4.1. Зачем вообще нужны тест?  6.4.2. Нужно ли писать тесты?  6.4.3. Как покрыть frontend-приложение тестами?  6.4.4. Как организовать процесс контроля качества приложения?  6.4.5. Системы мониторинга (Prometheus, Grafana, Sentry)  6.4.6. Какие метрики важны
Глава 7. Системный дизайн в мобильной разработке
Часть 1. Системный дизайн на собеседованиях и в работе мобильного разработчика
Часть 2. API и Rest API
Часть 3. Архитектура мобильного приложения
Часть 4. Backend-Driven Development (BDUI)
Глава 8. Подготовка к интервью
Часть 1. Разбор популярных вопросов по системному дизайну
Часть 2. Ошибки на собеседованиях

Глава 1. Что такое системный дизайн и кому он нужен?
Спикер: Антон

Прогрев (первые 30 секунд)
// Почему это важно? Почему это лучше чем у конкурентов, что получит человек который пройдет этот курс?
Часть 1. Кому будет полезен курс?
Описание части: кратко поясняем, что за системный дизайн, почему он нужен всем (почти), как строится курс.

Наверняка ты знаком со Slack – мессенджером, которые многие компании, такие как OpenAI или Spotify, используют для общения внутри. Но слышал ли ты о том, как плохой системный дизайн Slack’a привел к падению их акций на 14%, а квартальной выручки – на 5% всего за один день? 
29 июля 2019 года Slack пережил глобальный сбой, продолжавшийся более 2 часов. Пользователи по всему миру не могли отправлять и получать сообщения. В итоге компания потеряла около $8,2 млн дохода на компенсации клиентам.
Для монтажа:

Причины сбоя были в серии неудачных архитектурных решений:
во-первых, из-за централизованной архитектуры была затронута большая часть пользователей;
во-вторых, нагрузка на базы данных была распределена неравномерно;
в-третьих, были недооценены риски перегрузки отдельных компонентов, которые потянули за собой вниз другие сервисы;
в-четвертых, система просто не умела работать в ограниченном режиме.
Даже в успешном сервисе небольшой перекос в архитектуре может стоить миллионов долларов за пару часов. И в этом видео ты узнаешь, как избежать этого.

MAANG уже давно добавил system design для middle+ как отдельный этап наряду с кодингом и поведенческим интервью. 
Биг тех все чаще ставит секцию по системному дизайну отдельно наряду с другими техническими этапами, 
По сути, тебе дают задачу типа “Спроектируй систему уведомлений для Тиндера”, и ты рисуешь архитектуру нужного тебе куска Тиндера на онлайн-доске, попутно объясняя, что и зачем там нужно. 
Также тебя могут попросить спроектировать совершенно новый сервис или аналог уже существующего продукта. Менее частый случай – проработка гипотетических ситуаций, когда тебе предлагается спроектировать систему, которая будет бесконечно масштабироваться.

Главное — не нарисовать “красивую картинку”, а показать, что ты понимаешь:
	•	первое – как собирать и уточнять требования;
	•	второе – на какие архитектурные компромиссы идти (например, сделать систему максимально быстрой для отдельного запроса или дать ей возможность обрабатывать огромное количество запросов одновременно);
	•	третье – какие технологии и подходы подходят под задачу.

Поэтому “Научите меня системному дизайну” – запрос, который почти никогда не встречается отдельно, но при этом почти всегда возникает при работе с нашими менторами. 
Проблемы с процессами на разных стадиях разработки, кривая архитектура сервиса, невозможность масштабирования, да и, в конце концов, system design interview, которое ждет тебя, если ты претендуешь на позицию middle+ в IT – это то, с чем поможет это видео.
Кому это нужно:
	•	backend-разработчикам уровня middle+ (часто обязательно для senior);
	•	тимлидам, архитекторам, менеджерам — для принятия архитектурных решений;
	•	DevOps/SRE — для понимания отказоустойчивости и масштабирования;
	•	аналитикам — в части формализации требований и взаимодействия с архитектурой;
	•	Frontend-разработчикам — для работы с API и распределенными системами;
	•	мобильным разработчикам – для упрощения масштабирования продукта и внедрения нового функционалов.

Именно по этому мы построили это видео таким образом: сначала пара глав, общих для всех направлений – будь ты мобильщиком или менеджером, они будут тебе полезны и понятны.
 Дальше идут главы применительно к каждой отдельной специальности. Их можно смотреть подряд – это будет полезно потенциальным и настоящим лидам и менеджерам, а можно выбрать те, которые интересуют больше всего.

Иногда будут встречаться домашки для отработки навыков. Конечно, ты можешь не делать их, но если готовишься к интервью и уж тем более хочешь применять эти навыки в работе, то рекомендую сначала попрактиковаться.

И немного об авторах курса: это Senior Go Developer, который строил архитектуру в “Авито”, Senior Backend Developer, старший системный аналитик из “Альфа-Банка”, бэкенд-разработчик из одного большого классифайда (кажется, он не афиширует) и техлид из Premier.One.
Часть 2. Где и как используется системный дизайн?
Описание части: раскрываем тему того, как часто встречается использование системного дизайна в IT.

Теперь к самому системному дизайну. Правильно будет начать с определения: системный дизайн – это процесс проектирования архитектуры ПО и инфраструктуры с учетом всех их компонентов и взаимодействия этих компонентов, 
а также требований по функциональности, масштабируемости, надежности, безопасности и стоимости. Системой может являться, например, приложение или сервис.
Что включает в себя системный дизайн:
	•	снятие требований;
	•	выбор технологии и методов реализации архитектуры;
	•	непосредственно проектирование архитектуры системы.

Прикладное значение system design в разработке достаточно очевидно – он помогает создавать более эффективные и устойчивые решения, которые легко масштабируются, соответствуют потребностям бизнеса и пользователей.
Например, когда Amazon начал показывать рост пользователей и заказов, то столкнулся с проблемой: их монолитный сайт уже не справлялся с нагрузкой. Тогда появилось новое бизнес-требование – масштабируемость и надежность при нагрузке в миллионы пользователей.
Amazon сделал следующее:
	•	разделил систему на независимые сервисы – в дальнейшем это стали называть микросервисной архитектурой;
	•	простроил масштабируемые базы и очереди;
	•	уделил отдельное внимание отказоустойчивости и надежности.
Amazon не просто выдержал рост, но превратил свои наработки в отдельный бизнес — AWS, который сегодня приносит десятки миллиардов долларов.

А теперь разберем подробнее каждый компонент системного дизайна в разработке.
1. Сбор требований На этом этапе мы определяем, что именно нужно системе, а сами требования делятся на функциональные и нефункциональные:
	•	функциональные требования – то, что система должна делать, какие функции предоставлять пользователю. Пример таких требований для условного онлайн-сервиса выгула котов – это функционал бронирования, уведомление хозяина о начале и окончании прогулки; 
	•	нефункциональные требования – это качества системы, которые описывают, как система выполняет функции. Они отвечают на вопросы: насколько быстро/надежно/удобно и так далее. Для того же сервиса они могут быть следующими: система должна обрабатывать до 10 000 одновременных запросов от пользователей,  поддерживать рост пользователей до 1 миллиона без перезапуска сервисов, возможность подключения внешних API для карт. 
2. Архитектура системы Здесь мы рисуем общую схему — какие компоненты будут и как они связаны. Эта схема показывает, как система устроена, чтобы выполнять функции и соответствовать нефункциональным требованиям
	•	Например, мы можем нарисовать, как пользователь взаимодействует с фронтендом, далее фронтенд делает запросы к API, эти запросы направляются к нужному микросервису, микросервис общается с базой данных и/или кэшем, а результаты возвращаются через API на фронтенд.
Для монтажа можно типа такого примера архитектуры:
 
3. Выбор технологий На данном этапе мы подбираем конкретные инструменты под задачу, чтобы детализировать нашу схему. Подробнее о том, как это выглядит на собеседовании и что именно от тебя ожидают, ты можешь посмотреть в последней главе.
	•	Здесь нужна конкретика, если твоя специализация это позволяет, но углубляться в детали не стоит. Например, в рамках нашего проекта мы можем решить, что фронт у нас будет на Vue.js, бэкенд на Go, база PostgreSQL. 
4. Масштабируемость (Scalability) Теперь переходим к качествам системы. Масштабируемость необходима для того, чтобы наша система выдерживала рост нагрузки.
	•	Например, сейчас сервис обрабатывает 1000 заказов в час, но архитектура должна позволять горизонтальное масштабирование до 100k заказов без полной переработки самой системы. 
5. Надёжность и отказоустойчивость (Reliability & Availability) Это означает, что система должна работать даже при сбоях.
	•	Например, если один датацентр выходит из строя, запросы автоматически перенаправляются в другой. 
6. Безопасность (Security) Здесь мы учитываем защиту данных и доступов.
	•	Например, мы можем определить, что все соединения будут через HTTPS, данные пользователей шифруются, доступ к API ограничен ролями, есть двухфакторная аутентификация. 
7. Производительность (Performance) Тут все просто: система должна отвечать быстро и эффективно использовать ресурсы.
	•	Например, часто запрашиваемые данные (в нашем случае – история заказов) могут храниться в кэше – это поможет сократить задержки. 
8. Управление данными (Data management) Здесь мы решаем, как храним, обрабатываем и защищаем данные.
	•	Пример решения здесь: все пользовательские данные хранятся в PostgreSQL, есть регулярное резервное копирование и репликация для отказоустойчивости. 
9. Интеграция (Integration) В этом блоке соединяем разные части системы и внешние сервисы.
	•	Например, наш сервис может интегрироваься с платёжной системой через API. 
10. Наблюдаемость (Observability) Здесь мы говорим о том, как контролируем работу системы.
	•	Пространство для действий здесь достаточно большое, вплоть до того, что можем настроить алерты через Telegram-бота. 
11. Тестирование (Testing) Тут проверяем, что система работает правильно при разных сценариях.
	•	Классический пример – нагрузочное тестирование на 10x от реальной нагрузки, юнит-тесты на бизнес-логику, интеграционные тесты API. 
12. Документирование (Documentation)  Так нелюбимое многими – описываем архитектуру и ключевые решения.
	•	Например, RFC или гайд для новых разработчиков. 

На самом деле, в разных источниках можно найти разный перечень, часто без пунктов про тестирование и документирование. Мы взяли наиболее исчерпывающий, включающий в себя процессные шаги, которые я описывал в первых трех пунктах, и системные качества, которые шли далее. Позднее мы еще расскажем, какие компоненты обычно вызывают наибольший интерес у интервьюеров и какие нужны именно тебе.

Так или иначе, из этого огромного списка становится понятно, что в процессе построения и реализации системы задействован не один архитектор или девопс

 а разные участники команды разработки: некоторые этапы практически полностью завязаны на фронте, другие – на тестировщике, третьи – на бэкендере и так далее. 

Про снятие требований мы конкретно в этом списке не упомянули, но именно эта часть лежит на аналитике. И крайне важно понимать, как именно ты можешь использовать system design в своей работе и за что отвечаешь.

Если все еще сложно и непонятно, зачем все это нужно, есть кейс, который считается примером хорошего system design – WhatsApp. На ранних этапах (ещё до покупки Meta) WhatsApp поддерживался очень маленькой командой инженеров — в 2015 году СМИ писали, что сервис с сотнями миллионов пользователей обслуживало около 50 разработчиков. Это стало легендой про то, как грамотная архитектура позволяет обслуживать колоссальные нагрузки малыми силами.

Но важно понимать, что это — исторический контекст. Сегодня WhatsApp — это огромная экосистема внутри Meta: текстовые и голосовые сообщения, видеозвонки, стикеры, группы на тысячи участников, платежи, интеграции с Instagram и Facebook. Для такой системы нужны уже сотни или тысячи инженеров, хотя точные цифры компания не раскрывает.

С инженерной точки зрения WhatsApp того периода интересен тем, что:
	•	хранит сообщения в распределенной системе;
	•	доставляет их с минимальной задержкой даже при плохом интернете;
	•	обеспечивает надёжность доставки (офлайн-сообщения доходят, когда пользователь снова в сети);
	•	применяет end-to-end шифрование;
	•	масштабируется до миллиардов пользователей. 
Именно поэтому WhatsApp часто используют как учебный пример: он сочетает в себе простую идею (обмен сообщениями) и сложную реализацию на уровне глобальной распределенной системы.

В следующей главе ты узнаешь о том, как тебе самому разработать первую систему
но сейчас, когда мы в самом начале, представь, что ты уже на собеседовании
тебе уже задали вопрос из серии “Спроектируй онлайн-сервис выгула котов”, и уже ждут от тебя действий. 
Первое, что тебе надо будет сделать – собрать формализованные требования. Я предлагаю тебе записать, какие вопросы ты задал бы интервьюеру на этом этапе, когда тебе надо понять, что именно от тебя требуется, и попробовать повторить это упражнение после просмотра видео до конца. 
Кстати, подробный разбор того, как собирать требования с интервьюера, чтобы они действительно были полезны при прохождении интервью, можно посмотреть на моем Бусти.

И перед тем, как мы перейдем к основам проектирования систем, предлагаю тебе поделиться, сталкивался ли ты уже с секцией по system design, и поставить лайк, чтобы я знал, что курс тебе действительно интересен.
Глава 2. Основы проектирования систем
Спикер: Нияз (с 05.11 – Инара)
Часть 1. Свойства систем
Я Нияз, GO-разработчик и ментор. В этой главе расскажу про Основы проектирования систем. Будет 5 частей: поговорим про свойства систем, требования к систем, архитектура систем дизайна, Масштабирование и нагрузка и проектирование API   
Когда ты начинаешь проектировать любую систему, первое с чем ты сталкиваешься - это ее свойства. Это не про то, на каком языке ты пишешь или какую базу ты возьмешь. Это про то, каким должен быть сам сервис: быстрым, надежным, защищенным, готовым к росту нагрузки.  Возьмем наш сервис выгула котов. Пользователь открывает приложение и нажимает “заказать выгул кота” и ждет, что все сработает без сбоев. Вот какие свойства важны:

	•	Масштабируемость
Сегодня у нас 100 заказов в день, а завтра может быть 10 000. Система должна выдерживать рост и не ломаться Пример: если котоводы внезапно массово приходят в приложение в выходные, система должна без проблем распределять нагрузку по серверам
	•	Производительность
Пользователь не будет ждать полминуты, пока найдется выгульщик. Ответ нужен быстро Пример: поиск ближайшего выгуливателя должен занимать миллисекунды, иначе пользователь закроет приложение
	•	Надежность
Данные должны сохраняться всегда. Если заказ уже создан, он не должен исчезнуть из-за ошибки Пример: хозяин нажал «заказать выгул» и даже если база перезапустилась, заказ не потеряется.
	•	Отказоустойчивость
Сервис должен работать, даже если что-то пошло не так
Пример: один сервер упал, но пользователь этого не заметил - их запросы ушли на резервный сервер
	•	Консистентность
Данные в системе должны совпадать
Пример: хозяин видит, что заказ подтвержден, и выгульщик тоже видит этот статус. Если статусы не совпадают, начинаются проблемы
	•	Безопасность
Данные пользователей и питомца должны быть защищены
Пример: информация о пользователях шифруется, авторизация только по токенам, платежи идут через защищенный канал

Все эти свойства никогда не бывают в максимуме сразу. Всегда приходится выбирать: скорость против надежности, консистетность против доступности. В этом и есть искусство системного дизайна - это находить баланс  


Часть 2. Требования к системам
Для начала нам нужно понять, что именно требуется от системы. На собеседовании по системному дизайну это первый шаг, задавать вопросы и уточнять требования.
В реальной работе требования не появляются сами по себе. Их нужно собрать у стейкхолдеров. Это люди, которые заинтересованы в системе:
	•	заказчики 
	•	пользователи (те, кто реально будет пользоваться) 
	•	техническая команда (разработчики, аналитики, DevOps, тестировщики),  
Как снимать требования:
	•	разговаривать с продуктом или заказчиком, чтобы понять бизнес-цели («что хотим получить и зачем»), 
	•	спрашивать пользователей про сценарии («как вы будете этим пользоваться»), 
	•	обсуждать с командой технические ограничения («что мы реально можем потянуть»). 
Какие вопросы стоит задавать:
	•	Сколько пользователей будет одновременно в системе? 
	•	Какая скорость отклика считается нормальной? 
	•	Что важнее: чтобы данные были всегда точные или чтобы система отвечала быстрее? 
	•	Какие действия обязательны для MVP, а какие можно отложить? 
	•	Какие законы или стандарты нужно учесть (например, хранение персональных данных)? 
Требования делятся на два типа:


Функциональные - что система должна уметь
Пример для нашего сервиса выгула котов:
	•	регистрация пользователей и создание профиля питомца
	•	поиск и выбор выгуливание рядом
	•	бронирование времени и места
	•	геотрекинг, чтобы хозяин видел, где гуляет кот
	•	уведомление о начале и конце выгула
	•	оплата внутри приложения
	•	отзывы и рейтинг выгуливателей

Нефункциональные - это система должна работать
Это то, что не видно пользователю, но критически важно
Примеры:
	•	выдерживать 2000 запросов в секунду в пиковое время
	•	SLA (доступность) 99.9
	•	задержка не более 1 с на поиск выгуливателя

Если не договориться о требованиях заранее, можно построить систему, которая работает, но не решает задачу Например представь, что мы сделали супербыструю систему поиска, но забыли про безопасность платежей. Или наоборот сделали железобетонный уровень защиты но каждое бронирование идет 10 секунд  Представь что ты сидишь на интервью, и тебе сказали “спроектируй сервис выгула котов”. Какие 5 вопросов ты задашь заказчику чтобы собрать требования?

ПЕРЕЗАПИСАНО В КОНЦЕ: Подумай, какие вопросы ты бы задал заказчику, например, я бы начал с количества пользователей. На бусти ты можешь посмотреть видео, в котором я подробно рассказываю о таких вопросах и том, что нам дают ответы на них

  Например, если бы мне дали задачу «спроектировать сервис выгула котов», я бы уточнил такие вещи:
	•	Сколько пользователей будет в пике?  Зачем спрашивать: это определяет, какую нагрузку должна выдерживать система — один сервер или десятки. 
	•	Нужно ли показывать геопозицию в реальном времени?  Зачем спрашивать: от этого зависит архитектура канала связи — обычный REST-запрос раз в минуту или WebSocket/стрим с постоянным соединением. 
	•	Какой способ оплаты критичен? Картой внутри приложения или можно внешней ссылкой?  Зачем спрашивать: это влияет на интеграцию с платёжными системами, безопасностью и сложностью разработки. 
	•	Какая задержка ответа считается нормальной для пользователя?  Зачем спрашивать: если допустимо ждать 2–3 секунды — можно делать медленнее, но дешевле; если нужно 200 мс — архитектура станет дороже и сложнее. 
	•	Нужно ли хранить историю прогулок и как долго?  Зачем спрашивать: если хранить долговременно — нужна устойчивая база и бэкапы; если можно удалять через неделю — инфраструктура проще и дешевле. 



Например какие бы я задал:
	•	Сколько пользователей будет в пике?
	•	Нужно ли показывать геопозицию в реальном времени
	•	Какой способ оплаты критичен?
	•	Какая задержка ответа считается нормальной?
	•	Нужно ли хранить историю прогулок?

Попробуй выписать свои вопросы. Это лучший способ прокачать навык работы с требованиями
Часть 3. Архитектура дизайн-систем
Когда мы собрали требования, пора переходить к самой интересной части - аритектуре.
Начинаем с простого и понятного, а потом усложняем, когда появляются реальные пользователи и нагрузка

Представь, что мы только запускаем сервис выгула котов. Нам нужно:

	•	Мобильное приложение или веб клиент
	•	Бэкэнд который отрабатывает запросы
	•	База данных, где лежат пользователи, питомцы и бронирования
	•	Кэш, чтобы ускорять часто повторяющиеся запросы
	•	нотификэйшн сервис, чтобы присылать пуши и смс
Такую систему можно собрать как один большой монолит. Все в одном приложении, одна база, один сервер. Но это быстро становится узким местом, один сервер падает - падает весь сервис.

Добавляем балансировку и несколько бэкэндов
Чтобы система была живой и надежной, мы не можем держать все на одном сервере. Если он упадёт — упадёт весь сервис. Поэтому делаем несколько копий бэкенда (реплики).
Перед ними ставим «распределителя запросов» - специальный сервер, который принимает все входящие обращения и решает, на какой из бэкендов их отправить. Такой распределитель называется балансировщиком нагрузки. Если говорить точнее, здесь нужен балансировщик седьмого уровня (L7). он работает на уровне HTTP-запросов. Например, nginx.
Балансировщик равномерно распределяет запросы между копиями бэкенда


Получается у нас:
Если один сервер упал, балансировщик уберет его из пула
Можно легко добавлять новые сервера при росте нагрузки

Разделение сервисов
Когда функционала становится больше, все держать в одном монолите становится неудобно.  Почему?  – Каждое изменение в коде требует пересобирать и выкатывать все приложение целиком.  – Командам сложно работать параллельно: изменения одного модуля могут случайно поломать другой.  – С ростом нагрузки трудно масштабировать выборочно: например, поиск выгульщиков нагружается сильнее, а авторизация почти не растёт — а в монолите приходится масштабировать всё сразу.
Поэтому систему делят на части — отдельные сервисы с понятной зоной ответственности:
	•	Auth - логин и всякие токены
	•	Users & Pets - пользователи и их питомцы
	•	Booking  - бронирования и статусы
	•	Mathing - поиск подходящего выгуливателя рядом
	•	Notification - пуши, смс, емаилы
	•	Payment - интеграция с платежками
API Gateway будет стоять перед ними и решать куда именно отправить запрос, проверять токены, ограничивать частоту запросов

Данные и кэш
 Основная база, например PostrgreSQL хранит все самое важное, пользователей брони и платежи
У Базы есть реплики для чтения, чтобы не нагружать главный экземпляр
Поверх этого используем Redis: туда кладем самые горячие данные - список ближайших выгульщиков, карточки пользователей, сессии. ТТЛ ставим небольшой
Фото и видео храним в объектном хранилище S3
Часть 4. Масштабирование и нагрузка: базовая часть
Одна из самых частых причин проблем в реальных проектах - рост нагрузки. Сервис работает отлично пока пользователей мало, но как только их становится больше, система начинает тормозить
Чтобы этого не случилось, при проектировании нужно сразу думать о масштабировании

Вертикальное и горизонтальное масштабирование
Вертикальное - поставить сервер помощнее: больше CPU, памяти, SSD. Это быстро и просто но предел есть всегда
Горизонтальное - добавить больше одинаковых серверов. Тут мы упираемся не в железо а в то как архитектура умеет делить нагрузку

Наш сервис выгула котов должен уметь горизонтально масштабироваться. Балансировщик распределяет запросы, новые реплики сервисов легко добавляются в пул

Чтение и запись
В любой системе нагрузка чтения всегда больше записей
Что делаем?
	•	Для чтений подключаем реплику базы и кэш Redis
	•	Записи всегда идут в основной экземпляр базы, чтобы не было рассинхрона

Репликация и шардинг
Репликация
Давай представим, у нас есть одна тетрадь (база данных), куда мы записываем все бронирования. Если все читают из этой тетради одновременно, она быстро изнашиваются
Нам нужно сделать копии тетради
	•	В одну (главную) мы пишем новые записи
	•	Из копий все только читают
Так нагрузка делится: писать все равно нужно в одно место, но читать можно хоть с десяти копий сразу. 

Шардинг
А теперь давай представим, что у нас не тетрадь а огромная книга на миллион страниц, и мы все время ищем в ней нужное бронирование. Это медленно и тяжело
Решение: разделить книгу на несколько томов
	•	В один кладем заказы из Москвы
	•	В другой из Казани
	•	в третий из Питера
Так каждая база обслуживает только свою часть пользователей и поиск работает быстрее
Часть 5. Проектирование API
АПИ - это способ, с помощью которого разные части разговаривают друг с другом. Можно сказать, что это язык общения между приложением и сервером. 

Давай представим это через наш сервис выгула котов.  Хозяин открывает приложение, нажимает на кнопку “заказать выгул” - и приложение делает запрос на сервер через API. Сервер отвечает: “Окей заказ создан вот его номер” Все что пользователь видит в приложении, приходит именно через API
 Как обычно строят API?
Сегодня чаще всего используют два подхода:

REST API
Запросы выглядят как адреса
Пример:
	•	GET /walkers - получить список выгуливателей
	•	POST /bookings - создать бронирование
	•	GET /bookings/123 - узнать статус бронирования номер 123

GRPC
Более технический формат, быстрый и бинарный протокол
Хорошо работает для общения сервис - сервис (backend - backend)
Но сложнее новичкам, поэтому в мобильных и веб приложениях чаще остается REST


Правила хорошего  API

Простота
Один и тот же тип запросов всегда работает одинаково
Например GET всегда получает данные, POST всегда что-то создает

Версионирование
API меняется, поэтому нужно указывать версию
/v1/bookings
/v2/bookings
Так старые клиенты продолжают работать, даже если мы добавили новые поля

Идемпотентность
Это страшное слово значит что если запрос повторить несколько раз, результат не будет меняться. Например пользователь нажал создать бронирование, интернет лаганул, и приложение отправило запрос дважды
Пример API для нашего сервиса

POST /v1/bookings - создать бронирование

GET /v1/bookings/{id} - посмотреть статус

POST /v1/payments/{booking_id}/confirm - подтвердить оплату

Глава 2. Основы проектирования систем
Спикер: Инара

ВСТУПЛЕНИЕ (2–3 мин)
  «Привет! Меня зовут Инара - системный аналитик и ментор, и сегодня мы поговорим про основы проектирования систем.  Когда ты создаёшь любой цифровой продукт — сайт, приложение или сервис — ты на самом деле проектируешь систему.  Это не только код или база данных. Это всё: от того, как пользователь нажимает кнопку, до того, как сервер отвечает, и как система выдерживает рост пользователей.
Чтобы сделать это наглядно, давай разберёмся на понятном примере — сервисе аренды электросамокатов whoosh, юрент или яндекс go.  Пользователь открывает приложение, видит ближайший самокат, арендует его, катается и возвращает.  Просто, правда? А теперь посмотрим, что за этим стоит.
ЧАСТЬ 1. СВОЙСТВА СИСТЕМ (6–8 мин)
Когда вы приступаете к проектированию новой системы, первым делом важно понять не технологии, а характер будущего сервиса.  Речь не о том, на чём вы напишете бэкенд или какую базу выберете — а о том, каким должен быть сам продукт:  устойчивым, быстрым, безопасным и способным справляться с ростом пользователей.
Представьте: пользователь открывает приложение whoosh, юрент или яндекс go, находит ближайший самокат и нажимает «Начать поездку».  Он ожидает, что всё сработает мгновенно: самокат разблокируется, таймер запустится, а деньги спишутся корректно.  Чтобы это происходило без сбоев, система должна обладать набором ключевых свойств, которые определяют её качество и поведение.

Масштабируемость
Система должна расти вместе с бизнесом.  Сегодня — тысяча пользователей, завтра — сто тысяч.  Серверы не должны падать от наплыва запросов, а инфраструктура — позволять быстро добавлять новые ресурсы.
Пример: если в солнечные выходные тысячи людей одновременно открывают приложение, система должна автоматически распределить нагрузку и продолжать работать стабильно.
Производительность
Пользователь не станет ждать полминуты, пока появится список самокатов.  Всё должно происходить быстро: поиск ближайшего транспорта, старт аренды, оплата.
Пример: отклик сервера при поиске самокатов должен занимать доли секунды — иначе пользователь просто закроет приложение.
Надёжность
Данные о поездке, оплате и статусе самоката должны сохраняться всегда, даже если где-то произошёл сбой.  Если пользователь уже начал аренду, она не должна исчезнуть при перезапуске сервера.
Отказоустойчивость
Даже если часть системы выходит из строя — сервис должен оставаться доступным.  Например, один из серверов может упасть, но пользователь этого не заметит, потому что запросы автоматически перенаправятся на резервный узел.
Консистентность
Информация внутри системы должна быть согласованной.  Пользователь, оператор и база данных должны видеть одни и те же статусы поездки.  Если приложение показывает, что самокат арендован, сервер не может считать его “свободным”.

Безопасность
Любая система обязана защищать данные пользователей — личные данные, платежи, маршруты.  Для этого применяются шифрование, токены доступа и защищённые каналы связи.

 Главное — нельзя сделать всё идеально сразу.  Между скоростью, надёжностью и стоимостью всегда приходится искать баланс.  И это — суть системного дизайна.
ЧАСТЬ 2. ТРЕБОВАНИЯ К СИСТЕМЕ (6–7 мин)  
Прежде чем что-то проектировать, нужно понять — что именно нужно пользователям и бизнесу.  Без этого можно построить систему, которая работает, но не решает задачу.
  Требования бывают двух типов: функциональные — что система делает, и нефункциональные — как она это делает.
Функциональные требования (что система делает):
	•	Регистрация пользователя
	•	Поиск ближайшего самоката
	•	Начало и завершение аренды
	•	Оплата
	•	История поездок 
Нефункциональные (как система работает):
	•	5000 запросов/сек
	•	SLA 99.9%
	•	Отклик не более 1 секунды 
 
 На собеседовании по системному дизайну тебе часто скажут:  "Спроектируй систему аренды самокатов".  И твоя первая реакция должна быть — задать вопросы.
Какие вопросы задать заказчику?
  Например:  1. Сколько пользователей будет одновременно? 2. Нужно ли показывать самокаты в реальном времени?  3. Какая задержка отклика допустима?  4. Какой способ оплаты обязателен? 5. Как долго хранить историю поездок?
Ответы на эти вопросы определяют не просто детали — а всю архитектуру.

ЧАСТЬ 3. АРХИТЕКТУРА СИСТЕМЫ (7–8 мин)
 Теперь, когда мы уже определили цели и требования к системе, пора перейти к самому интересному — архитектуре.  То есть к тому, как все элементы нашего сервиса связаны между собой и взаимодействуют.
Архитектура — это каркас системы.  Она определяет, как данные проходят путь от пользователя до базы и обратно, как распределяется нагрузка и как сервис справляется с ростом.
Начнём просто — потом постепенно будем усложнять, как это происходит в реальной жизни.


Этап 1. Монолит — всё в одном приложении
  Когда сервис только запускается, обычно никто не строит сложные схемы.  Для MVP — минимально жизнеспособного продукта — часто достаточно монолита.  Монолит — это единое приложение, где всё находится в одном кодовом проекте:  и логика авторизации, и работа с самокатами, и платежи, и уведомления.
Например, наш whoosh, юрент или яндекс go только запускается:  приложение отправляет запрос на сервер, сервер проверяет данные, записывает заказ в базу, и всё работает.
Плюсы монолита — простота разработки и быстрый запуск.  Минусы — низкая гибкость. Когда пользователей становится много, всё ложится на один сервер.  А любое изменение требует перекомпиляции и перезапуска всего приложения.
Это нормально для старта, но как только система растёт — она начинает “захлёбываться”.

 Этап 2. Балансировщик нагрузки — распределяем запросы
  Следующий шаг, когда пользователей становится больше, — добавить балансировщик нагрузки.
Балансировщик — это специальный сервер, который принимает все запросы от клиентов и распределяет их между несколькими копиями backend-приложения.
Теперь, если один сервер перегружен или упал, балансировщик перенаправляет запросы на другие экземпляры.  Для HTTP-приложений часто используется балансировщик 7-го уровня, например Nginx или AWS Elastic Load Balancer.
Такой подход делает систему устойчивее и позволяет горизонтально масштабироваться — просто добавляем новые копии сервера в пул.
Но всё ещё остаётся проблема: приложение остаётся единым.  То есть если что-то ломается в модуле платежей, оно может потянуть за собой остальное.  А значит — пора переходить на следующий этап.

Этап 3. Микросервисы — делим систему на части
Когда система вырастает, монолит перестаёт быть управляемым.  Поэтому мы делим её на микросервисы — отдельные самостоятельные модули, каждый со своей задачей.
В whoosh, юрент или яндекс go это может выглядеть так:

	•	 Auth — отвечает за регистрацию, вход, токены авторизации.
	•	 Users — хранит профили пользователей, их историю поездок.
	•	 Vehicles — управляет состоянием самокатов: где они находятся, какой заряд, свободен ли самокат.
	•	 Rentals — обрабатывает начало и конец аренды, фиксирует маршрут и стоимость.
	•	 Payments — отвечает за оплату и интеграцию с платёжными системами.
	•	Notifications — отправляет пуши, смс и email-уведомления.
	•	API Gateway — единая точка входа для клиентов, проверяет токены и направляет запросы в нужный сервис. 


 Так каждая команда может отвечать за свой кусочек.  Auth-разработчики не мешают Payments-команде, а изменения в Rentals не влияют на Notifications.  Можно обновлять, масштабировать и деплоить каждый модуль отдельно.
Например:  если в выходные количество аренд резко вырастает, можно просто добавить несколько копий Rentals,  не трогая остальные части системы.  Это и есть сила микросервисного подхода.

Хранение данных
  Теперь разберём, где хранятся все данные.
У whoosh, юрент или яндекс go, как и у большинства подобных систем, архитектура хранения выглядит примерно так:
	•	 PostgreSQL — основная база данных.  Хранит пользователей, заказы, транзакции.
	•	 Реплики — копии базы, только для чтения.  Они снимают нагрузку с основной базы:  чтения (например, получение истории поездок) идут с реплик,  а записи — в основной экземпляр.
	•	 Redis — кэш.  Используется для хранения сессий и часто запрашиваемых данных — например, список ближайших самокатов.  Это снижает нагрузку и ускоряет ответы API.  S3-хранилище (или аналог) — для файлов: фотографии, отчёты, чеки, скриншоты поездок. 
Такая комбинация делает систему быстрой и надёжной.  Даже если база временно перегружена, часть данных берётся из кэша,  а при сбое можно восстановиться с реплики.

 Взаимодействие между сервисами
 Как эти сервисы вообще “разговаривают” друг с другом?
Для простых случаев — через HTTP-запросы (REST или gRPC).  Для обмена событиями — через брокер сообщений, например Kafka или RabbitMQ.
Например:  сервис Rentals завершил аренду — отправляет событие rental_finished.  Это сообщение улавливает Payments, чтобы списать деньги, и Notifications, чтобы отправить чек.
Так мы избавляемся от жёстких зависимостей:  если один сервис временно недоступен, событие не теряется — очередь доставит его позже.

Архитектура в разрезе (резюме)
Итак, архитектура whoosh, юрент или яндекс go на зрелом этапе выглядит так:
Эта структура даёт:
	•	независимость сервисов,
	•	гибкое масштабирование,
	•	устойчивость к сбоям,
	•	и возможность развивать систему без полной остановки.» 
ЧАСТЬ 4. МАСШТАБИРОВАНИЕ И НАГРУЗКА (6–7 мин)
 На первых порах система работает прекрасно — сто пользователей, тысяча, даже десять тысяч.  Но вот приходит лето, начинается сезон самокатов, и пользователей whoosh, юрент или яндекс go становится сто тысяч.
Серверы начинают задыхаться: аренда открывается с задержкой, приложение подвисает, база не справляется с запросами.
Значит, настало время говорить о масштабировании — о том, как сделать так, чтобы система росла вместе с нагрузкой.»


Условно, если сегодня у нас 10 тысяч поездок в день, а завтра — миллион, система должна просто расшириться, а не переписываться заново.»
Вертикальное масштабирование — усиливаем железо
Самый очевидный способ начать — вертикальное масштабирование.  Это значит: взять сервер помощнее.
Добавляем больше процессоров, больше оперативной памяти, быстрые SSD-диски — и всё работает быстрее.
Например, whoosh, юрент или яндекс go может поднять базу данных PostgreSQL на сервере с 32 ядрами и 128 ГБ памяти,  и она начнёт справляться с большим числом запросов.
Плюс вертикального масштабирования — простота.  Минусы — очевидны: это дорого и имеет предел.  Рано или поздно вы упрётесь в физические возможности одного сервера.  И тогда приходит время настоящего масштабирования
 Горизонтальное масштабирование — растём вширь
Следующий уровень — горизонтальное масштабирование.  Вместо того чтобы усиливать один сервер, мы добавляем несколько и распределяем между ними нагрузку.
В whoosh, юрент или яндекс go это работает примерно так:
	•	Пользователь открывает приложение.
	•	Запрос идёт в балансировщик — например, Nginx или AWS ELB.
	•	Балансировщик выбирает наименее загруженный сервер backend и отправляет запрос туда.
Так нагрузка равномерно распределяется между несколькими экземплярами.  Если пользователей становится больше — мы просто добавляем ещё пару серверов в пул.»
Горизонтальное масштабирование — гибкий подход.  Он лежит в основе современных распределённых систем, от соцсетей до стриминговых сервисов.»
Репликация баз данных — разделяем чтение и запись
Окей, серверов стало больше, но узким местом может стать база данных.  Каждый запрос “посмотреть историю поездок” или “найти ближайшие самокаты” всё ещё идёт в одну точку.
Решение — репликация.
Главная база данных — Primary — принимает записи: аренды, платежи, регистрацию.  А копии — реплики — обслуживают чтение: список самокатов, история, отчёты.
Таким образом, мы разгружаем основную базу и ускоряем работу.
Например, при запросе “показать самокаты поблизости” whoosh, юрент или яндекс go обращается к реплике,  а при завершении аренды — к главной базе.
PostgreSQL отлично поддерживает такую модель.  А при сбое главной базы реплика может быстро стать новой “главной” — это повышает отказоустойчивость.»
Такой подход уже даёт системе возможность обслуживать сотни тысяч пользователей в реальном времени.
Шардинг — делим данные на части
Но что делать, если данных становится слишком много, и даже репликация не спасает?  Тогда приходит на помощь шардинг — разделение данных на независимые куски.
В whoosh, юрент или яндекс go можно, например, делить данные по регионам.
На экране:  Карта России с зонами: Москва → База 1, Санкт-Петербург → База 2, Казань → База 3.
  Москва хранится на одном кластере баз, Санкт-Петербург — на другом, Казань — на третьем.  Каждая база обслуживает только “своих” пользователей и самокаты.
Если в Москве пик нагрузки — это никак не влияет на Питер или Казань.
Шардинг уменьшает нагрузку на каждую отдельную базу и повышает общую устойчивость.  А главное — позволяет расти без предела:  можно просто добавлять новые шард-сервера для новых городов.
Главная сложность — это маршрутизация:  нужно понимать, в какой шард записывать данные и откуда их читать.  Обычно это решается через шардинг-ключ — например, city_id или region_code.

Масштабирование кэша и очередей
Не только базы нуждаются в масштабировании.  В whoosh, юрент или яндекс go также активно используется Redis — кэш для быстрых запросов.
Когда пользователей становится слишком много,  можно поднять кластер Redis — несколько узлов, распределяющих ключи по слотам.  Так кэш тоже масштабируется горизонтально.
А для обмена событиями между микросервисами — например, аренда завершена или платёж прошёл —  используется Kafka.  Kafka изначально масштабируется за счёт разделения сообщений на топики и партиции,  что позволяет обрабатывать миллионы событий в секунду.»

Авто-масштабирование и мониторинг
В современных облаках (AWS, GCP, Yandex Cloud) масштабирование может быть автоматическим.  Когда нагрузка растёт — автоматически поднимаются новые контейнеры или виртуальные машины.  Когда пользователей меньше — ресурсы освобождаются.
whoosh, юрент или яндекс go, например, может увеличивать количество backend-серверов в часы пик,  а ночью — уменьшать, чтобы не платить за простаивающие ресурсы.»
Чтобы всё это работало стабильно, нужно следить за метриками:
	•	загрузка CPU,
	•	время отклика,
	•	количество активных аренд,
	•	пропускная способность базы.
Инструменты вроде Prometheus + Grafana или New Relic помогают вовремя замечать проблемы.»
Итоги блока
Итак, масштабирование — это не просто “купить больше серверов”.  Это стратегия роста.
whoosh, юрент или яндекс go проходит типичный путь:  1️⃣ Сначала один сервер (монолит).  2️⃣ Потом балансировщик и несколько backend-экземпляров.  3️⃣ Затем репликация базы.  4️⃣ Потом шардинг по регионам.  5️⃣ И наконец — кластеры Redis, Kafka и авто-масштабирование.
Грамотно спроектированная система не боится роста.  Добавляются ресурсы — и всё просто продолжает работать.»
Масштабирование — это искусство делать систему гибкой,  чтобы рост пользователей воспринимался не как угроза,  а как естественное развитие.

ЧАСТЬ 5. ПРОЕКТИРОВАНИЕ API (5–6 мин)

API — это язык общения между приложением и сервером.  Каждое действие пользователя — от входа в аккаунт до старта аренды самоката —  на самом деле набор запросов, которые проходят через API.
Открыл приложение whoosh, юрент или яндекс go, нажал “Арендовать” — значит, отправил запрос на сервер.  Получил ответ — аренда началась, таймер запущен.  Так работает любая современная система.»
Что делает API
API — это мост между фронтендом и бэкендом.  Фронтенд ничего не знает про базу данных, микросервисы или архитектуру.  Он просто говорит:
“Вот запрос — дай мне список самокатов поблизости.”
А сервер отвечает:
“Вот они — координаты, модели, уровень заряда.”
Всё, что видит пользователь в интерфейсе, приходит именно через API.»
REST API — стандарт общения с клиентом
Самый распространённый формат — REST API.  Он строится на принципах HTTP и понятен почти любому разработчику.  Каждое действие выражается как запрос по определённому адресу:
GET /vehicles/nearby?lat=55.75&lon=37.61
→ получить список самокатов рядом

POST /rentals/start
→ начать аренду

GET /rentals/{id}
→ получить статус аренды

POST /rentals/{id}/finish
→ завершить аренду

REST прост, читабелен и легко тестируется.  Любой разработчик может открыть документацию и сразу понять, как всё устроено.  Поэтому REST чаще всего используется для связи клиента (мобильного приложения) с сервером.
Основные принципы хорошего API
Теперь разберём, каким должен быть хорошо спроектированный API.  Неважно, REST это или gRPC — принципы одинаковы.»
1 Простота и единообразие
Пользователь или разработчик должен понимать логику API без документации.  Если GET — значит получаем данные,  POST — создаём,  PUT — обновляем,  DELETE — удаляем.
А структура должна быть предсказуемой:  все ресурсы называются во множественном числе, а действия следуют шаблону.
GET /users/{id}
POST /rentals
DELETE /vehicles/{id}
Такой подход делает API логичным, а интеграции — быстрыми.
2️ Версионирование  API неизбежно развивается.  Добавляются новые поля, методы, меняются правила.  Чтобы старые клиенты не сломались, важно использовать версии.
/v1/rentals/start
/v2/rentals/start

Например, в /v1 аренда создаётся без скидок,  а в /v2 — уже с учётом промокодов.  Старое приложение продолжает работать, новое — использует обновлённый функционал.  Без версионирования — хаос и падения.»
3️ Идемпотентность
Слово страшное, но смысл простой:  если клиент повторит один и тот же запрос несколько раз —  результат должен быть одинаковым.
Представьте: пользователь нажимает “Начать аренду”,  но интернет на секунду завис, и запрос ушёл дважды.  Плохо, если система создаст две аренды и дважды спишет оплату.
Идемпотентный API защитит от этого:  система проверит идентификатор запроса и убедится,  что аренда уже существует.
Это делает API надёжным и безопасным даже при нестабильных сетях

4️ Безопасность
API — точка входа в систему.  Если её не защитить — злоумышленники смогут получить данные или подделать запросы.
Поэтому в whoosh, юрент или яндекс go каждый запрос сопровождается токеном авторизации.  API проверяет права пользователя:  может ли он начинать аренду, привязан ли самокат к этому аккаунту и т. д.
Плюс — все запросы проходят только по HTTPS,  а чувствительные данные (например, платежные токены) шифруются.  Такой подход защищает и пользователей, и сам бизнес.»

5️ Документирование и тестирование
Ещё один важный момент — документация API.  Если разработчик не понимает, как вызывать методы,  никакие архитектуры не спасут.
В whoosh, юрент или яндекс go, как и во многих компаниях, используется OpenAPI (Swagger):  это интерактивная документация, где можно нажать кнопку “Try it” и увидеть реальный ответ.
А тесты API — автоматические, чтобы при каждом изменении мы знали,  что ничего не сломалось.»
🎨 На экране:  Swagger UI с запросом /rentals/start.
API как связующая ткань системы
Когда система вырастает до десятков микросервисов,  API становится её “нервной системой”.
Фронтенд общается через внешний REST API,  внутри микросервисы связываются по gRPC,  а посредником выступает API Gateway.
Он проверяет токены, маршрутизирует запросы и собирает статистику.
Если API спроектирован грамотно, всё остальное — от логики аренды до аналитики —  работает предсказуемо и стабильно.
Итоги блока
API — это не просто техническая деталь, это фундамент взаимодействия.
В whoosh, юрент или яндекс go API обеспечивает:
	•	корректное общение клиента и сервера, 
	•	надёжную работу микросервисов, 
	•	безопасность и масштабируемость всей системы. 
Хорошо спроектированный API — это как идеальный мост:  незаметный, но жизненно важный.
 ЗАКЛЮЧЕНИЕ (2–3 мин)
 Проектирование систем — это искусство баланса.  Ты постоянно выбираешь: скорость или надёжность, простота или гибкость.
Главное — понимать, для кого и зачем ты строишь систему.  Начинай с требований, продумывай архитектуру, закладывай масштабирование и делай API понятным.
И тогда твой сервис, как и whoosh, юрент или яндекс go, будет стабильно работать, даже когда им пользуются тысячи людей.


Глава 3. Продвинутое проектирование систем
Спикер: Тимур

Вступление к 3 главе


Добро пожаловать в третью главу нашего курса. Меня зовут Тимур, я Senior разработчик и ментор.  В предыдущей главе мы построили фундамент — научились проектировать базовую архитектуру систем. Теперь пришло время углубиться в те вопросы, знание которых поможет вам глубже погрузиться в system design, расширить свой кругозор и начать применять эти знания в работе.


Сегодня мы познакомимся с понятием согласованности  данных в распределенной системе и поймем как ее можно обеспечивать, также узнаем о разных вариантах кэширования для снижения latency нашей системы. Эти знания вы сможете применять как и в работе, так и на system design интервью, когда будете углубляться в конкретные решения в вашей системе."

Согласованность
Вспомним термины из CAP теоремы
- Доступность означает, что  система отвечает на запросы, при этом не обязательно являются ли ответы валидными.
- Согласованность (консистентность) означает, что  пользователи не видят противоречащих друг другу данных.   
И ещё одно важное понятие, которое часто звучит «страшно», но я объясню просто —линеаризуемость:
Линеаризуемость — это свойство системы при котором она ведёт себя так, будто все операции исполняются по очереди в одном месте. Если запись подтвердилась, то все последующие чтения увидят именно эту запись.

Это самый строгий режим. Представь интернет-магазин с последней парой кроссовок. Двое — Алиса и Боб — пытаются купить их почти одновременно. Линеаризуемость гарантирует, что итог будет таким, как будто покупки обработали по одной в некотором порядке, уважая реальное время совершения операции: если покупка Алисы успела завершиться до начала покупки Боба, Боб не сможет купить товар.

Реальные системы чаще живут в компромиссах. Есть модели слабее:
- Причинная(causal) модель выглядит так — события, связанные причинно, видны в правильном порядке. Независимые события могут «обгонять» друг друга. В такси отзыв не появится раньше самой поездки.В социальной сети пользователь видит свой комментарий сразу, даже если его не видят другие пользователи.
- Конечная (eventual) модель она гласит— если новых записей нет, все копии данных «сойдутся» со временем. В промежутках разные пользователи могут видеть разные значения. 
    
    

Теперь давайте рассмотрим согласованность на примере реальной системы. В системе такси: заказ поездки затрагивает минимум 4 сервиса: сервис поездок, сервис поиска водителей, сервис платежей и уведомлений. У сервиса поездок и сервиса платежей отдельные базы данных. Как нам обеспечить согласованность нашей системы?


 


Явно распишем все инварианты нашей системы(сноска в видеоряде -  Инвариант — это правило/утверждение про состояние системы, которое **всегда должно быть истинно во всех допустимых состояниях. Мы запускаем систему так, чтобы инвариант был верен в начале, и проектируем каждую операцию так, чтобы она не могла его нарушить.). Это правила, которые система не имеет права нарушать ни при каких сбоях:

- У водителя не может быть двух активных заказов одновременно.
- Платёж либо проведен(зарезервирован), либо нет — промежуточного состояния не бывает.
- Нельзя начать поездку, если у пользователя на счету недостаточно средств.(тут для простоты картины мы считаем, что пользователь имеет внутренний счет, которым мы можем легко управлять)     
- Статусы переходят только вперёд по валидным шагам, например статус поездки может обновляться строго по порядку: поездка создана, ожидание водителя, ожидание оплаты, подтверждена. 
  
   Все остальные участки — допускают короткую рассинхронизацию.

Теперь рассмотрим сценарий начала поездки. Пользователь отправляет запрос на создание поездки, в это время наша система определяет стоимость поездки, резервирует деньги на счету, закрепляет водителя, подтверждает поездку и затем отправляет уведомление водителю и пассажиру с информацией о заказе. Все шаги, кроме отправки уведомления необходимо сделать атомарно, ведь исходя из наших инвариантов мы не можем позволить поездке начаться, если пользователь не способен за нее заплатить. Но данные находятся в разных сервисов. Тут проблему атомарности нам помогут решить  распределенные транзакции. Есть два популярных способа осуществить подобную транзакцию: двухфазный коммит и сага.

Принцип работы 2PC

Двухфазный коммит работает напрямую с храналищами, в нашем случае мы могли бы его реализовать с помощью оркестратора, имеющего доступ к базам данных всех нужных сервисов.
Как следует из названия, протокол выполняется в две фазы

Фаза подготовки (prepare phase, фаза голосования): Координатор рассылает всем участникам запрос на подготовку к коммиту. Каждый участник локально выполняет свою часть работы транзакции (например, делает необходимые изменения в своей БД) но не фиксирует их, а помечает как “готовые к коммиту” (в базах данных это обычно означает записать изменения в журнал и заблокировать ресурсы). После этого участник отвечает координатору либо “готов” (Yes), если его этап прошёл успешно и он готов зафиксировать, либо “не могу” (No), если произошла ошибка и он не сможет закоммитить. Все участники по сути голосуют “за” или “против” общей фиксации.

Фаза фиксации (commit phase): Координатор собирает ответы. Если все участники ответили "готов/Yes", то координатор посылает команду Commit всем участникам. Каждый участник получает эту команду и выполняет фиксацию своих изменений (окончательно применяет их в своей системе). Если хотя бы один участник ответил отказом ("No"), либо не ответил из-за сбоя, координатор посылает команду Rollback всем тем, кто был готов. В результате все участники отменят свои изменения (или не будут фиксировать). Таким образом, достигается атомарность: либо все commit, либо все rollback.

После завершения второй фазы координатор может сообщить инициатору (например, приложению, начавшему транзакцию), что транзакция успешно выполнена или откатилась.


Для реализации 2PC необходима поддержка со стороны всех участников. Каждый ресурс должен обладать интерфейсом приготовления/фиксации. В мире реляционных БД это стандарт XA; в NoSQL-хранилищах или пользовательских сервисах такой поддержки часто нет. Поэтому в чистых микросервисах, где сервисы используют разные хранилища, самостоятельно реализовывать 2PC сложно – нужно либо писать слой-адаптер для каждого (чтобы сервисы умели принимать команды prepare/commit), либо ограничиться теми ресурсами, что уже поддерживают XA. 

Ограничения и недостатки двухфазного коммита
Хотя 2PC гарантирует строгую согласованность, в распределённых системах он имеет известные недостатки:

Блокировка ресурсов и производительность: На фазе подготовки каждый участник обычно блокирует изменяемые ресурсы (например, строки в базе) до получения команды commit или rollback. Если участников много, и они ждут друг друга, это может затормозить систему. В случае задержек или сбоев ожидание может быть длительным, и заблокированные ресурсы недоступны другим транзакциям – снижая параллелизм. 2PC поэтому плохо масштабируется с ростом числа участников: задержка одного узла удерживает всех.

Точка отказа – координатор: Координатор транзакции – центральный “командир”. Если он выйдет из строя в неподходящий момент, участники останутся в подвешенном состоянии. Например, участники подготовились и ответили “Yes”, ждут команду, а координатор умер или потерял связь – они не знают, коммитить им или откатывать. Такая ситуация называется блокировкой (blocking) в 2PC. В базах данных участники в ожидании решения могут держать транзакцию открытой очень долго. Требуется администратор или специальный механизм восстановления координатора, чтобы решить судьбу транзакции. Существуют расширения (протокол трехфазного коммита, 3PC) для смягчения этой проблемы, но полностью без координатора 2PC не работает.

Оверхед на коммуникацию: 2PC требует два раунда сетевого взаимодействия ( prepare, затем commit/rollback). В условиях высокой нагрузки или сетевых задержек это добавляет значительный оверхед к каждой транзакции. Saga же, напротив, чаще работает асинхронно, и шаги могут выполняться без центральной синхронизации.

Слабая приспособленность к отказам связи: Если один из участников недоступен, координатор не получит от него “готов”, и вся транзакция должна откатиться (ради консистентности жертвуем доступностью). В микросервисах, где отказ отдельного сервиса не редкость, 2PC приводит к более частым отказам всей операции по сравнению с Saga, которая могла бы дождаться восстановления сервиса и продолжить (или выполнить компенсации).

Сложность внедрения в микросервисах: Применение 2PC между независимыми сервисами идёт вразрез с философией слабой связанности. Все участники должны быть “подчинены” общему координатору и протоколу. Это усложняет архитектуру – фактически получается распределённый монолит на уровне транзакций. Поэтому многие избегают 2PC в микросервисной архитектуре, предпочитая eventual consistency подходы (Saga, TCC и пр.).

Следует отметить, что 2PC остаётся полезным в ограниченных сценариях, где небольшое число участников и жёсткое требование атомарности. К примеру, финансовые операции между двумя банковскими системами могут использовать двухфазный коммит, если наличие центрального координатора приемлемо. Но во внутренней архитектуре современных веб-сервисов 2PC – редкий гость; чаще его заменяют на паттерны, обеспечивающие “логическую” атомарность, как Saga.




Saga – это паттерн, позволяющий достичь целостности данных в распределённых системах без использования глобальных ACID-транзакций. Идея состоит в том, чтобы разбить большую бизнес-транзакцию, затрагивающую несколько сервисов, на последовательность локальных транзакций в каждом из сервисов. Каждый шаг выполняется в рамках своего сервиса атомарно. Если все шаги прошли успешно – вся сага считается успешной. Если же на каком-то этапе произошла ошибка, паттерн Saga предусматривает выполнение компенсирующих транзакций для отмены уже выполненных действий и возврата системы в согласованное состояние.


Оркестрация vs хореография саги
Существуют два подхода к координации шагов саги: оркестрация и хореография. Разница в том, где заложена логика последовательности действий:

Оркестрацией управляет специальный центральный компонент – оркестратор саги. Оркестратор знает сценарий целиком и по шагам рассылает команды сервисам: что делать дальше. Он вызывает Service A, затем Service B, и т.д., и в случае ошибки также инициирует компенсирующие действия. Оркестратор упрощает понимание последовательности (вся логика сосредоточена в одном месте), но вводит дополнительный компонент и точку контроля. В нашем сценарии оркестратором может выступать сервис поездок, т.к он может знать все о сценарии создания поездки. Например сервис поездок создает при получении запроса на создание поездки создает оркестратор Саги, который начинает распределенную транзакцию:
	•	Создает поездку в собственной базе данных со статусом “created или pending”
	•	Отправляет запрос на поиск водителя в driver-service.
	•	Дожидается ответа и решает продолжать транзакцию или же откатить ее. В случае если водитель не был найден, в базе сервиса trip поездка помечается как canceled
	•	Если водитель был найден, обновляет в записи поездки информацию о водителе(например его id)
	•	Отправляет запрос в сервис платежей для того, чтобы зарезервировать средства клиента
	•	Если получилось зарезервировать, то помечает заказ как подтврежденный, одновременно с этим асинхронно отправляя сообщение в notification service. (не забыл  о паттерне transactional outbox). Если зарезервировать средства не получилось, то все предыдущие шаги откатываются. Водитель освобождается, заказ помечается как отмененный


При Хореографии нет центрального руководителя – каждый сервис сам реагирует на события и решает, что делать дальше. То есть взаимодействие основано на обмене сообщениями: выполнение шага одним сервисом генерирует событие, которое подхватывают другие сервисы, запускающие свои шаги. Например:
Все сервисы общаются через шину данных: 


Хореография исключает центральный координатор – система распределённая и более гибкая, но последовательность действий “размазана” по сервисам, что усложняет сопровождение и отладку.

Оба подхода широко используются. В простых системах часто хватает хореографии (реактивный подход), но для сложных процессов с множеством условий может быть удобнее оркестрация (явное прописывание сценария).

Преимущества паттерна Saga
Паттерн Saga стал популярным решением для согласованности данных между микросервисами благодаря нескольким сильным сторонам:

Отсутствие глобальных блокировок и распределённых ACID: Сага позволяет обойтись без двухфазного коммита и других тяжёлых механизмов. О двухфазном коммите поговорим позже. Каждый сервис работает в пределах своей транзакции, что упрощает масштабирование. Данные между сервисами синхронизируются через события/команды, а не через общий транзакционный менеджер.

Устойчивость к частичным сбоям: Если один из сервисов в процессе недоступен или операция не удалась, Saga может компенсировать уже выполненные действия. Система возвращается в консистентное состояние без ручного вмешательства. Это повышает отказоустойчивость (каждый локальный шаг откатывается независимо).

Гибкость бизнес-логики: Компенсационные действия не обязательно должны быть точной обратной операцией – это могут быть любые меры, удовлетворяющие бизнес-требованиям для восстановления целостности. Например, если отменить платёж нельзя автоматически, можно пометить заказ как требующий ручного вмешательства. Такая бизнес-ориентированная обработка ошибок невозможна при механическом ACID-откате.

Масштабируемость и производительность: Сервисы взаимодействуют асинхронно (особенно при хореографии), что позволяет им работать параллельно и не ждать друг друга, если логика позволяет. Нет удержания глобальных блокировок – повышается пропускная способность системы. В больших распределённых системах Saga масштабируется лучше, чем централизованный транзакционный координатор.


Сложности и подводные камни Saga
Несмотря на плюсы, реализация Saga приводит к ряду сложностей, о которых архитекторам и разработчикам нужно помнить:

Сложность программирования и компенсирующая логика: Разработчики должны явно продумывать и кодировать компенсирующие транзакции для каждого шага. Это добавляет работы и требует глубоко понять бизнес-процесс: что делать, если шаг X уже выполнен, а шаг Y провалился? Проектирование таких компенсаций порой непросто (например, «отменить платеж» – нетривиальная задача, если деньги уже списаны).

Идемпотентность операций: Критически важно, чтобы и основные шаги, и компенсации были идемпотентны, то есть повторный запуск операции не изменял состояние сверх первого эффекта. Причина – в распределённой среде неизбежны ретраи и дублирующиеся сообщения. Например, сервис может случайно дважды получить событие или повторно выполнить компенсацию после сбоя. Если компенсационное действие (например, отмена заказа) вызвать дважды, на выходе система должна остаться в корректном состоянии (второй вызов не должен неожиданно что-то испортить). Для этого часто приходится проверять текущее состояние перед действием: если операция уже выполнена ранее, повторно не делать. Идемпотентность усложняет код, но без неё Saga ненадёжна.

Промежуточная неполная согласованность: Пока сага не закончена (или не откатилась при ошибке), система может быть в состоянии, когда часть сервисов уже применили изменения, а другие – ещё нет. Это состояние Eventually Consistent – «в конечном счёте согласованное». Нужно учитывать в бизнес-логике, что между шагами саги данные временно несогласованы. Например, поездка может быть создана, но оплата деньги еще не зарезервированы – другим сервисам нельзя считать его окончательно оформленным. Обычно эту проблему решают через статусы (pending, in progress, etc.) и сокрытие «полупроведённых» операций от конечных пользователей, пока сага не завершится.

Отладка и мониторинг: Разобраться, что происходит в распределённой саге, сложнее, чем в локальной транзакции. Логика разбросана по сервисам (особенно при хореографии), последовательность действий не видна в одном месте. Необходимо внедрять корреляцию (например, уникальный идентификатор саги, передаваемый во все сообщения), вести детализированные логи, использовать распределённые трейсинг-системы. Только так можно отследить цепочку действий и выявить, где произошла ошибка.

Долговечность состояния саги: Если используется оркестратор, важно хранить прогресс саги в надёжном хранилище. В случае сбоя оркестратора он должен восстановиться и продолжить сагу (или откатить её). Это требует дополнительной работы – например, сохранять статус каждого шага в БД оркестрации. При хореографии долговечность обеспечивается брокером сообщений (сообщения не теряются) и идемпотентностью обработчиков.

Не подходит для коротких строго консистентных операций: Saga хороша для бизнес-процессов, где допустима некоторое время отложенная консистентность. Если же нужна мгновенная атомарность и недопустима промежуточная несогласованность, то Saga может быть слишком рискованным выбором.


Мы познакомились с паттернами, помогающими нам обеспечивать согласованность данных в распределенной системе на примере нашего сервиса такси. Далее рассмотрим теорию кэширования данных. 

КЭШИРОВАНИЕ
В распределенных высоконагруженных системах часто применяют кэширование. Зачастую на собеседованиях по систему дизайну от вас ожидают грамотного использования кэша.
Сейчас мы разберем основные аспекты кэширования: 
	•	Узнаем что такое кэширование и как оно помогает сервисам держать нагрузку?
	•	Поговорим о видах кэширования 
	•	Разберем основные алгоритмы взаимодействия с кэшом на уровне сервиса.
	•	Поговорим об алгоритмах вытеснения данных из кэша
	•	Разберем способы инвалидации данных в кэше
	•	Также я расскажу зачем нужно версионировать и тегировать кэш
	•	Поговорим о многомерном кэшировании


Для чего нужно кэширование? - Кэширование помогает снизить latency сервисов на запись и чтение в зависимости от выбранного подхода. 
	•	Благодаря кэшированию можно снизить нагрузку на сторонние сервисы
	•	Переиспользование вычисленией - например вы реализуете рекомендательную систему, котоая по ночам создает наборы рекомендаций для всех пользователей сервиса. С помощью кэшей можно заранее рассчитать нужные пользователю данные и отдавать их почти мгновенно. 
	•	Правильно настроенный подход к кэширование помогает системе жить при краткосрочных отказах. Актуально 



Прежде чем погружаться в алгоритмы кэширования, определим ключевые термины.

Cache Hit — это когда запрошенные данные нашлись в кэше. Это наша цель.
Cache Miss — противоположность, данных нет и приходится обращаться к базе данных.

Hit Ratio — процент успешных обращений к кэшу. Если из 100 запросов 85 нашлись в кэше, hit ratio составляет 85%. Это главная метрика эффективности кэширования.

Hot key(Горячий ключ) - это ключ, на который приходится бОльшая часть запросов.

TTL-  определяет, как долго данные живут в кэше. После истечения TTL данные считаются устаревшими и удаляются.

Прогрев кэша - процесс наполнения кэша данными

Инвалидация — процесс удаления данных из кэша, когда заканчивается место

Начнем с того, что разберемся с тем, какие есть виды кэширования на уровне приложения: 
Внутреннее и внешнее кэширование:
Внутреннее кэширование использует структуры данных вашего языка программирования. Закешированные данные находятся в рамках процесса приложения. Таким образом мы получаем самый быстрый доступ к этим данным, нам не нужно делать никаких сетевых вызовов, соответственно мы избавляемся еще и от операций сериализации и десериализации данных. Однако при таких плюсах есть очевдиный минус - наше приложение становится stateful, то есть начинает хранить какое-то состояние. Если процесс завершится, то и все данные исчезнут. При реализации внутреннего кэширования придется решать проблему согласованной инвалидации данных во всех репликах, а также правильно настраивать балансировку трафика, чтобы в одну реплику всегда приходили запросы на данные, находящиеся в ее кэше.




Внешнее кэширование использует сторонние инструменты, такие как redis/valkey. Благодаря такому подходу наше приложение остается stateless, все реплики могут обращаться к одному внешнему кэшу. При таком подходе легко инвалидировать данные, ведь они находятся в одном месте. Расплачиваемся за такое удобство скоростью, сразу добавляется оверхед на подход по сети и сериализацию/десериализацию данных.




Давайте детально разберём каждый алгоритм уровня приложения, чтобы вы точно понимали, когда и что использовать."
Алгоритм Cache-Aside (Lazy Loading)

Cache-Aside — самый популярный паттерн. Приложение полностью контролирует процесс. При чтении сначала проверяем кэш. Если данных нет — идём в БД, сохраняем результат в кэш и отдаём клиенту. При записи обновляем БД и инвалидируем кэш.



Преимущества этого подхода:
 простота реализации, приложение решает что кэшировать. 
простая инвалидация - при записях в бд, можно автоматически обновлять кэш 
хорошо подходит для горячих ключей 

Недостатки: 
дублирование логики кэширования
возможен cache stampede при одновременных запросах - это проблема, возникающая когда множество запросов одновременно пытаются получить данные, которые только что истекли в кэше.
холодный старт - первые запросы медленные, необходимо прогревать кэш
"

Когда применять:
При большой читающей нагрузке 
Нужен контроль над тем, что кладется в кэш на уровне кода
Допустимо читать неактуальные данные 

Далее обсудим Cache Through алгоритмы

Алгоритм Read-Through
Read-Through делает кэш прозрачным для приложения. Приложение обращается только к кэшу, а тот сам решает, когда и как загружать данные из БД. Это инкапсулирует логику кэширования и упрощает код приложения.



Преимущества:
Простота для клиента из-за наличия только одной точки входа
После прогрева кэша почти все запросы попадают в кэш


Недостатки:
Холодный старт - необходимо заполнять кэш перед стартом приложения
Конечная согласованность - без продуманной стратегии инвалидации в кэше могут оставаться старые или ошибочные данные

Когда использовать:
Чтения доминируют над записями 
Есть повторяемость ключей, то есть одни и те же данные нужны разным пользователям
Допустима конечная согласованность




Алгоритм Write-Through
Write-Through обеспечивает строгую консистентность.
При записи приложение пишет в кэш, а кэш синхронно (в том же потоке) «проталкивает» изменение в базовый хранилище (origin) и подтверждает успех только если оба слоя записали данные. 


Преимущества
кэш всегда согласован  с основным хранилищем после успешной записи — меньше риска вернуть старые данные.
Недостатки:
Дополнительная задержка на запись
Чувствительность к отказам - при недоступности одного из узлов(бд или кэша) останавливается вся работа, поэтому нужно продумывать поведение системы при отказе одного из узлов.
Когда применять:
При большой читающей нагрузке
Важна сильная согласованность на уровне кэша. 


Критически важен порядок операций — сначала идем БД, потом кэш. Иначе при сбое БД в кэше окажутся несохранённые данные."

Алгоритм Cache-Ahead


Этот алгоритм стоит использовать в ситуациях, когда мы имеем предсказуемые паттерны доступа. Приложение ничего не знает о базе данных, работает только с кэшем, зачастую только на чтение. Например, мы знаем, что в нашей системе такси всегда нужно узнавать локацию водителей при заказе. Мы можем заранее наполнять кэш данными о водителях в определенном секторе города. Так мы всегда будем иметь актуальную информацию и читать данные почти без задержки.
Важно следить за тем, чтобы кэш был всегда прогрет и доступен, иначе наше приложение не сможет работать. 


Вытеснение данных из кэша
Нам стоит помнить, что память не бесконечна. У кэша всегда должно быть ограничение по памяти. Поэтому существует процесс вытеснения данных из кэша. Рассмотрим базовые алгоритмы.

Random — случайный выбор. Просто выбираем случайный элемент для удаления. Работает хорошо при равномерном доступе к кэшу без горячих ключей.





FIFO просто удаляет самые старые данные. Быстро, предсказуемо, но не учитывает паттерны доступа. Используется для кэша с TTL, где возраст важнее популярности.


 LIFO - по сути это стэк, удаляем самые новые данные.
 
LRU — самый популярный алгоритм. Удаляются данные, к которым давно не обращались. Реализуется через двусвязный список и хэш-таблицу для O(1) операций.
Идеально для кэша с равномерным доступом к данным.


LFU отслеживает частоту обращений. Удаляются редко используемые данные. Проблема — популярные данные могут застрять в кэше навсегда.







Инвалидация данных
Существует два основных подхода:
инвалидация по TTL и event-driven.

Во время инвалидации по TTL важно помнить, о паттернах заполнения кэша. Если вы одновременно заполняете кэш большим количеством данных с одинаковым TTL, то по истечению TTL вы столкнетесь с проблемой, при которой все запросы пойдут в базу данных, которая может не выдержать такой нагрузки. Это явление называется thundering herd problem. Для того, чтобы предотвратить эту проблему используют jitter вместе с ttl. По сути это просто добавление рандомного времени, для того, чтобы кэши не протухали одновременно.


Event driven инвалидация обычно реализуется через брокеры сообщений. Сервис слушает определенные события и удаляет данные из кэша, связанные с этим событием. Например мы хранили в кэше статус нашей саги, как только получили информацию о завершении последнего шага - удаляем данные из кэша.

Версионирование кэша
Все сервисы подвержены обновлению, но что происходит, когда при добавлении новой фичи мы обновляем модель данных, которую храним в кэше? Если ничего не делать и раскатить изменения, то пользователи не получат обновленные данные, т.к были закэшированы старые версии нашей модели. Можем представь наш сервис такси, представим, что вся информация о текущих тарифах хранится в кэше, соответственно сервис отдает данные из этого кэша. В новой фиче мы добавили новое поле в нашу модель, теперь мы добавили новое поле, отвечающее за наличие высокого спроса, фронтенд ожидает данные в новом формате, но приложение до сих пор отдает старую модель. Для того, чтобы решить эту проблему и актуализировать данные, мы должны добавить версии для нашего кэша. Обычно версию пишут как суффикс к ключу в кэше. Сервис с новой версией кэша, которая обычно выставляется в конфигурации сервиса попытается запросить данные, произойдет cache miss, соответсвенно ему придется обновить данные в кэше, взависимости от способа взаимодействия.




Тегирование кэша
Бывают ситуации, когда в кэше хранятся связанные по смыслу данные, которые обязаны быть согласованными, но работаю с ними разные сервисы. Например у нас есть сервис каршеринга, в нем кэш в котором мы храним информацию о нашем автопарке. Сервис A хранит в кэше данные о марках автомобилей и их количестве, а сервис Б хранит всю информацию о нахождении конкретных автомобилей с разделением по городам/секторам. Внезапно мы решаем, что автомобиль марки aston martin не достоин находиться в нашем автопарке, и при новом обновлении выпиливаем его из сервиса А. Но видим такую ситуацию, информации на странице выбора авто больше нет в кэше, а вот позиции на карте автомобилей aston martin остались и отображаются в приложении. В случае, если мы закладывали такой сценарий заранее, нам стоило подумать о тегировании. Если на записях сервиса А и сервиса Б были навешаны одинаковые теги,эти данные были бы одновременно инвалидированы.


Многослойный кэш
Ранее мы рассматривали кэш уровня приложения, но на самом деле кэш выстраивается на разных уровнях, начиная от браузера и заканчивая кэшем базы данных. Расскажу о тех уровнях, которые стоит знать и всегда держать в уме при проектировании вашей системы:
	•	Браузер кэширует статические файлы
	•	Кэш на уровне прокси-сервера, самый яркий пример — это CDN
	•	Кэш уровня приложения, который мы разбирали ранее
	•	Кэш базы данных
Настроить согласованное поведение между всеми этими уровнями достаточно сложно, и мы не будем рассматривать, как это сделать в рамках текущего курса, однако вам будет полезно знать о существовании этих уровней. В высоконагруженных приложениях, особенно распределенных по разным регионам, применяются все возможные способы оптимизации работы приложения. Это нетривиальная задача, тут также нет серебряной пули, и при выборе подхода к работе с многоуровневым кэшем придется опираться на паттерны доступа к кэшу, которые предварительно необходимо выявить.




ЗАКЛЮЧЕНИЕ И ДОМАШНЕЕ ЗАДАНИЕ


В работе вы никогда не увидите идеальных систем, потому что их не бывает. Вам не нужно стремиться построить идеальную систему, гораздо важнее понимать: не нужно применять все паттерны сразу. Начните просто, усложняйте по мере роста.

Ключевое при проектировании систем - это заложить правильные абстракции, чтобы эволюция была возможна без полного переписывания системы."

ДОМАШНЕЕ ЗАДАНИЕ: ПРОЕКТИРОВАНИЕ СЕРВИСА ДОСТАВКИ ЕДЫ

Задача звучит так: у нас есть сервис доставки еды. Мы хотим спроектировать главный пользовательский путем: доставка заказа, которая включает в себя: проверку наличия блюда в конкретном ресторане, заполнение корзины, создание заказа, оплата заказа, назначение курьера. Какие гарантии согласованности стоить заложить заранее? Какие инварианты для данной системы вы выберете?   Определите все инваринаты и распишите каким образом вы будете достигать выбранных гарантий согласованности. 
Разбор домашки будет на бусти.

Удачи в выполнении домашнего задания! Напишите в комментария о том какая тема для вас была самой полезной во всей главе.







Глава 4. Системный дизайн в аналитике
Спикер: Артем

Введение
Давайте поговорим о том, как от роли, которую иногда называют «сборщиком требований», перейти к роли архитектора решений. И главное — зачем вам это вообще нужно?
Ключевой скачок для Middle-аналитика на пути к Senior-уровню — это переход от ответа на вопрос «Что мы делаем?» к способности самостоятельно отвечать на вопрос «А как мы это будем делать?».
Это значит, что вы перестаёте быть просто транслятором требований — когда вы собрали информацию, формализовали её в User Story или Use Case, согласовали и передали на разработку. Вы становитесь тем, кто может эти требования технически обработать:
	•	составить архитектуру; 
	•	спроектировать все интеграции; 
	•	и выдать разработчикам такое техническое задание, по которому у них практически не останется вопросов. (когда говорю про выделенное желтым - надо показать картинку снизу)  
Конечно, небольшие уточнения будут всегда, но общая концепция именно такая: создать настолько понятный проект, чтобы команде оставалось просто брать и делать.
Зачем это нужно? Ваша карьера и ценность на рынке
Понимание системного дизайна открывает для аналитика несколько путей роста и значительно повышает вашу стоимость как специалиста.
	•	Solution Architect (Архитектор решений) Это путь углубления в технологии и архитектуру. Вы учитесь видеть систему целиком, а не только её отдельные части. Вы уходите от простого сбора требований и становитесь самостоятельной единицей, которая может спроектировать сложную систему с нуля. 
	•	Менеджер / Product Owner Здесь системный дизайн даёт вам суперсилу — умение оценивать сложность и объём продуктов не просто по списку фич, а по компонентам и необходимой инфраструктуре. Вы начинаете понимать, почему продукт с одними и теми же функциональными требованиями будет стоить совершенно разных денег и времени, если им будут пользоваться 100 человек, а не 100 тысяч.  (когда говорю про выделенное желтым - надо показать картинку снизу) 
	•	Ваша ценность на рынке прямо сейчас Давайте будем честны: особенно сейчас на рынке ценятся именно те аналитики, которые владеют системным дизайном. Умение проектировать системы — это уже не просто "плюс", а ожидаемый навык для высокооплачиваемых позиций. Это как минимум то, о чём вас будут спрашивать на собеседованиях на большие зарплатные вилки, и как максимум — это решающий критерий для вашего карьерного роста и дохода. Можно сказать, что это новая тенденция и стандарт профессии, который отделяет высококлассного специалиста от всех остальных. 
Цена ошибки: почему хороший дизайн экономит миллионы
Становиться более высококлассным аналитиком важно ещё и потому, что цена ошибки аналитика на ранних этапах проектирования — очень большая.
Используем простую аналогию: ошибка в фундаменте при проектировании системы стоит в 100 раз дороже, чем мелкий ремонт «крыши» уже в готовом продукте. Если архитектурный просчёт обнаружится на поздних стадиях, это может потребовать переделки огромных частей системы, что приведёт к потере времени, денег и мотивации команды.
Работа с требованиями
Что вы уже должны уметь
Во-первых, у вас, как у аналитика, должны быть базовые вводные. Вы умеете собирать требования, формализовывать их и грамотно взаимодействовать с заказчиком. Это фундамент. Если вы не умеете собирать требования, вы не сможете проектировать системы. Это первый шаг. Это база. Это знают даже джуны, но я всё равно лишний раз проговорю.
Важно понимать: даже если вы научились проектировать системы, сбор требований и взаимодействие с заказчиком никуда не исчезают. Вы не станете монахом на горе, которому что-то приносят, и он своей мудрой рукой пишет ТЗ. Нет. Вы должны быть в продукте, в процессах, глубоко погружены в задачи бизнеса и всегда взаимодействовать с заказчиком. Это первая и постоянная обязанность.
Теперь давайте разберёмся по действиям аналитика.
Участие в Discovery
Если мы говорим о жизненном цикле ПО, то аналитик в самом начале не просто собирает требования. Если продукт новый или фича крупная (а не просто минорная доработка), то на этапе discovery нужно проверить жизнеспособность идей. Это критически важно: оценить реализуемость и сложность, чтобы не тратить месяцы и ресурсы на заведомо бесперспективный проект.
Кто может это делать? Аналитик, архитектор, техлид — в общем, человек, который понимает и продукт, и потребности бизнеса, и основы проектирования систем. Он должен дать честную оценку: возможно ли это реализовать, насколько это сложно, какие риски и ограничения.
Возьмём пример.
Пример discovery
Бизнес приходит и говорит:
«Мы хотим собирать аналитику по каждой покупке на маркетплейсе в реальном времени и сразу же на её основе показывать рекомендации пользователю».
Звучит классно, но давайте разберём. Это реалтайм-сценарий. То есть:
	•	покупка фиксируется,
	•	система мгновенно должна обработать событие,
	•	пересчитать модели,
	•	и выдать новые рекомендации прямо на глазах у пользователя.
Что это значит для нас?
Это высокая нагрузка, постоянные вычисления «на лету», сложная архитектура и дорогостоящая инфраструктура. Реализовать можно, но это непросто.
Задача аналитика — донести это до бизнеса. Показать, что такие требования автоматически тянут за собой рост стоимости и сроков. И задать правильный вопрос: действительно ли критично выдавать рекомендации прямо в момент покупки?
А теперь предложим альтернативу.
Мы можем собирать данные по каждой покупке, но обрабатывать их асинхронно. То есть покупки копятся в системе, данные агрегируются и периодически пересчитываются. И уже после этого пользователю выдаются рекомендации — например, на следующий визит или даже через несколько минут.
Что важно: асинхронный подход проще, дешевле и быстрее в реализации. Да, «вау-эффекта» реалтайма не будет, но бизнесу придётся решить, насколько он готов инвестировать ради этого эффекта.
И вот здесь начинается настоящая работа аналитика — уметь показать разницу, объяснить варианты и последствия, а потом вместе с бизнесом принять взвешенное решение. Иногда нужно замедлиться, чтобы проект в целом стал более жизнеспособным.
Работа с ФТ
Соответственно, чтобы уметь это делать, у вас должны быть базовые навыки системного аналитика.
Первое — умение собирать и формализовывать функциональные требования так, чтобы они были понятны разработчикам.
Здесь важно не просто зафиксировать хотелку бизнеса, а провести декомпозицию. То есть разбить большую идею на конкретные ключевые возможности.
Например, бизнес говорит:
«Хотим программу лояльности с баллами и уровнями».
Аналитик должен разложить это на части:
	•	начисление баллов,
	•	списание баллов,
	•	система уровней и переход между ними,
	•	правила отображения статуса пользователю.
И только после такой декомпозиции у команды разработки появляется чёткое понимание, что именно нужно реализовать.
В помощь вам — формализация через популярные и проверенные инструменты: User Story, Use Case, BPMN-диаграммы, Activity Diagram.
Эти нотации позволяют структурировать требования так, чтобы и бизнес, и разработчики говорили на одном языке.
Но на этом работа не заканчивается.
Работа с НФТ
Второе — это умение работать с нефункциональными требованиями.
И вот тут мы выходим на уровень, который отличает крутого аналитика от простого переводчика бизнеса на технический язык.
Нефункциональные требования напрямую влияют на архитектуру. Это производительность, масштабируемость, отказоустойчивость, безопасность, время отклика, удобство поддержки.
Например, если бизнес говорит «нам нужно 1000 пользователей», архитектура может быть одна. Если «нам нужно 100 000 пользователей с доступом 24/7», архитектура будет совсем другая.
Именно учёт нефункциональных требований превращает аналитика в человека, который действительно понимает, как система будет работать в реальных условиях.
№
Категория
Коротко (для человека не в теме)
Вопрос, который задать
Пример формулировки требования (готово в ТЗ)
Примеры метрик (понятно и конкретно)
Коротко — влияние на архитектуру
1
Производительность
Насколько быстро система должна отвечать пользователю.
Сколько времени допустимо ждать ответа для основных операций?
Ответ должен приходить в real-time.
Cреднее время ответа ≤ 200 ms (avg).
Кэширование, индексы в БД, оптимизация запросов, CDN, in-memory.
2
Пропускная способность (Throughput)
Сколько действий/запросов система обрабатывает в секунду.
Сколько запросов/транзакций в секунду в пике должна выдерживать система?
Система должна выдерживать 5000 запросов в секунду (RPS)
RPS (запросов в сек); TPS (транзакций/сек);
Балансировка нагрузки, очереди (message queues), горизонтальный скейл.
3
Масштабируемость
Насколько легко система может «расти» при увеличении пользователей/запросов
«На сколько раз и за какой срок вы ожидаете рост нагрузки (например, за 12 мес)? Ожидаются ли пик-флеши?»
Система должна горизонтально масштабироваться и выдерживать рост нагрузки в 4× за 12 месяцев
Время запуска новой ноды;
Сервисы без состояния для масштабирования
4
Доступность (Availability)
Насколько часто система должна быть доступна (SLA).
Какой % времени система должна быть доступна? Допустимый простой?
SLA 99.95% (макс. простой ≈ 4.4 часа в год).
Uptime % (процент успешной работы сервиса); MTTR (время восстановления).
Наличие резервных инстансов/реплик. Мониторинг + алерты - Health-checks: состояние сервисов проверяется автоматическ
5
Надёжность (Reliability)
Система должна корректно работать длительное время без ошибок/потерь.
Какие типы сбоев/потерь данных недопустимы?
Критичные операции (платежи) не должны приводить к потере данных.
% успешных транзакций; error rate (ошибок/запросов).
Репликация данных, гарантированные записи (durable writes), retry/идемпотентность.
6
Устойчивость (Resilience)
Как система ведёт себя при ошибках: должна деградировать, но работать.
Что должно происходить, если внешние сервисы недоступны? (деградация/ошибка)
При недоступности платёжного провайдера сервис переходит в режим очереди и не теряет операции.
Время переключения в деградированное состояние; % функционала в деградации.
Circuit breakers, bulkheads, очереди, graceful degradation.
7
Отказоустойчивость (Fault tolerance)
Сколько компонентов может сломаться и система всё ещё работает.
Сколько серверов/узлов может потеряться без заметного влияния?
Потеря до 2 узлов в кластере не должна влиять на 99% операций.
% операций успешных при потере N узлов; время на восстановление.
Кворумы/реплики, leader election, распределённые хранилища.
8
Консистентность данных
Насколько быстро все копии данных должны стать одинаковыми. Нужно ли, чтобы данные были точными сразу или можно подождать несколько минут?
«Нужна ли мгновенная точная информация (например, баланс счета) или допустима небольшая задержка (например, отчёты)? Какой максимум задержки вы готовы принять?»
«Балансовые и платёжные операции — строгая (моментальная) консистентность. Отчёты и аналитика — eventual consistency, расхождение не более 20 минут.»
Время синхронизации: допустимая задержка, напр. ≤ 20 минут for отчётов.
ACID-БД где нужна консистентность
9
Безопасность
Защита данных и доступа: чтобы никто неавторизованный не взломал систему и важные данные не попали наружу.
“Какие данные считаются чувствительными? Нужен ли SSO/MFA для админов? Какие требования по шифрованию и аудиту?”
«Все персональные и платёжные данные шифруются при хранении и передаче. Админ-доступ через корпоративный SSO + MFA. Все операции с правами и критичные изменения логируются.»
% шифрования данных
• Включить TLS для всех внешних/внутренних соединений. • Внедрить IAM/ролевую модель (RBAC) и SSO + MFA для админов. • Хранить ключи в KMS/HSM.
10
Конфиденциальность / соответствие
Правила хранения/удаления персональных данных и соответствие законам.
Есть ли регуляторные (законодательные) требования?
Данные RU-пользователей хранятся на серверах в RU; удаление по запросу (right to be forgotten) ≤ 30 дней.
Время удаления данных
Контроль выполнение СОПД, механизмы удаления ПД по требовани
11
Логируемость и аудит
Что и как долго нужно фиксировать (для разбирательств/налога).
Какие события нужно логировать и как долго хранить логи?
Все изменения платежей и прав пользователей логируются и хранятся 5 лет.
Срок хранения логов (дней/лет);
Централизованный лог-стек (ELK/stack), контроль доступа к логам.
12
Наблюдаемость (Observability)
Возможность быстро понять, что и почему сломалось (метрики/трейсы).
Какие метрики и трассировки нужны для поддержки и мониторинга?
Сбор метрик: latency, error rate, cpu, memory; трассировка ошибок для 100% упавших запросов.
% запросов с логами;
Prometheus/Grafana, трассировка логов, централизованный алертинг.
13
Локализация
Поддержка разных языков и локальных форматов (даты, валюта).
На какие языки и локали нужна поддержка?
Интерфейс должен быть RU/EN/KZ; форматы дат и валюты под локаль.
% интерфейса переведено; корректность форматов локали.
Функционал локализации, тесты локализации.
14
Долговечность данных (Retention / Durability)
Как долго хранить данные и где архивировать старые.
Какие данные и на какой срок нужно хранить/архивировать?
Транзакционные записи хранятся 5 лет; логи — 1 год, затем удаляются.
Срок хранения (годы/дни);
Политики хрпанения данных (архив/горячее или холодное хранение), регулярные бэкапы и тесты восстановления.
Практический пример проверки реализации идеи бизнеса
Теперь, когда мы немного познакомились с НФТ, можем рассмотреть такой пример:
	•	Идея бизнеса: «Давайте добавим в наше приложение доставки еды фичу, где клиент видит иконку курьера, плавно движущуюся по карте в реальном времени, как в популярных сервисах такси».
	•	Результат Discovery: Аналитик вместе с архитектором начинают задавать уточняющие вопросы, чтобы оценить реальный масштаб задачи:
	•	Масштаб: «Сколько курьеров у нас будет на линии в пиковое время? 1000? 10 000?»
	•	Частота: «Как часто приложение курьера должно отправлять свои координаты? Раз в 10 секунд? Раз в 3 секунды?»
	•	Нагрузка: «Сколько клиентов одновременно будут следить за своими заказами в час пик?»
	•	Выявленные критические NFRs:
	•	Высокая нагрузка на запись (High Ingress Load): 10 000 курьеров, отправляющих координаты каждые 3 секунды, — это более 3 300 запросов в секунду, которые нужно принять и обработать. Обычная база данных может не справиться.
	•	Низкая задержка (Low Latency): Чтобы движение было "плавным", координаты нужно не только быстро принять, но и почти мгновенно доставить клиентам.
	•	Нагрузка на "веерную" рассылку (High Fan-out Load): Каждое из 3 300 обновлений в секунду нужно доставить соответствующему клиенту. Это требует специальной инфраструктуры, например, WebSocket-серверов.
	•	Стоимость (Cost): Обработка такого потока геоданных и использование карт в реальном времени может привести к огромным счетам от облачных провайдеров.
	•	Вывод для бизнеса: Эта фича — не просто «показать точку на карте». Это создание высоконагруженной системы потоковой обработки данных в реальном времени. Её реализация потребует выделенной инфраструктуры (например, брокеров сообщений вроде Kafka), отдельных сервисов и значительных затрат на разработку и поддержку. Необходимо оценить, окупит ли бизнес-ценность от этой фичи подобные инвестиции.
Влияние НФТ на архитектуру: одинаковые ФТ, но разные НФТ
Представим, что у нас есть простое функциональное требование (ФТ): "Система должна формировать для менеджера отчёт по продажам". Само по себе это требование не говорит нам, как строить систему. Всё меняется, когда мы добавляем всего одно нефункциональное требование (НФТ), касающееся актуальности данных.
Критерий
Вариант А: «Отчёт к утру» 🌅
Вариант Б: «Отчёт в реальном времени» ⚡️
Нефункциональное требование (НФТ)
Отчёт должен быть готов каждый день к 9:00 утра.
Данные в отчёте должны отставать от реальных не более чем на 5 секунд.
Подход к реализации
Пакетная обработка — данные собираются и считаются раз в день.
Потоковая обработка — данные считаются сразу при поступлении.
Принцип работы
1. Ночью запускается скрипт.2. Собирает все данные за день.3. Считает результат и сохраняет.4. Утром менеджер видит готовый отчёт.
1. После продажи создаётся событие.2. Оно сразу обрабатывается системой.3. Менеджер видит обновлённые данные почти мгновенно.
Ключевые технологии
База данных (PostgreSQL, MySQL) — хранение и простая выборка данных.

Скрипт SQL — считает отчёт по расписанию.


Планировщик — запускает скрипт ночью автоматически.
Брокер сообщений (Kafka, RabbitMQ) — «почтовый ящик» для событий, которые приходят от системы.

Потоковая обработка — считает данные на лету.


Аналитическая БД (ClickHouse) — очень быстрая база, оптимизированная для мгновенных отчётов. Шардирование — деление данных на «кусочки» (шарды) по определенным принципам (например по региону), которые хранятся и обрабатываются параллельно на разных серверах. Это позволяет масштабировать систему и не перегружать один сервер.


Сложность и стоимость
Низкая — можно сделать силами обычных разработчиков.
Высокая — нужна команда DevOps/Data инженеров и сложная инфраструктура.
Идеально подходит для...
Стратегического планирования и анализа исторических данных, где не важна сиюминутная точность.
Онлайн-мониторинга и быстрого реагирования (например, отслеживание подозрительных транзакций и заказов).
💡 Кейс плохого сбора требований из реальной жизни (Идемпотентность):
	•	Проблема: В банковском приложении можно было быстро нажать на кнопку "Перевести" несколько раз. Система не проверяла уникальность запроса и создавала несколько одинаковых переводов, позволяя "дюпать" деньги.
	•	Причина: Отсутствовал ключ идемпотентности — уникальный идентификатор, который позволяет серверу понять, что это повторный запрос, и не обрабатывать его дважды. и блокировки кнопки на фронте
	•	Вывод: NFR "операция должна выполняться ровно один раз" — это критическое требование, которое аналитик должен выявить и зафиксировать.
	•	«Фронт должен блокировать кнопку для UX».
	•	«Бэкенд обязан проверять идемпотентность для защиты от повторных запросов».
Проектирование решения
Итак, мы переходим от работы с требованиями к самому интересному — к проектированию системы и оформлению детальной спецификации. В этом модуле мы разберём архитектуру, интеграции и работу с данными.
Шаг 1. Анализ и принятие решения:
На этом этапе мы проходим путь от анализа требований до создания визуального чертежа системы. Это единый процесс, где сначала принимается стратегическое решение, а затем оно немедленно фиксируется в виде наглядной архитектурной карты.
Прежде чем рисовать, аналитик отвечает на главный вопрос: как новая функциональность впишется в существующую IT-архитектуру?
Это делается через «примерку» требований к системе и анализ компромиссов (trade-offs) для трёх основных стратегий:
	•	Эволюция (доработка существующего)
	•	Суть: Вносим изменения в уже работающие сервисы.
	•	Плюсы: Быстро для небольших, минорных изменений.
	•	Минусы: Рискованно (можно сломать то, что работает) и ограничено возможностями текущей архитектуры.
	•	Революция (создание нового)
	•	Суть: Пишем один или несколько новых сервисов с нуля.
	•	Плюсы: Безопасно и гибко. Мы сможем полностью реализовать всё, как хотим.
	•	Минусы: Долго и дорого на старте.
	•	Гибрид (интеграция нового в старое)
	•	Суть: Создаём новые компоненты и встраиваем их в существующую архитектуру.
	•	Плюсы: Самый частый и сбалансированный подход. Это быстрее, чем всё делать с нуля, но, конечно, медленнее, чем просто доработать существующее.
	•	Когда подходит: Идеально для больших фич, особенно если IT-ландшафт компании уже достаточно большой. Например, можно вынести что-то высоконагруженное в отдельный, новый сервис.

Шаг 2: Выбор паттерна проектирования сервисов
После того как мы приняли стратегическое решение (например, выбрали «Революцию» или «Гибрид»), перед нами встаёт следующий ключевой вопрос: «А как именно мы будем „нарезать“ новую функциональность на отдельные сервисы?». Просто сказать «делаем микросервисы» недостаточно. Нам нужно определить их логические границы.
Именно для этого существуют архитектурные паттерны — готовые, проверенные временем решения для типичных задач проектирования. Важно понимать, что мир паттернов огромен, и под разные ситуации подходят разные из них. Некоторые — более технические и касаются отказоустойчивости или управления данными, другие — стратегические. Изучение этих паттернов — важный шаг в развитии любого архитектора или системного аналитика.
Можно выделить несколько ключевых групп паттернов:
	•	Паттерны декомпозиции. Отвечают на главный вопрос: «Как разбить большую задачу на маленькие, независимые сервисы?». Здесь есть два популярных подхода:
	•	Декомпозиция по бизнес-возможностям (By Business Capability). 🏢 Это самый простой и логичный способ для начала. Мы смотрим на то, что бизнес делает, и создаём сервис для каждой его функции. Например, в интернет-магазине есть возможности: «Управление каталогом товаров», «Обработка заказов», «Управление клиентами». Каждая из них становится отдельным микросервисом. Этот подход легко понять, потому что архитектура напрямую отражает структуру бизнеса.
	•	Декомпозиция на основе предметно-ориентированного проектирования (Domain-Driven Design, DDD). 🗣️ Этот подход глубже. Он предлагает разбить сложную систему на «ограниченные контексты» (Bounded Contexts) — отдельные мини-миры со своими правилами и языком. Например, слово «Товар» в контексте «Склада» означает физический объект с весом и размером, а в контексте «Каталога на сайте» — это описание, фото и цена. DDD предлагает сделать для каждого такого контекста свой сервис. Это помогает справляться со сложностью в очень больших и запутанных системах.
	•	Паттерны взаимодействия. Определяют, как сервисы будут общаться друг с другом. Яркий пример — паттерн API Gateway, который создаёт единую точку входа для всех внешних запросов к системе, скрывая её внутреннюю структуру.
	•	Паттерны управления данными. Решают проблемы хранения и консистентности данных в распределённой среде (например, «База данных на сервис»).
Для системного аналитика самый простой и понятный способ начать — это использовать паттерн декомпозиции по бизнес-возможностям (Decomposition by Business Capability). Его суть в том, чтобы выделить ключевые направления деятельности бизнеса и создать отдельный сервис для каждой такой возможности. Этот подход идеален для аналитиков, так как он напрямую связывает архитектуру с бизнес-логикой.
Пример: Декомпозиция программы лояльности
Давайте вернёмся к нашему примеру: бизнес хочет «программу лояльности с баллами и уровнями». Применим паттерн декомпозиции по бизнес-возможностям.
	•	Сначала определим эти самые «возможности»:
	•	Управление баллами: Всё, что связано с начислением, списанием и расчётом баллов.
	•	Управление уровнями клиентов: Логика присвоения уровней (например, Bronze, Silver, Gold) и переходов между ними.
	•	Управление акциями: Создание и настройка специальных предложений (например, «двойные баллы по выходным»).
	•	Информирование клиента: Отправка уведомлений об изменении баланса или уровня.
	•	Теперь сопоставим каждую возможность с будущим микросервисом:
	•	Points Service (Сервис баллов): будет отвечать за всю логику работы с баллами.
	•	Tiers Service (Сервис уровней): будет управлять статусами клиентов.
	•	Promotions Service (Сервис акций): будет хранить и применять правила акций.
	•	Notifications Service (Сервис уведомлений): будет отправлять email/push-уведомления (это может быть уже существующий сервис, с которым мы просто интегрируемся).
В результате этого шага мы получаем итоговый список сервисов. Теперь мы чётко понимаем, из каких независимых компонентов будет состоять наша система. Этот список является фундаментом для следующего этапа — визуализации архитектуры.

Шаг 3: визуализация архитектуры
Итак, после того, как мы определились со стратегией, нам необходимо её визуализировать. Главным артефактом на этом шаге является архитектурная карта. Для её создания мы будем использовать два ключевых уровня модели C4.
Ключевая концепция для аналитика:
Для вас, как для проектировщика, система состоит из набора «чёрных ящиков» (контейнеров). Ваша главная задача — определить, какие это ящики, за что каждый из них отвечает, и как именно они должны общаться друг с другом. Что происходит внутри каждого ящика — это уже зона ответственности команды разработки.
Уровень 1 — Системный контекст 
Есть оч хороший пример схемы из хабра? Можно ли взять его или самому перерисовать?

Цель: показать где границы вашего продукта и кто / что с ним взаимодействует. Это диаграмма для бизнеса, продакт-менеджеров, стейкхолдеров и первичного согласования архитектурных решений.
Аналогия: Мы смотрим на карту мира и видим нашу страну, её границы и соседние государства, с которыми она взаимодействует с остальным миром через автодороги, аэропорты, морские порты, жд станции
Что рисуем?
	•	Вашу систему в самом центре, как единый «чёрный ящик». Мы пока не знаем, что у неё внутри, но знаем её название и основное предназначение.
	•	Пользователей (акторов), которые с ней взаимодействуют (например, Клиент, Администратор, Менеджер).
	•	Внешние и существующие системы, с которыми она обменивается данными (например, Платёжный шлюз, SMS-провайдер, Бухгалтерская система 1С).
	•	Основные потоки данных (стрелки): что и в каком направлении передаётся (с краткой пометкой про частоту/временной характер — «батч/реалтайм/по запросу»).
Легенда:
	•	Пользователь  (когда говорю про выделенное желтым - надо показать картинку снизу) 
	•	Разрабатываемая система  (когда говорю про выделенное желтым - надо показать картинку снизу) 
	•	Внешние системы  (когда говорю про выделенное желтым - надо показать картинку снизу) 
	•	Пример диаграммы и пояснения:(когда говорю про выделенное желтым - надо показать картинку снизу)  
	•	Связи  (когда говорю про выделенное желтым - надо показать картинку снизу) 
Пример:
Уровень 2: Контейнеры (Ключевая архитектурная схема)
Здесь мы «приближаем» карту и заглядываем внутрь нашего «чёрного ящика». Этот уровень описывает основные технические блоки, из которых состоит система.
Аналогия: Мы приблизили карту и теперь видим не просто страну, а её крупные города, регионы и главные транспортные магистрали между ними. Мы видим Москву (сервис пользователей), Санкт-Петербург (сервис заказов), Екатеринбург (складская система) и федеральные трассы (интеграции), которые их соединяют.
Что такое «Контейнер» в C4?
Это любая отдельно развертываемая единица (deployable unit) в вашей IT-системе. Если вы можете запустить или остановить что-то независимо от остального, скорее всего, это контейнер.
Примеры контейнеров:
	•	Приложения: Микросервис, веб-приложение (Front-end), мобильное приложение.
	•	Хранилища данных: База данных (PostgreSQL), кэш (Redis).
	•	Интеграционные компоненты: Брокер сообщений (Kafka), API Gateway.
Что рисуем?
На этой диаграмме мы показываем, как эти контейнеры взаимодействуют для реализации системных сценариев.
	•	Конкретные сервисы, приложения и хранилища (новые и существующие).
	•	Связи между ними. Каждая стрелка должна быть подписана и отвечать на два вопроса:
	•	Зачем? (Краткое описание бизнес-цели: «Запрашивает данные о пользователе», «Отправляет событие об оплате»).
	•	Как? (Технология/протокол: «Синхронно, REST/HTTPS», «Асинхронно, Kafka»).
Легенда:
	•	Контейнер  (когда говорю про выделенное желтым - надо показать картинку снизу) 
	•	Группа контейнеров из которых состоит документируемая система (когда говорю про выделенное желтым - надо показать картинку снизу)  
	•	Хранилище данных (когда говорю про выделенное желтым - надо показать картинку снизу)  
	•	Брокер или очередь сообщений (когда говорю про выделенное желтым - надо показать картинку снизу) 
Пример диаграммы и пояснения:(когда говорю про выделенное желтым - надо показать картинку снизу)
Ключевая ценность для аналитика:**
Для системного аналитика — это главная архитектурная диаграмма. Именно здесь:
	•	Визуализируется ваше стратегическое решение (появляются новые сервисы, которые интегрируются со старыми).
	•	Определяются границы ответственности каждого сервиса.
	•	Фиксируются все точки интеграции, которые затем нужно будет детализировать.
Это финальная точка детализации архитектуры с точки зрения аналитика. Дальнейшее погружение — уже в зону ответственности команды.
Почему мы останавливаемся на уровне контейнеров?
Уровни 3 (Компоненты) и 4 (Код) описывают внутреннюю реализацию контейнера. Для аналитика это не так важно, потому что то, как разработчики организуют код внутри сервиса — одним модулем или пятью — является деталью реализации, которая не меняет его внешнего поведения и контрактов.
Ваша задача — определить обязанности контейнера и его API, а не его внутреннее устройство.
Шаг 4: Проектирование интеграций
Итак, мы определились с компонентами и их связями на архитектурной карте. Теперь наша задача — превратить эти высокоуровневые стрелки в исчерпывающее руководство для разработчика. Цель — не оставить никаких «белых пятен», чтобы реализация была быстрой, точной и предсказуемой.
Этот процесс состоит из трёх последовательных шагов:
1. Выбор паттерна интеграции: Синхронный vs. Асинхронный
Первое, что мы делаем — смотрим на наши функциональные и нефункциональные требования и выбираем общий подход к взаимодействию.
	•	Выбираем СИНХРОННЫЙ, если:
	•	Клиенту нужен немедленный ответ для продолжения работы. Например, проверить баланс карты перед переводом или получить информацию о профиле пользователя перед заполнением заявки. Это взаимодействие требует мгновенного получения либо результата, либо ошибки.
	•	Синхронный вызов создаёт сильную временнУю связанность: сервис-клиент не может продолжать работу, пока сервис-исполнитель не ответит. Если исполнитель "лежит", то "лежит" и клиент.
	•	Выбираем АСИНХРОННЫЙ, если:
	•	Операция длительная, и мы не хотим заставлять пользователя ждать. Классические примеры: формирование годового отчёта, конвертация видео, скоринг в банковской системе для одобрения кредита.
	•	Нужно повысить надёжность и разорвать жёсткую связь между сервисами. Если сервис-получатель временно недоступен, сообщение просто подождёт его в очереди, пока сервис «оживёт».
	•	Одно действие должно запускать несколько независимых процессов в разных системах. Например, событие «Заказ оплачен» должно запустить процесс сборки на складе, списание бонусов, формирование чека и создание заявки на доставку.
	•	Асинхронный вызов создаёт слабую связанность (decoupling): клиент отправил сообщение и "забыл". Ему не важно, когда исполнитель его обработает и доступен ли он прямо сейчас. (когда говорю про выделенное желтым - надо показать картинку снизу)  
2. Выбор конкретной технологии
После выбора паттерна мы определяемся с технологией, учитывая требования и стандарты, принятые в компании.
Важно понимать: нет единого «правильного» решения, подходящего для всех. Выбор технологии сильно зависит от того, с чем умеет работать ваша команда, какие принципы приняты в компании. Где-то вся разработка ведётся на Go, и стандартом для внутреннего общения является gRPC. В другой компании может быть «зоопарк» языков, и универсальным клеем для них служит REST/HTTPS.
	•	Если выбран СИНХРОННЫЙ паттерн:
	•	REST/HTTPS: Самый популярный и универсальный выбор. Идеален для публичных и большинства внутренних API. (Стоит отметить, что с его помощью можно реализовать и некоторые асинхронные подходы, например, через webhooks).
	•	gRPC: Высокопроизводительный фреймворк, идеален для частого и быстрого общения между внутренними микросервисами.
	•	WebSockets: Специфический выбор для постоянного двунаправленного соединения между клиентом и сервером (чаты, биржевые котировки).
	•	Если выбран АСИНХРОННЫЙ паттерн:
	•	Очереди / Брокеры сообщений (RabbitMQ, Kafka): Это самые популярные технологии для построения надёжных асинхронных систем. Они позволяют «отвязать» сервисы друг от друга и гарантировать доставку сообщений.  (когда говорю про выделенное желтым - надо показать картинку снизу) 

3. Детализация — создание исчерпывающей спецификации
📌 Главная идея: у разработчика не должно быть двусмысленности — какие параметры есть, какие обязательные, какие форматы, и как выглядят успешные и ошибочные ответы.
Артефакт 1: Контракт интеграции (API, События, Сообщения)
Контракт — это «паспорт» нашего взаимодействия. Ключевая идея — детально описать структуру данных для любого взаимодействия между двумя сервисами, будь то REST API, сообщение в Kafka или WebSocket. Принцип всегда один: дотошно задокументировать все параметры, их типы, форматы и обязательность.
Первый артефакт: Контракт интеграции (Описание формата сообщений)
Контракт интеграции — это договор о формате и правилах обмена между сервисами. Ключевая идея — детально описать структуру данных для любого взаимодействия между двумя сервисами, будь то REST API, сообщение в Kafka или WebSocket. Принцип всегда один: дотошно задокументировать все параметры, их типы, форматы и обязательность.
На примере REST API (можно использовать спецификацию OpenAPI/Swagger):
	•	Пути и операции (Paths & Operations):
	•	Это конкретные адреса (эндпоинты) и действия, которые с ними можно совершать.
	•	Пример: POST /orders, GET /orders/{id}.
	•	Все виды параметров (Parameters):
	•	Path: Параметры, являющиеся частью пути (/orders/{id}).
	•	Query: Параметры для фильтрации или управления (?status=active&limit=10).
	•	Headers: Служебная информация в заголовках (X-Request-ID).
	•	Body: Основное «тело» запроса и ответа, содержащее данные.
	•	Тело запроса и ответа (Request & Response Body):
	•	Это подробная схема данных с описанием каждого поля. Здесь важна максимальная точность:
	•	Типы данных: string, integer, boolean, object, array.
	•	Форматы: uuid, date-time, email и другие.
	•	Обязательность: Какие поля должны присутствовать всегда.
	•	Примеры: Как выглядят корректные данные.
	•	Валидация входных параметров
	•	Критерии проверки корректности входных данных, например чтобы дата создания заказа не могла быть в прошедшем времени и тд
	•	Все возможные ответы (Responses):
	•	Разработчик должен чётко понимать, что система вернёт в любом случае — и при успехе, и при ошибке. Чаще всего вариант успешного ответа один, а ошибочных — несколько.
	•	Успешные ответы (2xx): 200 OK, 201 Created и т.д.
	•	Ошибки клиента (4xx): 400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found.
	•	Ошибки сервера (5xx): 500 Internal Server Error.
	•	Примеры запросов и ответов (когда говорю про выделенное желтым - надо показать картинку снизу) 

На примере Kafka
Что описываем:
	•	Топик (Topic): Название «канала», в который публикуется событие (orders.events).
	•	Схема сообщения (Message Schema): Так же, как и для REST, мы детально описываем все поля, их типы, форматы и обязательность. Для этого часто используется JSON Schema.
	•	Пример: Для события OrderCreated в схеме будут поля orderId (uuid), userId (uuid), amount (decimal), timestamp (date-time).
Главное, что нужно запомнить: неважно, какую технологию вы используете. Ваша задача как аналитика — обеспечить, чтобы обе стороны (отправитель и получатель) имели одинаковое, кристально ясное представление о структуре передаваемых данных.
Второй артефакт детализации интеграции — Алгоритм работы
Если контракт отвечает на вопрос: «Что мы передаём и получаем и в каком формате?», то алгоритм отвечает на вопрос: «Что происходит между запросом и ответом?»
Алгоритм описывает внутреннюю логику endpoint или процесса обработки сообщения.
Его можно оформить в двух видах, хорошая практика - использовать оба:
	•	текстовый алгоритм шагов (понятно даже без схем),
	•	sequence diagram (визуализация, которая даёт разработчику однозначное понимание). (когда говорю про выделенное желтым - надо показать картинку снизу)  
Что включать в алгоритм:
	•	Триггер — что запускает процесс (запрос в REST, событие в Kafka, пользовательское действие).
	•	Обработка — какие шаги выполняются внутри (обращения к БД, внешним сервисам, бизнес-правила).
	•	Альтернативные сценарии — при каких обстоятельствах у алгоритма могут быть развилки и описание логики внутри этих развилок
	•	Ошибки — где и какие могут возникнуть, что система должна делать (retry, возврат кода ошибки).
	•	Завершение — что именно возвращается инициатору
Хорошие практики описания алгоритма
	•	Описывать всегда — даже если процесс кажется простым (иначе разработчики могут трактовать по-разному).
	•	Одна sequence diagram на один endpoint/топик — не сваливать несколько сценариев в одну схему.
	•	Указывать ветвления — что происходит в случае успеха и в случае ошибки.
	•	Дробить — если внутри один endpoint вызывает 10 сервисов, лучше вынести блоки отдельно (например: Payment flow, Bonus flow).
	•	Указывать idempotency и retry прямо в алгоритме.
	•	Версионировать алгоритмы вместе с контрактом (v1, v2 и т.д.).
	•	Привязывать к бизнес-правилам — описывать, где именно применяется логика (например, «если сумма > 1000, отправить уведомление»).
	•	Не забывать про async последствия — в алгоритме писать, что «событие отправлено в Kafka → далее обрабатывается другим сервисом».
📌 В итоге:
	•	Контракт даёт разработчику «что на вход / что на выход». 
	•	Алгоритм (sequence) даёт понимание «что происходит внутри и какие шаги выполняются».   Вместе эти два артефакта снимают 90% вопросов у разработчиков и делают интеграцию однозначной. 
Пример 1. REST endpoint создания заказа
Endpoint: POST /orders — создание заказа.
Алгоритм шагов:
	•	Триггер — клиент отправляет POST /orders с телом заказа.
	•	Сервис Order Service валидирует токен (обращается к Auth Service).
	•	Проверяет, что заказ с таким orderId ещё не существует.
	•	Записывает заказ в БД orders.
	•	Публикует событие order.created.v1 в Kafka для последующей обработки сервисами складов и доставки
	•	Возвращает клиенту 201 Created с orderId и статусом.
	•	Ошибки:
	•	дубликат заказа → 409 Conflict;
	•	невалидный запрос → 400 Bad Request;
	•	проблемы с БД → 500 Internal Server Error. (когда говорю про выделенное желтым - надо показать картинку снизу)  

🔑 Главная цель аналитика:
Превратить абстрактную связи между сервисами в однозначное описание интеграции, чтобы разработчики понимали:
	•	что интегрируется,
	•	как это будет работать,
	•	какие технологии использовать,
	•	какие сценарии и ошибки учесть.
	•	какие контракты данных стоит использовать
Шаг 5. Работа с данными (Data Design)
	•	Первая задача — подобрать правильный инструмент для хранения данных. Выбор зависит от конкретной задачи,характера нагрузки и стандартов, принятых в компании.
Здесь мы ориентируемся на сценарий использования и технологический стек компании:
Тип Задачи
Подходящая технология
Ключевые свойства
Транзакционные операции (покупки, работа с балансами, остатками)
Реляционные СУБД (SQL): PostgreSQL, MySQL
ACID-транзакции, надёжность, строгость схемы.
Кэширование (ускорение доступа к часто запрашиваемым данным)
In-memory СУБД: Redis, Memcached
Высочайшая скорость чтения, хранение данных в оперативной памяти.
Высоконагруженная аналитика (обработка больших объёмов данных)
Колоночные СУБД: ClickHouse
Сверхбыстрые аналитические запросы, оптимизация под чтение.
Хранение документов (гибкая структура, например, профили пользователей)
Документо-ориентированные СУБД (NoSQL): MongoDB
Гибкость схемы, горизонтальное масштабирование.
Вы, как аналитик, должны проанализировать потребности и предложить наиболее подходящий тип хранилища.
📌 Хорошая практика: не стоит брать экзотическую СУБД, если в компании нет экспертизы для её поддержки.
📌 Хорошие практики проектирования данных:
	•	Сначала логическая модель → потом физическая. Не прыгать сразу в СУБД
	•	Нормализация до 3NF (убрать дубли, но не усложнять).
	•	Денормализация только при необходимости (высокие нагрузки).
	•	Версионирование схемы БД вместе с кодом.
	•	Фиксировать бизнес-ограничения не только в коде, но и в БД (например, уникальность email — в виде UNIQUE).
	•	Не допускать общей БД для нескольких сервисов в микросервисной архитектуре — у каждого сервиса своя база.
Логическая модель данных
Это бизнес-уровень — мы описываем сущности, их поля и связи между ними, без деталей конкретной СУБД.
Пример:
	•	Сущность Пользователь (User) — поля: id, имя, email, дата регистрации.
	•	Сущность Заказ (Order) — поля: id, дата, сумма, статус.
	•	Связь: у одного пользователя может быть много заказов, но у одного заказа может быть только 1 пользователь, то есть один ко многому
📌 Артефакт: ER-диаграмма (Entity-Relationship Diagram) с сущностями, атрибутами и связями (1:1, 1:many, many:many).
Пример:
Физическая модель данных
Это уже технический уровень, привязанный к конкретной СУБД. Здесь важно:
	•	Указать типы данных (например, bigint vs int, uuid vs varchar).
	•	Прописать ограничения (NOT NULL, UNIQUE, CHECK).
	•	Учесть индексы (поиск по email → индекс на поле email).
	•	Учесть максимальные размеры (например, varchar(50) для названия товара).
📌 Хорошая практика: общаться с бизнесом и уточнять реальные ограничения (например, длину названия товара, описания товара и тд).
Транзакционные свойства
Если система связана с балансами, заказами или остатками — проектируем транзакции:
	•	Какие таблицы участвуют.
	•	Как обеспечивается целостность (ACID).
	•	Какие шаги атомарные (либо выполняются все, либо откатываются).
Пример:
При создании заказа одновременно:
	•	Создаётся запись в таблице Orders.
	•	Списываются остатки в Stock.
	•	Фиксируется транзакция в Payments.
Все шаги должны быть либо успешными, либо откатиться (rollback).
Собеседования
Теперь перейдём, наверное, к самому важному вопросу: как всё это выглядит на собеседованиях? Задания и вопросы можно условно поделить на две большие группы: теоретические и практические.
Популярные теоретические вопросы
Цель этих вопросов — проверить глубину ваших знаний и понимание, почему та или иная технология или подход были выбраны. Вы должны не просто знать определения, а понимать тонкости, плюсы и минусы каждого решения.
	•	Брокеры сообщений:
	•	«В чём принципиальная разница между Kafka и RabbitMQ?» (Проверяют понимание разницы между потоковой обработкой событий и очередями задач).
	•	Виды интеграций:
	•	«Синхронная vs. Асинхронная: приведите примеры, когда вы выберете тот или иной подход».
	•	«REST против gRPC: когда что лучше использовать?»
	•	Архитектурные стили:
	•	«Назовите плюсы и минусы микросервисов и монолитов».
	•	«В каком случае вы бы порекомендовали не использовать микросервисы?»
	•	Масштабирование:
	•	Сервисов: «Горизонтальное против вертикального масштабирования: в чём разница? Что такое балансировщик нагрузки и зачем он нужен?»
	•	Баз данных: «Что такое репликация, партиционирование и шардирование? Какую проблему решает каждый из этих подходов?»
	•	Основы баз данных:
	•	«Расшифруйте принципы ACID».
	•	«В чём ключевая разница между SQL и NoSQL базами данных? Приведите примеры задач для каждого типа».
Типовые практические задачи
Цель этих задач — посмотреть, как вы мыслите в условиях, приближенных к реальным. Здесь оценивается не столько финальный "правильный" ответ, сколько ваш подход: умение задавать уточняющие вопросы, начинать с требований и обосновывать свои решения.
	•	Проектирование данных:
	•	«Нарисуйте ER-диаграмму для сервиса аренды самокатов / каршеринга / системы бронирования отелей».
	•	Оценка нагрузки (Capacity Planning):
	•	«Давайте рассчитаем примерную нагрузку и объём базы данных для сервиса сокращения ссылок типа bit.ly на 10 миллионов пользователей».
	•	Проектирование API:
	•	«Спроектируйте API (опишите эндпоинты, методы, модели запросов и ответов) для процесса создания заказа в интернет-магазине».
	•	Архитектура верхнего уровня:
	•	«Нарисуйте карту архитектуры для системы онлайн-голосования / простого аналога Instagram».
	•	Детализация процессов:
	•	«Нарисуйте Sequence-диаграмму для процесса онлайн-заказа еды / процесса выдачи кредитной карты в банке». (когда говорю про выделенное желтым - надо показать картинку снизу) 

Как видите, это те самые практические задачи, которые мы разбирали в предыдущих модулях. Умение быстро и уверенно выполнять их на собеседовании — прямой показатель вашего Senior-уровня.
У меня есть примеры задачи и их разборы в канале, если надо я могу их вытащить в текстовую версию или в отдельный телетайп
Заключение
Ну и в заключение скажу: для вашего профессионального роста и высокой зарплаты, умение проектировать системы — это один из самых необходимых навыков сегодня.
Теперь, когда у вас есть теоретическая база, давайте перейдём к самому главному — к практике. Я дам вам небольшое, но очень важное «домашнее задание».
Ваш план действий: Практические шаги
Если вы уже работаете аналитиком, то у вас есть две возможные развилки, в зависимости от ситуации в вашей компании.
1. Если у вас в компании «документационный хаос» (процессы и документация описаны плохо или отсутствуют):
Поздравляю, у вас идеальная «песочница» для тренировки!
	•	Станьте «картографом»: Возьмите на себя инициативу и опишите ваши системы, используя модель C4 (начните с Уровня 1 и Уровня 2).
	•	Опишите ключевые процессы: Выберите 2-3 самых важных бизнес-процесса и задокументируйте их с точки зрения системного взаимодействия. Это тот же use case, но на уровне алгоритмов и Sequence-диаграмм.
	•	Опишите API: Задокументируйте несколько ключевых эндпоинтов по той структуре, что мы разбирали (контракт, алгоритм).
	•	Попросите ревью: Обязательно покажите результат одному или нескольким опытным разработчикам. Ваша цель — получить обратную связь на соответствие с действительностью. Это бесценный опыт.
2. Если у вас в компании с документацией всё хорошо:
Отлично, у вас есть возможность учиться у лучших и постепенно вливаться в процесс.
	•	Погрузитесь в существующую документацию: Изучите, как спроектированы ключевые системы. Разберитесь в принятых в компании подходах и стандартах.
	•	Станьте «тенью» на архитектурных встречах: Начните ходить на встречи по проектированию систем. Старайтесь их записывать (если это разрешено), а потом в спокойной обстановке разбираться в незнакомых терминах и подходах.
	•	Попросите «боевую» задачу: Обратитесь к вашему менеджеру или техлиду с просьбой дать вам небольшую, некритичную задачу на проектирование новой доработки, конечно же, с последующим ревью от старших коллег.
3. Если вы сейчас не работаете или хотите больше практики:
Разберем одну задачку, но для двух разных нефункциональных требований.
“Есть клиент, он через фронт обращается к нашему сервису, дает свои паспортные данные. Наш сервис обращается к другому сервису запрашивая необходимые дополнительные данные. После этого на нашем сервисе модель вычисляет кредитный рейтинг и отправляет клиенту. Приведите два примера решения для двух кейсов:
	•	С момента запроса клиента до ответа модели должно быть не больше 5 секунд
	•	С момента запроса клиента до ответа модели должно быть не больше 30 секунд
Решение включает в себя небольшую архитектурную схему и sequence диаграмму
Попробуйте сначала решить ее самостоятельно, а потом сверьтесь с решением задачи на бусти.
Это самые практичные советы, которые позволят вам не просто прослушать курс, а уже сегодня начать что-то делать и двигаться в правильном направлении. Главное — не останавливайтесь и применяйте знания на практике. Успехов!
Заключение (2 мин)

Глава 5. Системный дизайн в backend-разработке
Спикер: Сергей
Часть 1. Системный дизайн на собеседованиях и в работе backend-разработчика
Привет, меня зовут Сергей Филичкин, я ментор и python backend разработчик с опытом работы от мелких компаний, где к системному дизайну относятся халатно, до компаний с десятками тысяч сотрудников, где без хорошо спроектированной системы компания может понести многомиллионные (если не миллиардные) убытки

И сразу же нужно понять, системный дизайн для бэкенд-разработчика — это не какая-то теория из статей и учебников, это крайне важный практический навык, от которого реально зависит масштабируемость, надёжность и безопасность продуктов. Если фронтенд отвечает за пользовательский интерфейс, а мобильная разработка — за работу на устройствах, то бэкенд формирует некий фундамент системы: обработку данных, бизнес-логику, интеграции и инфраструктуру.
Моя глава будет разделена на 4 части, где в первой я расскажу про системный дизайн на собеседованиях и в работе. Во второй про масштабирование и нагрузку. Третья будет посвящена балансировке нагрузки. И четвертая – хранению данных

Итак, в первой части мы разберём:
	•	какие зоны ответственности есть у бэкенд-разработчика в системном дизайне;
	•	что ожидают на собеседованиях;
	•	какие типовые задачи дают и что именно оценивают интервьюеры;
	•	и какие архитектурные паттерны и практики нужно обязательно держать в голове.
Итак, давайте рассмотрим, за что именно отвечает backend-разработчик при проектировании системы.
И коротко поговорим основные зоны ответственности:
Архитектура и управление данными
Бэкенд-разработчики отвечают за то, где и как хранятся данные. Вы проектируете схемы Базы Данных, которые должны масштабироваться от тысяч до миллионов записей, обеспечиваете целостность данных в распределённых системах и оптимизируете запросы. Сюда также входит выбор между SQL и NoSQL базами данных, проектирование индексов и внедрение стратегий миграции 
Чтобы понять о чем эта зона — Можете задать себе вопрос: “что будет, если база внезапно упадет”? Это вам поможет правильней спроектировать систему 

Проектирование и интеграция API
Это про то, как сервисы общаются между собой и снаружи. Вы делаете REST или GraphQL API, следите, чтобы новые версии не ломали старые клиенты, и обрабатываете ошибки сторонних сервисов.
Можете проверять простую вещь: что произойдет, если сервис ответит с задержкой и как это скажется на пользователе

Реализация бизнес-логики
Тут вы превращаете бизнес-требования в уже реально работающий код. Например, проектируете платёжный процесс с учетом транзакций или делаете рекомендательную систему для миллионов пользователей и гарантируете корректную валидацию данных на всех точках входа. 
Важно продумать “крайние случаи” — это те самые редкие ситуации, когда часто все и ломается. Например клиент шлет вам один и тот же запрос несколько раз или один из микросервисов или базы данных получил запрос, но ответ не вернулся из-за timeout и вы теперь не знаете применился ли эффект

Инфраструктура и производительность
Бэкенд-разработчики проектируют системы под нагрузку и обеспечивают надёжность. Вы внедряете кэширование для ускорения откликов, проектируете шардинг и партиционирование БД, добавляете мониторинг для раннего обнаружения проблем и создаёте CI/CD пайплайны для безопасных и частых релизов
По сути, это будет вашей некой страховкой от внезапных ночных “вызовов на работу” и срочных релизов, если вы все сделаете правильно


При проектировании нужно помнить, что ваши решения имеют долгосрочные последствия:
	•	Например неудачная схема БД может тормозить систему годами, всего из-за одной ошибки в проектировании;
	•	неэффективный API так же будет тормозить весь продукт и может стать узким местом для всех пользователей;
	•	ошибка в безопасности может привести к катастрофическим последствиям и убыткам в десятки миллионов рублей, а иногда и долларов
Именно поэтому на этот этап уделяется особое внимание, и об этом спрашивают на собеседованиях. Причем, чем на более высокий уровень разработчика вы претендуете, тем больше доля вопросов, направленных на системный дизайн.

 Теперь давайте поговорим что от вас ожидают на собеседовании Обычно процесс собеседования состоит из нескольких этапов, но акцент делается на области, где бэкенд-разработчик приносит наибольшую ценность: работа с данными, масштабируемость и надёжность. Обычно интервью проходит так: 
	•	Вы получаете задачу на проектирование — например, «Спроектируйте социальную сеть» или «Сервис такси». При получении такого задания, вам крайне важно в первую очередь задавать правильные уточняющие вопросы: какой объём данных, какие требования к консистентности, SLA  и так далее. Это покажет, что вы смотрите на задачу комплексно и не бежите реализовывать сервис без полного понимания что и для кого вы делаете. 
	•	Дальше проектируете архитектуру — сначала рисуете общую схему, затем делаете акцент на бэкенд: взаимодействие сервисов, хранение данных, отказоустойчивость, модель согласованности. Главное идти от общего к частному, сохраняя общую картину. 
	•	После этого оптимизируете сделанное решение — проработка Базы Данных, архитектуры сервисов, других оптимизаций. Могут попросить нарисовать схему таблиц, описать алгоритмы бизнес-логики или объяснить, как выдержать всплески нагрузки.
	•	И в конце могут быть уточнения — про безопасность, эксплуатацию, альтернативные подходы. Например: почему выбрали SQL, а не NoSQL? Как работает аудит? Что с логированием?
Интервьюеру важно увидеть, что вы не просто знаете инструменты, а умеете рассуждать и объяснять свои решения.

При проектировании архитектуры обращайте внимание на:
	•	Потоки данных — что, как и куда движется, какие преобразования происходят.
	•	Границы сервисов — правильная декомпозиция на сервисы с балансом между монолитом и микросервисами.
	•	Необходимо Продумать структуру Базы Данных — выбор технологии (SQL, NoSQL, графовые, time-series, поисковые), важно проектировать схему под запрос сервиса, который вы реализуете и дальнейший рост нагрузки.
	•	И, конечно, на производительность — кэширование, оптимизация запросов, эффективное взаимодействие сервисов.

Также важно упомянуть архитектурные паттерны, безопасность и надежность, а именно:
	•	Что выбрать: монолит или микросервисы — ваша задача — найти баланс простоты и гибкости в конкретном случае
	•	про событийную архитектуру — это, кстати,  хороший способ сделать систему менее зависимой между частями, более масштабируемой и устойчивой.
	•	Согласованность данных: где-то нужна строгая, где-то можно обойтись частичной — выбирайте под конкретную задачу.
	•	Безопасность: не забывайте про аутентификацию, авторизацию, шифрование, защиту от атак и нормальное логирование с аудитом.
	•	и надёжность: ретраи, circuit breaker, health-check — всё это нужно, чтобы система не падала при первом же сбое. А резервирование и disaster recovery помогут быстро восстановиться, если всё-таки что-то пойдёт не так.

На самом деле на собеседованиях часто дают типовые задачи, поэтому я бы рекомендовал их разобрать заранее. Вот некоторые из таких задач:
	•	URL Shortener — проектирование сервиса сокращения ссылок под миллиарды записей, с кэшированием, шардированием и аналитикой.
	•	Чат — реализация real-time коммуникации: доставку сообщений, хранение истории, оффлайн-очереди, масштабирование групповых чатов.
	•	Лента соцсетей — генерация персонализированных фИдов под миллионы пользователей с балансом между актуальности данных и производительностью
	•	Платёжная система — здесь обязательно нужно рассказать про транзакции с ACID, борьба с мошенничеством (т.е. уделить внимание безопасноти), интеграции с провайдерами, аудит.
	•	Видео-платформа — часто хотят поговорить про загрузку, CDN, подписки, рекомендации, аналитику.
Готовясь к собеседованию, попробуйте спроектировать подобные системы и нарисовать их схему.
И обязательно отвечая на вопросы, учтите, что нужно делать это в диалоге, описывая ваши действия и задавая уточняющие вопросы, если они появятся. 

Как пример можно рассмотреть супер простую схему сокращателя ссылок. Не всегда ваша система должна использовать 30 компонентов, 10 микросервисов, 5 баз данных и тд. Важно показать именно такую схему, которая применима для задачи после уточнения требований. Если вам все-таки сказали, что система должна выдерживать миллонные нагрузки, то схема могла бы выглядеть как-нибудь так

Рисунок. Схема архитектуры одного из примеров: URL-shortener. Перерисовать в едином стиле. 
Комментарий к этому рисунку(для : Здесь пользователь отправляет запрос, на
фронте, который может быть, например на (Next.js или React)
	•	Форма для ввода длинной ссылки.
	•	Отображение короткой ссылки и статистики (клики, переходы).
Дальше API Gateway / Load Balancer
	•	Балансирует нагрузку между бекенд-сервисами.
Далее уже наш Backend, который может быть написан на чем угодноFastAPI / Django / Node.js)
	•	URL Service — создаёт короткую ссылку, хранит соответствие.
	•	Redirect Service — по короткой ссылке делает редирект на оригинал.
	•	Analytics Service — может собирать статистику кликов.
Database (PostgreSQL / MySQL / Redis)
	•	Таблица urls → хранит mapping short → long URL, дату создания, owner.
	•	Таблица clicks → хранит события переходов (для аналитики).
Cache (Redis / Memcached)
	•	Кэширует короткие ссылки для быстрых редиректов.
Message Broker (Kafka / RabbitMQ)
	•	Асинхронная отправка событий кликов в Analytics Service.
Если вы объясните зачем здесь нужен каждый компонент, то интервьюеру уже может этого хватить

Как я уже говорил — на интервью не обязательно реализовывать очень сложную схему, чаще всего интервьюер обращает внимание на следующие пункты:
	•	Системное мышление: видите ли вы общую картину и взаимодействие компонентов? Думаете ли про сбои и крайние случаи?
	•	Моделирование данных: умеете ли проектировать оптимальные схемы БД, выбирать подходящие типы баз, объяснять шардирование?
	•	Масштабируемость: знаете ли вы типичные узкие места, умеете ли проектировать горизонтально масштабируемые решения?
	•	Практичность: умеете ли обосновать выбор технологий, учитывать эксплуатационные риски.

Как лучше готовиться и вести себя на собеседовании:
	•	Обязательно изучите распределённые системы, базы данных, кэширование, безопасность и масштабируемость.
	•	На собеседовании уточняйте требования, покажите системное мышление, аргументируйте выбор технологий, рисуйте схемы, обсуждайте trade-off’ы.
	•	Не торопитесь сразу погружаться в детали реализации без архитектуры, не игнорируйте нефункциональные требования, не придумывайте то, что не нужно этой системы (это называется оверинжиниринг)

Часть 2. Масштабирование и нагрузка: продвинутая часть
Теперь давай поговорим о масштабировании
Для backend-разработчика понятие масштабируемости выходит далеко за рамки “простого добавления серверов, когда пользователей станет больше”. На практике нужно проектировать системы так, чтобы они оставались стабильными, предсказуемыми и управляемыми под высокими нагрузками. Важно уметь отличать разные виды масштабируемости, понимать узкие места инфраструктуры и применять проверенные архитектурные решения.
Давайте разберём, какие вообще бывают виды масштабирования.
	•	Вертикальное масштабирование (Vertical Scaling, Scale Up)
	•	Это когда мы усиливаем один сервер: добавляем CPU, память, более быстрые диски. Такой подход подходит для монолитов или баз данных, которые сложно разделить на части. Но нужно понимать, что у него есть предел — “железо” нельзя добавлять  бесконечно.

	•	Горизонтальное масштабирование (Horizontal Scaling, Scale Out)
	•	Добавляем новые узлы — серверы или контейнеры. Это уже ключевой подход для современных распределённых систем. Чаще всего Используется вместе с балансировщиками, распределёнными БД и очередями сообщений.
	•	Например «ВКонтакте» изначально работал на одном PostgreSQL, но с ростом аудитории пришлось масштабироваться через десятки серверов и собственные системы хранения. Сегодня VK — это тысячи узлов, которые балансируют нагрузку между собой.

	•	И можно дополнительно отметить — Эластичное масштабирование (Elasticity)
	•	Это когда система сама подстраивается под нагрузку: ресурсов становится больше, когда нужно, и меньше, когда трафик падает. Типичные примеры — AWS Auto Scaling, Kubernetes Horizontal Pod Autoscaler.
	•	Главное помнить, что у облаков есть так называемый “cold start” — увеличение ресурсов не мгновенное.
	•	Многие компании использует Kubernetes для автоматического масштабирования в пиковые дни — «Чёрная пятница», «11.11» и так далее.

Давайте теперь рассмотрим что реально применяется в продакшене
1. Шардинг (Sharding)
Это когда мы Разделяем данные по нескольким базам или кластерам.
Пример: часть пользователей, которых мы распределяем по определенным правилам, хранится в одном шарде, остальные — в другом.
Проблемами здесь могут быть – балансировка между шардАми, миграция данных, перекрёстные запросы.

2. Репликация (Replication)
Здесь Данные просто копируются на несколько узлов.
Например Master-Slave (или Primary-Replica): один узел принимает записи, остальные только читают.
или Multi-Master: все узлы могут принимать записи, но сложнее обеспечить согласованность.

3. Event-driven архитектура и очереди сообщений
RabbitMQ, Kafka, и другие брокеры — позволяют не ждать синхронных ответов и помогают разгрузить систему. По сути Бэкенд ставит задачу в очередь, и дальше она обрабатывается асинхронно.
Благодаря этому повышается устойчивость и это нам позволяет масштабировать отдельные компоненты по мере необходимости

4. CQRS и Event Sourcing
	•	CQRS (Command Query Responsibility Segregation) разделяет команды (например запись) и запросы (например чтение).
	•	Такой подход позволяет оптимизировать разные части системы под конкретные сценарии.
	•	Event Sourcing же сохраняет все изменения в виде событий, что упрощает репликацию и восстановление состояния.
	•	По сути, вы храните “историю” системы, а не только её текущее состояние.
5. Кэширование на всех уровнях
	•	Вы можете кэшировать на уровне приложения(application-level caching): внутри backend (например, через библиотеки, в python это может быть functools.lru_cache или, например, Redis).
	•	Или использовать распределенное кеширование Redis Cluster, Memcached.
	•	Или же CDN  для статики и API-ответов

И Для backend-разработчика критично понимать, что именно становится узким местом, чтобы понимать по какой стратегии идти
Давайте посмотрим по типам нагрузки и возможные узкие места
	•	CPU-bound задачи
	•	Это по сути вычислительные задачи (шифрование, машинное обучение, обработка изображений).
	•	Решением здесь может быть параллелизм, асинхронность, специализированные worker-узлы.
	•	Memory-bound
	•	Большие объёмы данных в оперативной памяти (in-memory кэш, большие коллекции).
	•	Решение – оптимизация структур данных, распределённые кэши (Redis Cluster, Memcached).
	•	I/O-bound задачи
	•	Это когда происходит чаще всего ожидание дисковых операций или сетевых вызовов.
	•	Решение: асинхронный Input/Output, batching, кэширование, использование очередей сообщений. 
	•	И Network-bound
	•	Ограничение по пропускной способности сети или задержкам.
	•	Решение: сжатие данных, gRPC вместо REST, CDN для статических ресурсов. 


Так, давайте теперь предположим, что мы уже все сделали, но система получилась непростая, постоянно возникают различные проблемы по мере увеличения нагрузки или под воздействием внешних факторов. Что мы можем сделать тут? 
Есть несколько стратегий, но вы должны понимать, что это всегда компромисс. Не на все готов будет пойти бизнес, но вы должны знать об этих вариантах и как минимум предлагать их
	•	Graceful Degradation (Постепенное ухудшение)
	•	При перегрузке часть функционала можно постепенно отключать (например, отключаем рекомендации, но бронирования работают).
	•	Rate Limiting и Throttling
	•	Здесь мы ограничиваем число запросов от одного клиента.
	•	Чаще всего это можно сделать с помощью Nginx, Envoy, API Gateway.
	•	Backpressure (бэкпрэшэр)
	•	Механизм, когда система сообщает клиенту или producer’у, что не может принять больше данных.
	•	Используется в Kafka или, например, в gRPC streaming.
	•	Circuit Breaker
	•	Если сервис недоступен, временно «разрываем цепь», чтобы не заспамить его новыми запросами.
	•	Реализуется через библиотеки (Hystrix, Resilience4j) или на уровне gateway.
	•	Bulkhead Pattern (Переборки)
	•	Разделяем ресурсы, чтобы сбой одного компонента не смог уронить всю систему целиком
	•	Например, мы можем выделить отдельные worker-пулы под разные типы задач.

Некоторых этих вещей можно было бы избежать, если бы у вас было заложено observability (наблюдаемость за системой), чтобы быть готовым локализовать источник проблемы и найти пути увеличения быстродействия системы, 
За чем мы могли бы наблюдать, чтобы заранее обнаружить проблемы, вот некоторые из ключевых параметров:
	•	Latency (время ответа). Можно наблюдать как за отдельными api, так и за средним временем ответа 
	•	Throughput (RPS/QPS) — количество запросов в секунду. Здесь мы могли бы своевременно увидеть повышающуюся нагрузку и предпринять необходимые меры
	•	Error Rate — процент ошибок. Может у вас стало что-то постоянно падать, а вы об этом не знали. Такая метрика поможет это заметить
	•	Resource Utilization — загрузка CPU, памяти, диска, сети.
	•	Saturation — насколько близки ресурсы к пределу (очереди, соединения).
Для этого используются специализированные инструменты, например: Prometheus + Grafana, OpenTelemetry, ELK/EFK стеки, Jaeger для трассировки. Подробно про них здесь мы не будем говорить, но в комплексных системах они всегда присутствуют.
Рисунок с скриншотом Grafana, где видно состояние системы:


Масштабируемость — это не про “добавим пару серверов и всё станет хорошо”.
Это про то, как заранее построить систему, которая выдержит рост — пользователей, данных и трафика — без падений и лишних тормозов.
Бэкенд-разработчику важно понимать, где могут быть узкие места, какие паттерны можно применять, и как следить за состоянием системы в реальном времени.
Это и есть настоящее искусство — предвидеть проблемы до того, как они случатся.

Часть 3. Балансировка нагрузки
Когда мы говорим о масштабируемых системах, первым делом встаёт вопрос: как распределить миллионы запросов так, чтобы система не сломалась и оставалась предсказуемой? Именно здесь нам потребуется балансировка нагрузки.
Backend-разработчики часто сталкиваются с этим понятием на собеседованиях, но в реальности роль балансировщиков куда шире, чем просто “раскидывать запросы по серверам”. Балансировка — это способ сделать из множества машин одну устойчивую систему, которая может пережить всплески трафика и сбои.

Давайте попробуем понять зачем вообще нужна балансировка
На самом деле балансировка нагрузки выполняет несколько ключевых функций:
	•	Распределяет запросы между серверами, чтобы ни один не был перегружен.
	•	Обеспечивает отказоустойчивость: т.е. если один сервер упал — трафик просто уходит на другие.
	•	Помогает с плавными деплоями — можно направлять часть трафика на новую версию системы (это еще называют Канареечным развертыванием (canary deployment))
	•	Оптимизирует использование ресурсов: здесь все просто – мощные машины получают больше нагрузки, слабые — меньше.
	•	и делает архитектуру гибкой: можно добавлять и убирать сервера “на лету”, без остановки системы.
Можете представить картину, когда в интернет-магазине на “Чёрную пятницу” трафик вырастает в десять раз. Один сервер бы просто лёг. А балансировщик равномерно раздаёт запросы по пулу серверов — и пользователи даже не замечают, что под капотом идёт настоящая борьба за выживание.

Еще что было бы полезно знать, так это то, что современные балансировщики работают на разных уровнях стека:
А нас интересуют уровни балансировки: L4 и L7
	•	L4 (Transport Layer) — решения принимаются на основе IP-адресов и портов. Это быстрый и лёгкий способ, но он не знает что находится внутри запроса. Такой вариант может хорошо подойти для TCP/UDP сервисов.
	•	и L7 (Application Layer) — анализируются данные прикладного уровня (HTTP-заголовки, пути URL, cookies). Это позволяет маршрутизировать запросы гибко: например, отправлять /api/payments на один сервис, а /api/profile — на другой.

Схема: на картинке несколько серверов API, перед ними балансировщик, распределяющий запросы по разным маршрутам в зависимости от URL.
Что еще полезно понимать: Балансировщики бывают внешние и внутренние. Давайте рассмотрим оба варианта:
	•	Внешние балансировщики ― обрабатывают трафик от пользователей.
	•	Принимают на себя удары DDoS.
	•	Завершают SSL-сессии.
	•	Часто интегрируются с CDN, чтобы вынести часть нагрузки на периферию.
Например: “Яндекс.Музыка” использует CDN и внешние балансировщики — поэтому миллионы людей слушают музыку без лагов, даже в пиковые часы.
	•	Внутренние балансировщики ― распределяют трафик уже внутри микросервисной архитектуры. Работают с минимальной задержкой, поддерживают gRPC и REST.
	•	и есть еще балансировщики баз данных ― отдельная категория.
	•	Они решают задачу разделения запросов на чтение и запись.
	•	Учитывают наличие реплик и мастера.
	•	Обеспечивают согласованность данных.

Помимо всего этого, backend-разработчику также важно понимать, что выбор алгоритма балансировки напрямую влияет на поведение системы.
Одними из самых популярных алгоритмов являются следующие:
	•	Round Robin ― циклическая раздача запросов. Хорошо для stateless-приложений. По сути запросы ходят по кругу между серверами и таким образом распределяются 
	•	Weighted Round Robin ― учитывает "вес" сервера. Более мощный сервер получает больше запросов.
	•	Least Connections ― запрос отправляется серверу с наименьшим количеством активных соединений. Подходит для API с непредсказуемой нагрузкой.
	•	Weighted Least Connections ― более гибкий вариант: серверы получают нагрузку в зависимости от мощности и текущей загруженности.
	•	Response Time-based ― выбор сервера по минимальной задержке.
	•	Consistent Hashing ― полезен для кэширования и шардинга: один и тот же пользователь всегда попадает на один сервер.
Например Consistent Hashing может быть полезен для распределения кешей изображений. Это позволяет пользователю при повторном открытии объявления загружать картинку с того же сервера, снижая задержку.


Здесь может возникнуть вопрос: а как быть с сессиями?
Для этого тоже есть некоторые решения
	•	Session Affinity (липкие сессии) ― пользователь закрепляется за одним сервером. Простое решение, но плохо масштабируется.
	•	Cookie-based affinity ― сервер помечает пользователя через cookie.
	•	IP-based affinity ― думаю из названия понятно – здесь мы пользоваеля просто закреплем по IP, но это может быть ненадежно, т.к. у пользователя может быть не всегда 1 IP
	•	Внешнее хранилище (Redis, DB) ― хороший вариант. т.к. любой сервер может обработать запрос, потому что сессия хранится централизованно.
	•	Stateless-сессии (JWT) ― когда информация о пользователе хранится прямо в токене.
Много где сессии пользователей могут храниться в Redis-кластере. Это позволяет при падении любого сервера продолжать работу без разлогинивания.


И так, у нас все работает, но не можем же мы круглосуточно вручную следить за тем, что сервера у нас нормально функционируют? Поэтому, балансировщик должен уметь проверять состояние серверов:
Здесь стоит упомянуть про Health-check и failover. 
Health check может быть 2-х видов
	•	Active checks ― сам отправляет тестовые запросы.
	•	Passive checks ― анализирует реальные запросы от пользователей.
Если сервер не отвечает, срабатывает failover:
	•	сервер исключается из пула,
	•	нагрузка плавно перераспределяется,
	•	включается circuit breaker, чтобы не отправлять запросы на упавший сервис.
Схема: балансировщик регулярно пингует серверы, отключает те, что не отвечают, и перенаправляет трафик на здоровые.


В микросервисных системах всё ещё интереснее. Могут применяться следующие подходы:
	•	Service Discovery ― это когда сервисы сами регистрируются и сами же снимаются с регистрации (Consul, etcd, Kubernetes).
	•	Health-aware discovery ― маршрутизация только на сервисы, которые гарантированно работают.
	•	Traffic Splitting / Canary Deployments ― при деплое новой версии на эту версию идет сначала идёт 1% трафика, затем 10%, потом 100%.
	•	Feature Flags ― это когда мы часть функционала можем включать и выключать. Т.е. часть пользователей увидит новую функцию, часть — старую.
В крупных приложениях часто используется именно canary deployment — сначала трафик идет на малую аудиторию, если все ок – выкатывают дальше.


Нам, конечно, мало всего этого, и мы еще должны учесть, что если система работает в нескольких регионах, важна еще географическая маршрутизация:
Географическая балансировка может быть следующей:
	•	Latency-based routing ― это когда пользователь идёт в ближайший дата-центр.
	•	Geo-routing ― маршрутизация по геолокации (IP).
	•	и Disaster Recovery ― это аварийное переключение трафика на другой регион.

И, конечно, классика — базы данных, где всё держится на балансировке подключений:
	•	Connection Pooling — управляет количеством открытых соединений.
	•	Read/Write Split — чтение на реплики, запись на мастер.
	•	Failover — если мастер падает, одна из реплик становится новым мастером. 

Что в итоге. Балансировка нагрузки — это не просто "распределить запросы". Для backend-разработчика она означает:
	•	Выбор алгоритма под конкретный сценарий.
	•	Выбрать правильную архитектуру приложений (stateless, централизованные сессии).
	•	Построение грамотной работы с базой данных.
	•	Про мониторинг и диагностику для предотвращения сбоев.
	•	и Интеграцию с микросервисами и DevOps-практиками (canary, feature flags).



Часть 4. Хранение данных
Итак, у нас осталась заключительная глава по системному дизайну в бэкенд-разработке – это хранение данных
Хранение данных — это один из ключевых элементов системного дизайна. Независимо от того, разрабатываем ли мы социальную сеть, онлайн-магазин или аналитическую платформу, данные — это то, без чего чаще всего система не может существовать

Часто можно услышать, что “главное — это выбрать СУБД, и все”. На самом деле этого мало. Нужен комплексный подход и чёткое понимание, как именно данные к нам попадают, обрабатываются, обновляются и защищаются.
В этой части разберём, какие подходы и технологии позволяют хранить данные надёжно, эффективно и с возможностью масштабирования.

Вот пример схемы проекта системы хранения данных некого интернет-магазина: Описание рисунка: показан пример схемы проекта системы хранения данных интернет-магазина(стилизовать под единый дизайн):


Когда мы проектируем хранилище, важно понимать, какие характеристики для нас критичны. Обязательно нужно обратить внимание на следующее:
	•	Надёжность (Reliability): т.е. наши данные не должны теряться. Даже если сервер упал, информация должна восстановиться.
	•	Доступность (Availability): данные должны быть доступны пользователю в любой момент.
	•	Масштабируемость (Scalability): это когда система выдерживает рост количества пользователей и объёма данных.
	•	Производительность (Performance): – операции чтения и записи должны выполняться быстро.
	•	и согласованность (Consistency): пользователи должны получать актуальные данные (но уровень согласованности может варьироваться).
По сути проектирование систем хранения данных обычно сводится к обеспечению оптимального соотношения этих характеристик.
 Давайте посмотрим, как этого добиться и в какой последовательности производится проектирование хранения данных. 
Шаг 1. Нам нужно ппределить цели и сценарии использования данных
Т.е. отвечаем на вопросы
	•	Какие данные будут храниться? (транзакции, логи, медиафайлы, аналитика).
	•	Кто потребляет эти данные? (пользователи, внутренние сервисы, аналитики).
	•	Какие SLA (Service Level Agreement) нужны? (99.9% доступности, согласованность, время отклика). 
Здесь еще нужно помнить про CAP-теорему (Consistency, Availability, Partition Tolerance), подробно останавливаться не будем

 Шаг 2. Необходимо классифицировать данные
	•	По структуре: т.е. это структурированные таблицы, полуструктурированные (какой-нибудь JSON) или неструктурированные (видео, изображения).
	•	По времени жизни: горячие (те, которые активно используются), тёплые (нужны иногда) или холодные (архив).
	•	и по критичности: критичные данные (деньги, заказы), некритичные (логи, кэши).

Далее переходим к Шагу 3, где нам нужно выявить нефункциональные требования
А именно:
	•	Объём данных (в GB/TB/петабайтах).
	•	Скорость роста (ежедневный прирост, пиковые нагрузки).
	•	Согласованность vs доступность (ориентируемся на CAP-теорему).
	•	и Частоту доступа (онлайн-запросы или офлайн-аналитика).


На 4 шаге выбираем модель хранения
	•	Это может быть реляционная БД (SQL): где есть строгая схема и транзакции (PostgreSQL, MySQL, Oracle).
	•	документоориентированная БД (NoSQL): где бы получаем гибкую структуру (MongoDB, CouchDB).
	•	База Ключ-значение где у нас преимуществом будет быстрый доступ (как пример Redis, DynamoDB).
	•	Колоночная, которая чаще всего используется для аналитики (ClickHouse, Cassandra).
	•	Графовая для связей 
	•	или мы выберем объектное хранилище для файлов и медиа (S3, MinIO).
И нужно понимать, что мы можем выбрать несколько баз под каждую задачу


Дальше нам нужно Спроектировать логическую схему. Это уже 5 шаг
	•	Здесь просто определяем сущности и связи (рисуем ER-диаграмму для SQL или JSON-структура для NoSQL).
	•	Определяем ключи (primary key, partition key).
	•	Планируем индексы. 
Пример ER-диаграммы для интернет-магазина(если мелко, можно показать только фрагмент):


Дальше Шаг 6. Проектируем физическую схему
	•	Определяем шардинг, т.е. как будем делить данные (по пользователям, регионам, диапазонам дат).
	•	Думаем про репликацию: master-slave, master-master, read-replicas.
	•	и про Архивирование: перенос холодных данных в отдельное хранилище.

Супер полезно будет Определить стратегии оптимизации. По сути это 7 шаг
	•	Еще раз думаем про индексы: B-Tree, GIN, hash и другие
	•	думаем про Кэширование: Redis, Memcached.
	•	учитываем Pre-aggregation: заранее посчитанные данные (материализованные представления).
	•	и партицирование: деление больших таблиц на части.


На 8 Шаге. Нужно обеспечить отказоустойчивость и подумать про бэкапы
	•	Т.е. будут ли у нас регулярные бэкапы (full, incremental).
	•	что с Geo-репликация.
	•	и Тестирование восстановления системы после сбоев.

Далее Шаг 9. где нам нужно Настроить мониторинг и алерты
	•	Это метрики:, где мы отражаем время отклика запросов, рост базы, нагрузка на реплики.
	•	Логи: медленные запросы, дэдлоки.
	•	и Алерты: падение реплики, нехватка места на диске.
Часто здесь используют Prometheus + Grafana, pg_stat_statements (PostgreSQL).

И завершающий шаг - это обязательно нужно Документировать и пересматривать дизайн по мере роста системы
	•	Для этого желательно иметь документацию схем, политик, SLA.
	•	Проводить Регулярный аудит нагрузки.
	•	и Пересматривать стратегию при росте системы.

Дизайн хранилища данных — это не просто “выбор базы”, а последовательный процесс:
от анализа требований и SLA → к выбору модели хранения → проектированию схем, шардинга, репликации → до мониторинга и резервирования.
 И еще несколько практических советов.
	•	Лучше всего начинать с одной базы (обычно PostgreSQL), затем добавлять NoSQL и кэш по мере роста нагрузки.
	•	Всегда проектируйте резервное копирование и восстановление. Этот совет вас убережет от кучи потраченных нервов
	•	Используйте миграции (Alembic, Django ORM migrations)
	•	и Следите за метриками БД: количество соединений, время запросов, размер индексов.

И еще раз – хранение данных –  это баланс между производительностью, согласованностью и масштабируемостью. Backend-разработчику важно уметь выбирать правильное хранилище под конкретные задачи, комбинировать SQL и NoSQL, проектировать репликацию, шардинг и кэширование.
В этом и заключается искусство системного дизайна — хранить данные так, чтобы они всегда были доступны при этом хранились достаточно надёжно и и их можно было получить максимально быстро. 
Главное не пытайся понять все и сразу, постепенно возвращайся к полученной информации и применяй ее по мере необходимости. Надеюсь, ты обязательно справишься с этой нелегкой темой и спроектируешь такую систему, которая выдержит самые большие нагрузки!

Глава 6. Системный дизайн во frontend-разработке
Спикер: Автушенко Андрей. (Черновой текст вышел на 50 минут чтения)  Оранжевым выделил то, что нужно добавить на монтаже в виде плашек с текстом или картинок 
6.0.0 О себе и о чем будет эта глава 
Привет! Меня зовут Автушенко Андрей. Я занимаю должность TechLead'а фронтенд-команды в компании PREMIER.ONE и также являюсь ментором сообщества Frontend Alliance.
В этой главе я расскажу о том, как системный дизайн применяется во фронтенде, какие задачи он помогает решать и что важно знать о собеседованиях по SD для фронтенд разработчиков. 
Часть 1. Системный дизайн на собеседованиях и в работе frontend-разработчика
  6.1.1. Про SD и какую задачу вам могут дать   System Design часто является одним из последних этапов собеседования на позицию фронтенд-разработчика в крупных IT-компаниях. Обычно эта секция появляется на грейдах Middle+ и Senior.
На ней вам могут предложить как общую задачу вида «Давай спроектируем YouTube», так и более прикладную задачу, привязанную к продукту или команде. Например:
«Давай спроектируем чат между покупателем и продавцом в нашем продукте»
Вопросы на интервью по System Design могут не иметь единственного правильного ответа, а процесс самого интервью может строиться по-разному: в зависимости от кандидата, задачи, интервьюера и специфики продукта.  6.1.2. Зачем проводят секцию SD у фронтенд разработчиков?  Может возникнуть вопрос: зачем вообще проводить эту секцию для фронтенд-разработчиков? Ведь в реальной работе редко приходится проектировать продукт «с нуля» или строить принципиально новые архитектуры.
Честный ответ: крупные компании с большим потоком кандидатов могут позволить себе добавлять такие этапы (как алгоритмы или системный дизайн), чтобы отсеивать только самых сильных специалистов с большим объемом знаний. 
Однако у этой секции есть и прикладная цель. Во время проектирования проверяют:
	•	может ли кандидат построить работоспособную архитектуру приложения на основе собранных требований;
	•	умеет ли он взвешивать плюсы и минусы разных решений;
	•	способен ли кандидат замечать долгосрочные риски и предлагать способы их минимизации.
  6.1.3. Как проходит собеседование у фронтенд-разработчиков  Как проходит собеседование у фронтенд-разработчиков? На самом деле, вас могут спросить практически все то же самое, что и кандидатов на другие специальности, особенно если вы идете на грейд Senior'a или Lead'a.
Но намерено сделать бОльших акцент именно на фронтовой части, например:
	•	выбор стэка и архитектуры клиентского приложение
	•	трейдофы выбранных решений
	•	вопросы ux/ui и доступности
	•	взаимодействие с бекендом
	•	оптимизации производительности
	•	тестировании и контроль качества
	•	мониторинг ошибок и метрики
Картинка на монтаже   В самом начале вам могут дать либо скрин проектируемой системы, или (что чаще) просто расскажут словами, что нужно будет спроектировать.  Можно выделить три основных этапа собеседования:
	•	Понимание поставленной задачи (определение границ проектируемой системы и сбор требований).
	•	Построение высокоуровневого дизайна системы.
	•	Погружение в детали (архитектуры, трейдофы, оптимизации, метрики).
  Понимание поставленной задачи
На интервью вас могут попросить спроектировать:
	•	новостной агрегатор
	•	чат
	•	редактор кода
	•	стриминговый сервис
	•	маркетплейс
	•	приложение для букинга
	•	почтовый клиент и многое другое.
Чаще всего задача приближена к продуктам компании. Например:
	•	в Авито часто дают спроектировать чат между покупателем и продавцом,
	•	в Т-Банк — мессенджер,
	•	в Яндексе — сервис вопросов и ответов (аналог Stack Overflow).
На секции дают задачу без четких требований. Это нужно, чтобы вы сами задавали вопросы на понимание границ проектируемой системы, сценариев использования продукта и собрали функциональные и нефункциональные требования.  От вас ожидают, что вы зададите эти вопросы и учтете собранную информацию при решении задачи.
Примеры таких уточняющих вопросов:
	•	Какую нагрузку должна выдерживать система?
	•	Какие действия будут доступны пользователям?
	•	Важна ли поддержка старых браузеров?
	•	Насколько критично ранжирование в поисковой выдаче?
	•	Требуется ли поддержка пользователей с ограниченными возможностями (a11y)?
В этой главе мы шаг за шагом разберем все этапы системного дизайна на примере проектирования ленты социальной сети (аналог Twitter или Facebook).
Давайте начнем с разбора функциональных и нефункциональных требований.  







Картинка на монтаже

Из нефункциональных требований нас, как фронтендеров, интересуют в бОльшей части пункты 2 и 4, остальное часто решается силами бекенда и девопса  Высокоуровневый дизайн  После того, как вы собрали требования и уточнили границы системы, наступает этап высокоуровневого дизайна.  Первое, что вас могут попросить — это нарисовать UI/UX-дизайн: простой макет или блок-схему интерфейса. 
Картинка на монтаже  
Сейчас вы видите пример такого макета. Сверху у нас есть навигация с кнопкой создания поста и ниже идет лента самих постов с картинкой, текстом и различными кнопками Дальше имеет смысл перейти к дереву компонентов и связям между ними. Нужно отобразить основные компоненты, их взаимодействие и обмен данными через выбранный нами стейт-менеджер или иной механизм. 
Картинка на монтаже  Наш фронтовый проект начинается с корневого компонента App, далее у нас идет основной компонент Feed, который будет отвечать за новостную ленту. Он состоит из двух компонентов - Навигации и Постов.  Компонент Posts это список наших постов, где сам компонент Post состоит из картинки, текста и кнопки реакции.  Внутри навигации у нас есть кнопка создания поста (CreatePost), она состоит из текстового редактора, кнопка для загрузки картинки и кнопка для отправки.  Общение с бекендом будет проходить через ApiClient (тут может быть как самописное решение, так и готовая библиотека, например, tanstack-query).  Так же мы отразили на схеме наш Redux стор, в котором будет храниться написанный, но еще не отправленный пост, чтобы иметь возможность сохранить его в случае перезагрузки страницы или закрытия вкладки  Несмотря на то, что вы «Frontend-разработчик» от вас будут ожидать верхнеуровневый дизайн всей системы — не только клиентской части, но и серверной.  Обычно в схему включают: сервера приложений, балансеры, BFF, кеш, очереди и хранилища данных.  В рамках этой главы мы не будем разбирать дизайн всех возможных подсистем, а сконцентрируемся именно на фронтенде, но для общей картины давайте посмотрим на нашем примере как такая схема может выглядеть.  Картинка на монтаже    Здесь мы можем увидеть сервис постов, сервис фида, сервис User и сервис Relation, который в будущем будет отвечать за логику подписок на ленту других пользователей.  Ещё один частый элемент на интервью - расчет целевой нагрузки на бэкенд. Это помогает понять, удовлетворяет ли спроектированный дизайн собранным требованиям.  Но часто на фронтенд собеседованиях этот шаг опускают. Для полноты картины давайте проведем такой расчет, сильно не погружаясь в детали.      Картинка на монтаже  
Из требований предположим, что количество уникальный пользователей за сутки у нас 1млн, а за месяц - 3млн.  Далее предположим, что в среднем пользователь делает запрос к ленте например 10 раз в сутки, таким образом мы можем расчитать, что у нас будет около 116 запросов в секунду.  Но в реальности основная часть запросов будет приходится на определенные пиковые часы, поэтому нужно отдельно учесть этот фактор, чтобы понять, какой rps должна выдерживать наша система. Теперь давайте рассчитаем объем хранения. Предположим, что в среднем один пост весит 500kb, а пользователь создает в среднем 1 пост раз в 50 дней. Исходя из этих данных получим, что наша система будет занимать 10гигабайт данных в сутки или 3.5 терабайта в год.   Наконец, вас могут попросить описать контракты API-эндпоинтов — какие данные приходят и в каком формате. Давайте разберем на примере нашего кейса.  Картинки на монтаже   Начнем с первой ручки, которая отвечает за создание поста. В эту ручку мы передаем контент, который может содержать html-разметку, это нужно, чтобы сохранить форматирование, которые сделал пользователь в редакторе.  Так же передаем изображение, в котором мы прокидываем ссылку на загруженную картинку, за которую отвечает у нас следующая ручка.   В нее мы передаем изображение в формате base64, далее на бекенде это изображение сохраняется, например, в s3 хранилище и отдается ссылка на загруженную картинку. Это сделано для оптимизации, чтобы не нагружать основную ручку создания поста отдельной логикой загрузки изображения, которое может быть достаточно большим.  В ответе ручки по созданию поста (1 картинка) мы получаем информацию о созданной сущности (айдишник, информацию про автора, сам контент, изображение, информацию о реакциях и прочую метаинформацию).    Теперь перейдем к основной ручке по получению всех постов. Тут мы используем cursor пагинацию, передавая количество получаемых постов и указатель на пост, откуда начинаем брать элементы. В ответе мы получаем массив постов и так же информацию про пагинацию.   (Подробнее про пагинацию и их отличиях я расскажу в части про оптимизации).  
Перейдем к последней ручки, которая отвечает за создании реакции, куда нам нужно передать айдишник поста и тип реакции, например лайк или дизлайк.   Погружение  Теперь перейдем к этапу погружения в детали.  Секция интервью ограничена по времени, поэтому на ней не получится детально обсудить все аспекты проектируемой системы. Поэтому важно согласовать с интервьюером список вопросов для подробного обсуждения. Будет плюсом, если вы сами предложите, какие части системы заслуживают более подробного обсуждения. Следующая часть посвящена полностью вопросам погружения.

Часть 2. Архитектура
 6.2.1. Что такое архитектура и зачем она нужна?  Давайте начнем наше погружение с вопроса "что такое архитектура и зачем она нужна?"  Обычно когда говорят «архитектура фронтенда», чаще всего разработчики думают только о структуре папок или выборе фреймворка. Но на самом деле под архитектурой в современной фронтенде подразумевается куда большее.  Архитектура - это набор правил и концепций, отвечающих за высокоуровневую организацию структуры вашей системы.  Основная цель архитектуры - уменьшить трудозатраты на сопровождение и поддержку системы.  Например, если с выходом каждой новой версии трудозатраты увеличиваются (например, количество затрачиваемых сторипоинтов) или, увеличивается количество багов с каждой новой фичей - это свидетельствует о плохой архитектуре и высокой связанности кода.  6.2.2. Что можно отнести к архитектуре фронтенда?  Что можно отнести к архитектуре фронтенда? На самом деле все, что влияет на организацию проекта и качество кода:
	•	подходы к автоматизации (притиер, линтеры, прекоммиты, CI/CD-процессы)
	•	код-стайл и конвенции, принятые в команде
	•	выбранный стэк
	•	структура папок
	•	разделение кода на слои и модули
	•	организация работы с API и глобальным хранилищем
	•	выбор основной парадигмы программирования и многое другое.
6.2.3. Выбор стэка

Давайте более подробно поговорим про выбор стэка.  Получив задачу, вам нужно определить, какие технологии лучше подходят для ее решения, почему был выбран именно этот стэк и какие существуют альтернативы и трейдофы.  Для повышения качества кода рекомендуется внедрять автоматизацию:
Prettier, Eslint, Stylelint, Tslint, Husky и Commitlint. С помощью Husky можно интегрировать проверки в цепочку Git-коммитов, а Commitlint помогает писать осмысленные коммиты по конвенции и поддерживать порядок в ченжлогах.  Не забываем про TypeScript: статическая типизация повышает DX и уменьшает количество ошибок на этапе разработки. Поэтому это всегда хороший выбор
Как выбрать фреймворк?  Он должен быть актуальным и популярным и вы должны быть уверены, что любая технология, которую вы выбираете - будет актуальна в ближайшие несколько лет и точно не пропадет с рынка.  В 2025 году я бы отдавал предпочтение Vue или React.  Почему не Angular? В СНГ он менее популярен и имеет малый процент использования в крупных компаниях, хотя сам по себе фреймворк отличный. При этом выбор всегда зависит от контекста: если компания использует Angular и команда имеет в нём основную экспертность, стоит выбрать его.
Давайте пройдемся по плюсам-минусам этих трех фреймворков.
React имеет
Плюсы:
	•	Самая большая экосистема и комьюнити.
	•	Больше всего вакансий на СНГ рынке.
	•	Поддержка Meta → долгосрочная стабильность.
	•	Подходит для проектов любого масштаба.
	•	Легкий найм и онбординг.
Минусы: при этом
	•	Это не фреймворк, а библиотека: много решений придётся собирать самостоятельно и строить своего франкенштейна (роутинг, стейт-менеджеры и прочее).
	•	Метафреймворк (Next) развивается хуже, чем его конкурент Nuxt для Vue.
	•	Очень большая конкуренция на рынке. На 1000 вакансий приходится 40-50к резюме кандидатов
Когда выбрать: если нужна гибкость, масштабируемость, и есть опытная команда.

Vue имеет
Плюсы:
	•	Полноценный фреймворк с прокаченной системой реактивности
	•	Более низкий порог входа и простой синтаксис.
	•	Идеальный вариант для запуска MVP.
	•	Декларативная шаблонизация, что повышает DX.
	•	Легкий найм и онбординг.
Минусы:
	•	Меньшее комьюнити, чем у React.
	•	Для enterprise-уровня меньше готовых решений, чем у конкуретнов.
	•	И хоть это и фреймворк, но при этом во вью есть несколько способов решить одну и ту же проблему, что порождает не консистентную кодовую базу и требует насмотренности в команде для больших проектов.
Когда выбрать: если нужно быстро сделать MVP, или команда имеет экспертность именно во Vue.

Angular в свою очередь
Плюсы:
	•	Самый большой «фреймворк с батарейками»: роутинг, DI, RXjs, декораторы и прочее.
	•	Отличная поддержка TypeScript (изначально на TS).
	•	Имеет хорошие архитектурные паттерны из коробки (DI, модули).
	•	Долгосрочная поддержка от Google.
	•	Подходит для больших команд с сотнями разработчиков
Минусы: но имеет
	•	Высокий порог входа, steep learning curve.
	•	Более «тяжёлый» runtime, очень большой boilerplate.
	•	На данный момент самое маленькое компьюнити в СНГ.
	•	Сложнее найти работу или разработчика в команду (4% крупных российских компаний используют Angular, остальные проценты практически поровну разделены между реактом и вью)
Когда выбрать: если у вас большой интерпрайз и есть опытная команда, которая имеет экспертность именно в Ангуляре. Для нашей системы возьмем Vue.  В своем телеграм канале я писал подробный пост о том, почему стоит изучать и переходить на Vue.js в текущих реалиях. Ссылка на канал есть в описании под роликом. Пост можно найти по поиску, вписав “Vue” Зачем вообще выбирать именно фреймворк вместо чистого JS?
Есть определенные критерии, почему большинство компаний пишет на готовых фреймворках.
Фреймворки позволяют стандартизировать и автоматизировать рутинные работы, связанные с данными, переиспользованием изолированных компонентов, реактивностью, условным рендерингом и оптимизацией работы с DOM (virtualDOM и incrementalDOM). 
Это готовый инструмент, который позволяет быстро приносить бизнес-ценность. Изучив фреймворк - ты сразу можешь решать реальные задачи бизнеса и легко переходить между разными проектами, не теряя контекста.
Кроме того, использование фреймворка снижает количество проектных знаний в команде, что упрощает найм, ускоряет онбординг и позволяет новым разработчикам быстрее включаться в работу.  Выбор стейтменеджера
Если на предыдущем шаге вы выбрали Vue, всё просто — используем рекомендованное решение от фреймворка, Pinia.
Если же выбран React, то стоит ориентироваться на актуальные и популярные решения: Redux Toolkit, RTK Query или Zustand. Лично я бы остановился на Zustand — он современный, гибкий и производительный.  Почитать отличия, плюсы и минусы популярных стецтменеджеров на Реакте можно по ссылке в описании:  Топ-5 библиотек для управления состоянием React - https://habr.com/ru/companies/ibs/articles/885868/   Важно понимать: глобально система от выбора стейт-менеджера на фронтенде не изменится — это влияет только на разработческий опыт (DX), но важно понимать плюсы/минусы современных решений и уметь их объяснить на собеседовании.  6.2.4. Выбор режима рендеринга (SPA/MPA/SSG/SSR/ISR)  Если требуется SEO-оптимизация и хорошая выдача в поиске, вместо чистого React или Vue стоит взять их метафреймворки — Next.js или Nuxt.js. Можно рассмотреть и создание своего решения для SSR, но тут важно понимать что не смотря на плюс такого подхода в виде полного контроля над реализацией вы получаете минус в виде повышенного порога входа и дополнительной точки отказа, т.к. это не opensource решение, которое будет валидироваться сообществом.
  Принцип работы SSR заключается в том, что он позволяют отдавать готовую HTML-страницу с сервера, что улучшает SEO и ускоряет начальную загрузку.   Помимо выбора разных режимов рендера, метафреймворки добавляют: удобный file-based роутинг и упрощенную работу с запросами и кэшированием.  Но важно уточнить, что даже если вы по умолчанию выбираете режим SSR, то не все страницы нужно генерировать на сервере.  Страница закрытой зоны (требующие авторизации) обычно не нуждаются в SEO, а соответственно и в серверном рендеринге.  Страницы с редко меняющимся контентом и без персонализации (например, список категорий в приложении для заказа еды) лучше генерировать как SSG — HTML будет формироваться на этапе билда (getStaticProps в Next или routes в Nuxt), что обеспечит очень быстрый рендер.  При этом если требуется периодическая ревалидация статических страниц, используется подход ISR (Incremental Static Regeneration): В Next.js это делается через опцию revalidate, в Nuxt есть аналогичный режим ISR для роутов. Сгенерированные страницы кэшируются на CDN на указанное время, после чего автоматически регенерируются.  
Давайте рассмотрим выбор режима рендеринга на основе нашей задачи.  Картинка на монтаже   Мы хотим, чтобы наши посты хорошо индексировались поисковыми системами. Но поисковики не индексируют бесконечный скролл (а он у нас точно будет для оптимизации).  Они видят только HTML, который пришёл при первом рендере.  Поэтому чтобы решить эту проблему, мы можем отдавать с сервера страницы профилей и отдельные твиты, а саму ленту делать через Client-side rendering (CSR) (таким образом основной контент твитов попадет в ранжирование).  Так же можно оставить SSR для первой порции постов в ленте, а дальше на клиенте догружать следующие порции твитов. Получаем гибридный подход.  При этом хочу обратить внимание, что можно использовать SSR только для поисковых ботов, чтобы улучшить CEO и ранжирование приложения, а реальных пользователь всегда вести на обычный SPA. Такой подход так же часто используется в проектах.
 6.2.4. Выбор архитектуры и методологий  Теперь перейдем к выбору методологий и архитектур. Что у нас есть в мире фронтенда?  1) Классическая архитектура То как у вас строится проект, когда вы просто берете фреймворк.  Компоненты в папке components. Страницы в папке pages. Вспомогательные функции в папке helpers.  Из плюсов: - быстрый старт проекта - минимум барьеров для новых разработчиков -  хорошо подходит для MVP  Минусы: - имеет ограниченное масштабирование. Если проект разрастется, то кодовая база быстро превратится в хаос.  Хороший вариант для маленьких и быстрых проектов, но сильно проигрывает в долгосрочной поддержке.   2) Модульная архитектура  В ней код группируется по модулям или доменам (например, auth/, profile/, feed/). Каждый модуль может содержать свои компоненты, сервисы и стили.  Плюсы: - Масштабируемо: новые модули легко добавлять, при этом модуль изолирует все в себе, поэтому его легко расширять.  Минусы: - Может быть много дублирования.  - Сложно определить"границы модуля".  - Есть риск кросс зависимости модулей;    3) Atomic Design Архитектура ориентирована на UI-компоненты.  Декомпозиция на слои: Atoms (кнопки) → Molecules (формы) → Organisms (блоки) → Templates → Pages.  Из плюсов: - Это очень простая архитектура.  - Отлично подходит для дизайн-систем и UI-китов.  - Легко поддерживать единый стиль во всем приложении.  Главный минус - она никак не решает вопроса бизнес-логики и разделение ответственности  4) Feature-Sliced Design (FSD) Архитектурная методология, придуманная конкретно для фронтенд-приложений, содержащая набор правил и соглашений по написанию кода.  Из плюсов: - Она бизнес-ориентированная: мы выделяем бизнес-сущностей (entities)  и пользовательских сценариев (features).  - Имеет однонаправленный поток данных и иерархию слоев - Изоляция через PublicAPI  - Можно внедрять постепенно и итеративно  Минусы: - Высокий порог входа относительно предыдущих вариантов - Избыточна для маленьких проектов - Требует экспертной команды и договоренностей. -  Могут быть проблемы с кросс-импортами и необходимость в добавлении новых слоев в проект.  Отлично подходит для проектов, но требует обучение и экспертности в команде.   5) Чистая архитектура (Clean Architecture)  Архитектура, которая выделяет слои от бизнес домена до юайя. Бизнес-логика при этом полностью отделена от UI и инфраструктуры. Плюсы: - Максимальная гибкость: можно менять фреймворк/UI без переписывания ядра. - Имеет очень высокую тестируемость и маштабируемост.  Минусы: - Огромный порог входа - Большая сложность и много абстракций - Сильно усложняет найм и онбординг - Плохо подходит для фронтенда, где редко нужно менять фреймворк на лету.  В 99% случаев я бы не рассматривал такой вариант для фронтенд приложений.
Самое главное, что вы должны понять, что выбор архитектуры состоит не в том, чтобы взять "самую сложную и крутую" архитектуру, а чтобы подобрать решение, которые принесет максимальную бизнес-ценность. Поэтому учитывайте:  - экспертность команды - процесс найма и онбординга новых разработчиков  и ментальную сложность решения.
На данный момент в большинстве случаев я бы отдавал предпочтение именно FSD за счет своих преимуществ.  Эту методологию и возьмем для нашего кейсе с лентой новостей.  Картинка на монтаже   6.2.5. Готовая дизайн-система или пишем свою?  Что по поводу дизайн системы? Тут возможности очень обширные. Существует большое количество готовых юай-фреймворков и библиотек. Но их не всегда стоит использовать.
	•	Если проект планируется развиваться и поддерживаться долго, готовая библиотека может сильно ограничивать дизайнеров, а время от времени придётся переписывать компоненты под нужды проекта.
	•	Готовые UI-библиотеки отлично подходят для внутренних проектов, где нет строгих требований к соблюдению брендбука — это позволяет экономить ресурсы бизнеса и дизайна.
	•	Для долгоживущих продуктов лучше инвестировать время и договориться с дизайнерами о внедрении атомарной дизайн-системы и собственного UI-кита.
При этом важно помнить про изоляцию стилей, чтобы избежать конфликтов и утечек: использовать BEM, CSS Modules или CSS-in-JS. В нашем случае будем писать компоненты сами и возьмем классический BEM.
Картинка на монтаже   6.2.6. Архитектурные подходы к разделению проекта  Теперь хочу немного поговорить про способы организации кода в репозиториях.  Есть 4 основные способа организации.   Монолит - всё приложение живёт в одном репозитории и одной кодовой базе.  Плюсы:  - просто деплоить и разрабатывать небольшие и средние проекты - Имеет минимальные накладные расходы на отдел эксплуатации в части организации ci\cd, мониторинга и объема серверных мощностей.  Минусы:  - при росте команды и бизнеса проект превращается в высоко связный проект, где бОльшая часть модулей проекта начинают дублировать друг друга.  - Также вы получаете "блокер" на быстрые обновления различных частей проекта, т.к. деплой происходит одномоментно для всего приложения.  - К скрытым минусам на больших проектах можно отнести проблему кэширования компонентов и страниц.   Микрофронтенды - архитектура, где приложение делится на независимые модули, за которые отвечает отдельная команда и деплоится независимо друг от друга. Каждая команда может использовать свой стэк и конвенции по написанию кода.  Плюсы:  - высокая гибкость - независимое развертывание модулей - низкая связанность компонентнов внутри всего проекта.  Минусы:  - сложная инфраструктура и высокие "косты" на отделе эксплуатации, т.к. каждый микрофронтенд это де-факто, отдельный продукт, требующий своего мониторинга и своих серверных мощностей - эффективно только для очень крупных продуктов с сотнями разработчиками, где альтернативные подходы имеют больше проблем, чем преимуществ.   Монорепозиторий - Один репозиторий, внутри которого есть несколько пакетов. Подходит, когда несколько команд работают над продуктами, в которых можно явно выделить переиспользуемые компоненты в отдельный пакет. Пример - отдельно реализуемые версии для ПК и мобильных устройств: они будут иметь единый API слой, единые хелпер-функции, единый стор, но при этом у них будет разный ui.  Плюсы:  - позволяет переиспользовать общий core для разных клиентов - не нужно отдельно выпускать релиз при изменении общего core Из минусов:  - каждая команда может изменить общий код и «выстрелить в ногу другой команде»;  - требует высокой осознанности команды и строгой дисциплины при работе с "core" пакетом.   NPM-пакет - код оформляется как отдельная библиотека и публикуется в npm (публичный или приватный registry). Актуален, когда одна из команд делает "универсальное" решения для N других команд.  Плюсы:  - Единая точка ответственности переиспользуемого пакета, управляемая независимо от развития потребителей.  - Возможность организовать свой релизный цикл.  Минусы:  - каждый фикс требует выпуска нового релиза.  - Часто возникает проблема с приоритетами в релизах, т.к. бэклог изменений должен учитывать "хотелки" всех потребителей.  - Накладные расходы в виде обратной совместимости.
Выбор подхода зависит от задачи и масштаба проекта.  В нашем случае подходит обычный монолит, а UI-библиотеку будем держать прямо внутри проекта в папке shared/ui по FSD.  Картинка на монтаже   6.2.7. BFF (Backend For Frontend)  Теперь давайте поговорим про BFF. 
BFF расшифровывается как Backend For Frontend. Это слой между фронтендом и бекендом, за который часто могут отвечать именно фронтендеры.
BFF выступает как дополнительная прослойка между различными бэкендами, которые не адаптируют свои контракты под клиентов.  Цель такой прослойки — унификация контрактов и реализация необходимых SLA, например, по кэшированию.
Подходит если:
- У вас есть несколько клиентов (веб, мобилки, смартТВ) и нужно адаптировать данные под разные интерфейсы.
- На бекенде микросервисы, и фронтенду неудобно собирать данные самостоятельно.
- Илм если нужны универсальные кэшированные точки входа
Из плюсов: - оптимизация запросов - гибкость под разные клиенты - централизованная логика обработки данных - и уменьшение костов по сетевым запросам, т.к. BFF чаще всего располагается в одной приватной сети с клиентами. 
Минусы: - Дополнительная инфраструктура и потенциальная критическая точка отказа.  - Чаще всего не предусматривает fallback на изначальные бекенды, из-за чего в случае недоступности BFF все клиенты "умрут".  - Увеличенные затраты на разработку и поддержку.  - Необходима синхронизация фронтенд и бекенд команд.  - При неправильной эксплуатации BFF может превратиться в «толстый бекенд»
В рамках нашей задачи BFF не нужен.  6.2.8. UI/UX и Доступность
Если в нефункциональных требованиях указано, что системой должны пользоваться люди с ограниченными возможностями, необходимо уделить внимание доступности (accessibility).
 Для этого применяются
  - ARIA-атрибуты — для описания элементов интерфейса и их состояния для вспомогательных технологий;
  - tabindex — для корректного управления порядком навигации с клавиатуры. Картинка на монтаже  Давайте рассмотрим доступность на примере нашей системы с лентой постов. У нас есть следующий пример верстки  Заголовок
aria-labelledby="newsfeed-heading" — говорит, что заголовок для всей секции берётся из элемента с id="newsfeed-heading".
 Лайв-уведомления
aria-live="polite" — превращает элемент в живой блок с уведомлениями.  Когда текст внутри этого блока меняется, скринридер автоматически зачитывает его пользователю. polite означает «озвучь, но не перебивай текущую речь». 
Есть ещё assertive (завершай текущий текст и озвучивай этот).
Здесь newsfeed-status будет динамически сообщать, что «Добавлено n новых постов»
newsfeed-loading будет сообщать, что «Загружаем новые посты...».
class="sr-only" делает элемент невидимым для обычных пользователей, но доступным для скринридеров.
Роль ленты и инструкций
role="feed" — это ARIA-паттерн, который обозначает потоковый список постов. Он ожидает дочерние элементы с ролями article или document.
aria-describedby="newsfeed-info" — связывает ленту с блоком информации. Скринридер при фокусе на ленте зачтёт текст из скрытого блока.
Это даёт пользователю дополнительный контекст («в ленте 6 постов, навигация с помощью таба»).
Пост и заголовки
aria-labelledby="post-title" — говорит о том, что озвучиваемым заголовком этого поста будет <h3> с id="post-title". tabindex="0" — делает весь пост фокусируемым, чтобы пользователь мог перемещаться между постами.
Кнопка
aria-label у button — задаёт понятное имя кнопке. Без него скринридер бы попытался прочесть символ и цифру, что не всегда очевидно. Теперь будет текст «Текущее количество лайков - 47».
aria-live="polite" — если число лайков изменится - скринридер озвучит обновление.
aria-haspopup="menu" — указывает, что кнопка открывает меню.
aria-expanded="false" — показывает текущее состояние меню, например, оно закрыто. При открытии меню надо переключать на true.
aria-controls="reactions-menu-post-id" — связывает кнопку с меню, которое она открывает.
aria-hidden="true" — скрывает иконку от скринридеров (иначе они бы дополнительно  озвучили «эмодзи»).
Меню реакций
role="menu" — определяет контейнер как меню.
role="menuitem" — говорит, что дочерние элементы — это пункты меню.
aria-hidden="true" — меню скрыто от ассистивных технологий, пока не будет открыто. При открытии нужно переключать на false.
Вообще, ARIA-атрибутов огромное количество и я предлагаю вам самостоятельно со всеми ними ознакомиться и попробовать применить в работе, если хотите хорошо разобраться с доступностью. 
Часть 3. Оптимизация производительности
  6.3.0. О чем эта часть?  Теперь настало время поговорить про оптимизацию производительности. 
Мы не будем здесь обсуждать как ускорять запросы на бекенде, а сосредоточимся именно на фронтенд-оптимизациях.
Некоторые аспекты мы уже косвенно затронули при обсуждении метафреймворков:
SSR помогает ускорить TTFP (Time to First Paint) — то есть первую отрисовку контента.
Так же мы не будем тут останавливаться на различных код-сплитинге, динамичевских компонентов, tree-shaking, prefetch, мемоизации и оптимизации бандла. 
Всё это часто уже реализуется фреймворком «из коробки» и достаточно просто настраивается.
Итак, что еще может нам помочь в оптимизации?  6.3.1. Оптимизация изображений  Про CDN уже упоминали в прошлых главах. Рассмотрим дополнительные способы оптимизации изображений на клиенте. Картинка на монтаже   
Нужно использовать современные форматы. Предпочтительно WebP вместо JPEG/PNG для экономии трафика и ускорения загрузки. Но учитывайте, что WebP может не поддерживаться в старых браузерах.
Тут нам поможет тэг picture, который позволяет указать сет картинок с фолбэком для старых браузеров и позволяет загружать изображение в зависимости от размеров экрана.
Также можно добавить lazy-loading для картинок. Изображения будут загружаться только при попадании в область видимости пользователя
 6.3.2. Оптимизация пагинации  Существует два основных подхода к реализации пагинации: offset и cursor (keyset).

Offset пагинация.   Картинка на монтаже     Offset пагинация Работает с параметрами: limit (количество элементов на странице) и offset (отступ).
Плюсы:  - можно легко переместиться на конкретную страницу;  - Легко интерпретируется на бекенда: offset вычиляется как (page - 1)* size
Минусы:  - с ростом offset запросы замедляются, так как СУБД должна пройти все записи до нужной позиции.  - Могут дублироваться записи в ленте для часто обновляемых данных (например, соц. сетей).
Например, мы посмотрели 5 постов, но за время пока мы их смотрели появилось еще 5 записей, мы грузим следующую пачку, и у нас произошел отступ и мы видим опять те же 5 постов, которые только что посмотрели. Поправить это можно только дополнительными проверками на фронте (но это костыль).

Cursor Картинка на монтаже     Cursor пагинация в свою очередь работает с параметрами size (количество элементов на странице) и cursor (указатель на id, timestamp или другой специфичный параметр), который указывает от какого элемента получать следующую порцию данных.
Из плюсов:  - у нас нет проблем с замедлением запроса при больших отступах;  - и это отлично подходит для часто меняющихся лент (соц. сети)
Из минусов:  - нет возможности перейти на конкретную страницу;  - И возможны коллизии, когда пропускаются некоторые записи, если использовать указатель по timestamp. (Но данную проблему можно решить используя гибридный указатель)

В нашем случае, так как лента часто обновляется и нам не нужно переходить на конкретные страницы, идеальным выбором будет именно cursor пагинация.
  6.3.3. Бесконечный скролл  Когда речь заходит о подгрузке новых данных, встаёт вопрос: по сколько элементов загружать за один запрос?  
Самый простой вариант — фиксированное число (например, 6 постов).
Но если у пользователя маленький экран и он видит только 2 поста, то это приводит к избыточным ресурсам на запрос.
Более оптимальный подход — динамический: определять количество элементов исходя из высоты экрана.
Например, если пользователь одновременно видит 2 поста, можно запросить именно столько + небольшой запас.
Так же хорошей UX практикой считается, когда пользователь не видит загрузку и лоадеры совсем.
Для этого данные нужно подгружаются заранее — например, когда пользователь дошёл до 80–90% уже загруженного контента.
Для того чтобы это реализовать - у нас есть несколько вариантов.
Intersection Observer API и обычный скролл.
 Картинки на монтаже 
 Intersection Observer - это нативное современное решение, в котором не нужно дополнительно оптимизировать сам обработчик, ведь внутри используется RequestIdleCallback для оптимизацию. Так же у нас есть возможность вешать несколько наблюдателей. Единственный минус - это решение не поддерживается в очень старых браузерах. Поэтому если нам важно их поддерживать, то можно рассмотреть реализацию через обычный scroll и getBoundingClientRect. Такое решение будет работать везде и у нас будет полный контроль над реализацией. Но при этом нам нужно будет самостоятельно оптимизировать обработчик скролла и вызова getBoundingClientRect, например, используя тротлинг или дебаунс.   6.3.4. Виртуализация  Виртуализация — это приём во фронтенде для оптимизации работы с очень большими списками элементов (например, та же бесконечная лента постов в соцсети).
Если отрендерить в DOM тысячи элементов, то приложение сильно замедлится, нагрузится память и ухудшится плавность скролла.
Что делает виртуализация? Мы рендерим в DOM только видимую часть списка + небольшой буфер вокруг. Остальные элементы заменяются "пустыми контейнерами" с корректными размерами (чтобы контент не скакал при скроле).
Когда пользователь скролит, новые элемент будут подгружаться динамически, а старые удалятся из DOM.
Таким образом, для пользователя лента будет выглядеть бесконечной и плавной, но реально в DOM будет находится всего ~20 элементов.
Это можно реализовать через через visability: hidden + фиксированную высоту элементов.  Картинка на монтаже 


Или же воспользоваться готовыми решениями, например, библиотеками react-window или vue-virtual-scroll-list.
К минусам такого подхода можно отнести, что это усложнит логику "подскрола" к элементу. И сложнее реализовать, если высота элементов будет динамической.  6.3.5. Optimistic Update  Если в рамках продукта есть пользовательское действие (например, добавление товара в избранное или клик по реакции), которое отправляет запрос на сервер, то можно применять Optimistic Update.
Идея в том, что при клике на кнопку пользователь ждет, пока придет ответ от сервера и может не понимать, что что-то произошло. Так же возможны двойные клики.
Optimistic update - это когда мы сразу после действия пользователя обновляем наш юай, не дожидаясь ответа от сервера. Но если с сервера придет ошибка, то откатываем изменения в UI и показываем эту ошибку. По сути это способ "замаскировать" сетевые задержки и сделать продукт более отзывчивым.  6.3.6. Нормализация данных
Нормализация данных — это практика организации данных таким образом, чтобы каждая сущность хранилась в одном месте, а взаимосвязи между сущностями выражались через идентификаторы.  Это позволяет:
- Избегать дублирования данных, Упрощать обновление и удаление и Оптимизировать производительность фронтенда, особенно в реактивных приложениях, где часто требуется обновлять только часть данных..
Противоположность нормализованного хранилища — денормализованное. Там данные дублируются, что может быть удобно для чтения, но создаёт сложности при изменении.
Картинка на монтаже   И Facebook, и Twitter используют нормализованное клиентское хранилище.
 Facebook использует Relay (он умеет нормализовать данные на основе GraphQL-схемы), а Twitter — использует Redux.

  Преимущества нормализованного хранилища:
  1. Меньше дублирования данных: одна «истина» для одного и того же куска информации, который может отображаться в разных местах UI.
    > Например: у многих постов будет один и тот же автор — без нормализации его данные будут дублироваться в каждом посте.
  2. Так же легко обновлять данные для одной сущности:
Если у пользователя много постов в ленте, и он сменил имя, нужно сразу отобразить новое имя во всех постах.
С нормализованным хранилищем это просто: обновили одну запись автора — и изменения применились во всех местах.
Но:
  - Это требует дополнительного кода для сборки данных
  - Дополнительные затраты на нормализации (Придется писать BFF прослойку или пользоваться готовыми решениями вроде normalizr)
 Для новостной ленты в контексте интервью нормализованное хранилище не обязательно, потому что:
    - кроме поля author, дублированных данных почти не будет;
    - лента в основном используется для чтения, обновлений данных мало (лайки и реакции меняют только локальный пост). 
По итогу: Преимущества нормализации раскрываются на реальных, больших продуктах с множеством функций (как у Facebook/Twitter), где одни и те же сущности используются в разных контекстах и местах приложения. Если мы рассматриваем только одну ленту, то нормализацию данных можно не использовать.
  6.3.7. Debounce

Debounce — это техника оптимизации, при которой выполнение функции откладывается до тех пор, пока пользователь не прекратит действие в течение заданного интервала времени. Например, в системе поиска пользователь начинает вводить текст. Вместо того чтобы отправлять запрос на сервер после каждой буквы, мы ждём n миллисекунд после последнего нажатия клавиши и только потом отправляем запрос.  Картинка на монтаже 
Этот приём хорошо применять в поиске, валидации форм, при ресайзе окна или обработке скролла. Он позволяет избежать лишних запросов, уменьшить нагрузку как на фронтенд, так и на бэкенд, и сделать интерфейс более стабильным.   Но важно понимать и минусы. Во-первых, появляется небольшая задержка в интерфейсе, пока мы ждём таймаут. Во-вторых, если мы используем кэширование результатов, то при частых исправлениях и опечатках полезные варианты могут не попадать сразу в кэш.  Например, пользователь ошибается и вместо “мел” пишет “мель” и мы кэшируем результаты для неправильного запроса. А потом стирает один символ и мы заново отправляем запрос на уже нужный квери.
    6.3.8. Graceful Degradation

Graceful degradation, или плавная деградация, — это подход в разработке, при котором система продолжает работать даже в условиях ограничений: будь то медленный интернет, слабое устройство, отключённый JavaScript или перегрузка сервера. 
При этом часть возможностей может быть урезана, но пользователь не остаётся с полностью сломанным приложением.  Идея проста: лучше дать пользователю хоть что-то, чем ничего.

Например, в нашей социальной сети если сломался сервис реакций, пользователь должен увидеть ошибку при попытке поставить лайк, но при этом продолжать просматривать ленту и создавать новые посты.  Если в другом продукте не работает модуль подписок, то мы всё равно должны дать доступ пользователю ко всему бесплатному контенту, не падая в аварийный экран.   Таким образом пользователь остаётся в продукте и может решать свои задачи, а не сталкивается с полным отказом работы системы.
(На фоне можно показать запись видео сервиса, когда часть функционала не работает)

Часть 4. Тесты и метрики
  6.4.0. О чем эта часть?  На архитектурной секции интервью часто задают вопросы про покрытие кода тестами и метрики. И это логично: тесты обеспечивают предсказуемость разработки, а метрики — предсказуемость эксплуатации. 
 Без тестов команда рискует ломать старый функционал при каждой новой фиче.  Без метрик продукт превращается в «черный ящик», где мы не понимаем, как он работает у пользователей, пока не начнут сыпаться жалобы. В этой части мы разберем, как правильно подойти к обоим аспектам.
6.4.1. Зачем вообще нужны тесты?  Представим, что вы реализовали бизнес-фичу, написали весь код. Но как проверить, что система работает правильно? Без тестов мы можем полагаться только на ручные проверки, а это резко увеличивает нагрузку на QA-отдел.  По мере роста системы, когда одна фича может затрагивать другие, мы быстро придем к ситуации, где количество ручных проверок при регрессе в рамках одного релиза станет просто нерентабельным.
Тесты позволяют экономить ресурсы на средней и длинной дистанции, снижая стоимость сопровождения продукта.  Кроме того, они выступают как документация поведения системы: зачастую проще открыть файл с тестами и посмотреть, какие сценарии заложены, чем разбираться в пятиста строках кода компонента и пытаться понять, какую бизнес-ценность он несёт.  6.4.2. Нужно ли писать тесты?

Здесь важен контекст.  Если мы говорим об MVP и проверке гипотезы бизнеса на рынке - время разработки обычно ценнее, чем написание тестов. В таком случае можно либо не писать тесты вовсе, либо ограничиться минимальными автотестами для критичных сценариев.
Если же система уже работает в продакшене, приносит деньги и должна быть стабильной - тесты становятся обязательными. Они помогают предотвратить регрессии и упрощают поддержку продукта.  Минусы тестов тоже есть: для их написания нужен опыт и понимание, как писать качественные и главное, полезные тесты. И также часть времени разработки будет уходить на покрытие кода тестами вместо реализации новых бизнес-фичей.  На собеседовании важно уметь аргументированно объяснить, когда покрытие тестами оправдано, а когда нет.

  6.4.3. Как покрыть frontend-приложение тестами?  Как покрыть приложение тестами? Существует пирамиду тестирования — это модель, которая помогает понять, какие тесты писать и в каком соотношении.  Картинка на монтаже 
Основные принципы следующие:
- Не пишем тесты на то, что проверяется типизацией и статическими линтерами. - Много unit-тестов — так как они быстро пишутся и проверяют целый модуль. - Интеграционных тестов пишем в меньшем количестве, чтобы проверить взаимодействие нескольких модулей в связке. - E2E-тестами покрываем ключевые бизнес-процессы, например, авторизацию или оплату подписки. Они хоть и мощные, но медленные, так как требуют поднятия реальной среды, работы браузера и выполнение реальных запросов. Поэтому их должно быть как можно меньше. 
Если система уже существует и важно показать её работоспособность, целесообразно сначала написать E2E-тесты на самые ключевые сценарии. Это даст максимальную ценность при минимальных усилиях.
Но важно понимать, что E2E-тесты могут давать ложное предситавление о проценте покрытия. Один большой e2e сценарий создает сразу высокий процент покрытия, хотя отдельные модули в изоляции не тестировались. 
 Дополнительно следует:  - прогонять тесты в CI, чтобы убедиться, что все тесты зеленые до релиза;  - сразу покрывать новый код тестами;  - писать тесты на баги, чтобы исключить их воспроизводимость;  - покрывать тестами рефакторинг старого кода; 
- дальше уже при наличии времени на техдолг –  пишем тесты на старый функционал
Эти правила позволят внедрить тесты в уже существующую систему.  В новой системе логично покрывать весь код тестами сразу.
  6.4.4. Как организовать процесс контроля качества приложения?  Контроль качества приложения не заканчивается релизом. Тесты нужны до релиза, но после него система продолжает работать и ломаться. 
Поэтому важно иметь процесс мониторинга и наблюдаемости. Без этого любые тесты в CI дают лишь частичную гарантию стабильности.  6.4.5. Системы мониторинга (Prometheus, Grafana, Sentry)  Пройдемся по основным элементам контроля качества:
1. Сбор метрик производительности и состояния системы
Один из способов сбора метрик это Prometheus. Prometheus - это система, которая собирает, хранит и агрегирует метрики. 
Примеры собираемых метрик это: время ответа API, количество активных пользователей, время рендера страницы, Web Vitals и любые альтернативные метрики, которые мы хотим собирать.
2. Визуализация и дашборды
Собранные метрики нужно уметь интерпретировать. Самое популярное корпоративное решение это Grafana. 
Она позволяет строить наглядные графики, дашборды и настраивать алерты. Например, вы можете видеть среднее время ответа сервера, количество падений API или время первой отрисовки страницы для разных устройств. 
Визуализация помогает ретроспективно понимать, в какой момент произошла авария (например, вышел релиз) и на какой % пользователей имеется аффект.
3. Сбор и агрегация ошибок
Тесты проверяют функциональность, но не могут предсказать все возможные ошибки на реальных устройствах или в сетевых условиях.  Sentry позволяет автоматически ловить ошибки на фронтенде и собирать информацию о стеке вызовов, окружении пользователя, браузере и версии приложения. Это помогает быстро локализовать проблему и исправить её до того, как она повлияет на бОльшее количество пользователей.
4. Алерты и оповещения
Важная часть мониторинга — это своевременное уведомление команды о критических проблемах. Для этого можно настроить алерты в Grafana или Prometheus и получать сообщение в корпоративный мессенджер, если метрики выходят за пороговые значения или возникает большое количество ошибок.

Такой подход к организации системы мониторинга на клиентах обеспечивает полный цикл контроля производительности и стабильности: от пользовательского опыта до корректной работы всех CJM'ов. В качестве домашнего задания предлагаю вам спроектировать масштабируемую архитектуру для компонента поиска в интернет магазине. Картинка на монтаже  Ваша задача самостоятельно пройтись по всем пунктам, что мы сегодня обсудили, от сбора функциональных требования до вопросов оптимизации и метрик. Подробный разбор решения этой задачи – смотрите на бусти.
На этом мы завершаем главу по системному дизайну во фронтенде. Спасибо за внимание.

Домашнее задание (SearchBox)  Всем привет! Меня зовут Автушенко Андрей. Я – ментор сообщества Frontend Alliance.  В этом видео мы разберем домашнее задание по System Design фронтенд части курса. Если вы еще не смотрели сам курс, то рекомендую сначала пройти его и попытаться самостоятельно выполнить домашнее задание, а уже потом вернуться к просмотру этого разбора. Ссылка на курс в описании под роликом.  В качестве домашнего задания была разработка архитектуры для компонента поиска в интернет магазине. 
Картинка на монтаже    Первое с чего начинаем – это с проектирования ui/ux дизайна решения. На макете мы видим input, в который пользователь сможет вводить запрос, кнопку поиска и список результатов ответа. Каждый ответ может содержать как текст, так и изображения.  Картинка на монтаже  Теперь перейдем к функциональным и нефункциональным требованиям. Картинка на монтаже 

Теперь давайте посмотрим на структуру фронтенд-компонентов  В качестве корневого компонента у нас компонент App, который содержит основной компонент SearchBox, который уже содержит оставшиеся компоненты: SearchInput (компонент, в который пользователь будет вводить свой запрос), SearchResults (список ответов) и SearchStatus, который будет отображать актуальный статус запроса. Для удобства и удовлетворения требованиям доступности у нас будет отдельный композабл, который будет сделать за событиями клавиатуры. Общение с бекендом будет проходить через ApiClient. Так же с точки зрения оптимизации у нас будет кэширование результатов, которое так же отмечаем на схеме.   
Картинка на монтаже 
Расчет нагрузки Картинка на монтаже   Таким образом мы получаем 69 запросов в секунду. Но нужно учесть фактор пиковой нагрузки. Если 20% всего трафика будет приходится на 1 час, то система должна будет выдержать 334 запроса в секунду.  Теперь перейдем к структуре API  У нас будет всего одна ручка для получения результатов поиска, которая в теле запроса принимает квери и limit (количество элементов в ответе). Картинка на монтаже    В качестве ответа будет приходить массив results, в котором будут содержаться объекты элементов. Они состоят их нескольких полей: type (тип ответа), допустим он может быть нескольких видов (image, text и организация), тип будет будет зависеть от бизнес задач и сложности продукта. Можно гибко настраивать поля в зависимости от типа, например для организации дополнительно приходит сабтайтл, с более подробной информацией (напрпмер, адрес), если картинка, то дополнительное поле img. Вот пример как это сделано в гугле Картинка на монтаже    Выбор стека  Выбор стека принципиально ничем отличаться не будет, за исключением того, что нам не нужен SSR, поэтому мы обойдемся обычным фреймворком и для разнообразия возьмем React и Zustand. Картинка на монтаже  

Доступность   
Теперь перейдем к доступности. На экране вы можете видеть пример верстки поиска с aira-атрибутами. Роль комбобокс указывает, что элемент будет полем ввода и будет связанный список, в котором будут результаты поиска. aria-haspopup=listbox сообщает скринридеру, что у нас есть всплывающий список aria-expanded показываем открыт ли сейчас список, при открытии нужно будет менять на true 
Для инпута мы добавили autofoсus, чтобы фокус сразу был на инпуте после загрузки страницы Так же отключаем различные автозаполнения, автодополнения, автозаглавление и автозамену текста от браузера, так же отключаем проверку орфографии, чтобы не исказить запросы пользователь.  aria-describedby ссылается у нас на id search-instructions. Это див, который будет иметь дополнительный информационный текст для пользователя, например “Найдено 10 результатов, используйте стрелки вверхи и вних для навигации”. Класс sr-only говорит о том, что блок будет скрыт от обычных пользователей, но доступен для скринридеров. С помощью кода мы уже задаем условие, что если результатов поиска нет, то мы не связываем блок, если есть, то связываем и уже скринридер зачитает информацию.  aria-actibedescendant связывает айди элемента из списка, чтобы скринридер понимал, который элемент находится сейчас в фокусе при перемещении стрелок пользователя.  role listbox говорит о том, что это выпадающий список с несколькими опциями на выбор, в котором может быть не только текст, но и картинки. aria-label указываем для скринридера для дополнительной информации. И внутри элементы списка со своими уникальными id.   Оптимизации  Теперь перейдем к оптимизациям. В качестве первой оптимизации мы можем использовать использовать кэширование результата поиска.  Мы можем реализовать кэш разными способами. Например в виде объекта, где ключом будет квери пользователя, а значением результат ответа.  Картинка на монтаже   Это очень простая структура, но имеет проблему с дубликатами ответов. Это может быть не страшно, если мы разрабатываем полноценную поисковую страницу как у гугла или яндекса. Ведь при клике на элемент мы средиректимся на другую страницу и кэш может перезатираться.  Но если страница и кэш планируется существовать долго, но стоит рассмотреть нормализованное представление кэша:
   В этом формате мы все ответы храним в одном объекте и присваиваем уникальный айди в качестве ключа. В самом кэше мы так же в качестве ключа используем квери, а значением идет уже массив айдишников.  В таком решении мы избавляемся от дубликатов, ведь используем только айди  Из минусов – нужны дополнительные операции на нормализацию данных.  Отлично подходит для страниц, которые должны жить долго, а соответственно и кэш. Но нужно учитывать накладные расходы на нормализацию.  При этом если в рамках системы в элементах поиска есть часто меняющиеся данные (например, цена, курс, время), то не стоит вообще использовать кэширование результатов.
 Дебаунс   Как я уже рассказывал в части оптимизации в курсе, у нас есть способ оптимизации с помощью дебаунса. Данная задача отличный пример, где ее можно применить.  Про плюсы и минусы я подробно рассказал в самом курсе  Бесконечный скролл Так же для оптимизации (если список результатов подразумевается очень большим и его нужно скролить) внедрить виртуальный скролл с подгрузкой.   В курсе так же рассказывал подробно о том как это можно сделать и какие плюсы/минусы подходов существуют.  Виртуализация Если используем бесконечный скролл, то так же можно добавить и виртуализацию, чтобы не нагружать DOM лишними нодами, которые пользователь не видит.   В целом это все основные оптимизации, которые можно применить в рамках элемента поиска. Очень надеюсь, что вам было полезно. Подписывайтесь на мои соц. сети и успехов в получении больших офферов!  
Глава 7. Системный дизайн в мобильной разработке
Спикер: Антон
Часть 1. Системный дизайн на собеседованиях и в работе мобильного разработчика

Приветствие и базовое описание


Я Антон, android-разработчик и ментор. В этой главе расскажу про системный дизайн в мобильной разработке. Будет четыре части: поговорим про важность системного дизайна на собеседованиях и в работе, обсудим как правильно проектировать api, научимся строить архитектуру приложения исходя из требований задачи, а также поговорим про кроссплатформу и современное веяние - Backend Driven UI

Пользователь в первую очередь сталкивается с результатом работы мобильного разрабочика, компании переходят к mobile-first подходу. Результат работы можно показать близким и увидеть на своем телефоне самому.
Мы любим эту профессию за это и за нативный опыт который так хорошо работает в современной среде
В этой части мы разберем собеседования, вопросы и роль мобильного разработчика для системного дизайна. А также поймем в чем уникальность его экспертизы и как стать успешным мобильным разработчиком.

Роль мобильного разработчика

Мобильный разработчик отвечает за стабильность и скорость работы приложения, а также за точку входа feature.
Основные зоны ответственности:
	•	API/контракты с бекендом
	•	Архитектура приложения и фичи
	•	Стабильность приложения и крэш фри
	•	Релизы приложения и выкатка фичей
API с бекендом

API может быть проработано системным аналитиком или бекендером, но сильный мобильный разработчик должен понимать как составить контракты между его системой и бекендом, а лучше, как это сделать между всеми бекендами. Но это уже уровень senior+.
Составление API обычно включается в интервью с мобильным разработчиком.

Архитектура приложения и фичи

Архитектура приложения выбирается из потребностей команды. Если команда большая - можно перейти к многомодульности, если она маленькая, будет легче поддерживать монолит
Фича строится продуктовыми разработчиками обычно с выбором шаблонных паттернов и подходов для проекта. Это делается так, потому что довольно часто приходится работать с чужим кодом. По статистике разработчик читает код в 5 раз больше, чем пишет

Стабильность приложения и краш фри

Мобильные системы, хоть и работают одновременно на одном устройстве, и, в отличии, от бекенда, падение одного приложения зааффектит одного пользователя, обычно имеют массовые ошибки.
Такие ошибки довольно сложно исправлять, ведь время до получения фикса пользователям обычно составляет 1-2 недели. А на битых версиях баг или краш останутся навсегда.
Здесь на помощь приходят фича-флаги, которые могут мгновенно отключить фичу на опредеденной версии приложения или на определенном устройстве/операционной системе
Для мониторинга критичных проблем используется отслеживание краш-фри (процент пользователей без крашей) метрика Для мониторинга работы фичей используется аналитика для разных действий пользоватля, которая пишется разработчиками

Релизы приложения и выкатка фичей

На первый взгляд релизы и выкатка фичей не относятся к системному дизайну, но о них нужно помнить. Так, для ускорения релизов, следовательно, для ускорения выкатки фичи в маркеты (метрика time-to-market), можно воспользоваться подходом BDUI, который будет рассмотрен подробно далее в этой главе.

Как строится собеседование на мобильного разработчика

На интервью обычно требуется спроектировать:
	•	Конкретную фичу приложения - сложность в копании вглубь реализации
	•	Приложение и его API с бекендом - сложность в верхнеуровневой картине и проектировании API
	•	Библиотеку для приложения - сложность в необычности задачи и том, что архитектура библиотеки обычно не бывает шаблонной
Итогом интервью считается спроектированная схема в каком либо сервисе проектирования
Обратите внимание, что разные интервьюеры по-разному оценивают полученную картину: кому-то надо получить законченное мвп архитектуры, а кто-то больше ценит глубину проработки сложных моментов. Поэтому без вопросов на интервью точно не обойтись.

План успешного интервью:

Первый этап - сбор функциональных и нефункциональных требований
Из функциональных требований 
вам требуется понять какие фичи должны быть в приложении, как они должны работать, как должно работать взаимодействие с бекендом  и что делать в случае ошибки или загрузки. 
Из нефункциональных требований я выделяю метрики краш-фри, размера команды, их любимый стек, time-to-market (время до выкладки в стор)
Второй этап - проектирование API
На этом этапе нужно выделить раздельные сервисы или бекенды и связать каждый из них подходящим API, учитывая функциональные требования.
Вы должны быть хорошо знакомы с:
	•	REST API
	•	WebSocket и SSE
	•	OAuth и access- и refresh- токенами
	•	Long и Short Polling
	•	Пагинацией
	•	И возможно, Graph QL
Проектирование верхнеуровневой архитектуры приложения 

Как и везде, здесь стоит действовать только после получения важной информации для этой части.
А именно:
	•	Какой нужен time-to-market?
	•	Нужна ли кроссплатформа?
	•	Какой размер команды?
	•	Планы по расширению есть?
Исходя из этого можно переходить к выбору варианта многомодульности или монолита, его подробном проектировании на уровне всего приложения 

Проектирование фичи

Данный этап может идти вместо полной секции, а также потребуется на работе при декомпозиции фичи дороже 2-х недель. Поэтому к нему стоит подготовиться особенно тщательно.
Для нативного приложения с многомодульностью/монолитом я советую подготовить архитектуру, которая вам по душе и работать с ней на каждом собеседовании, обязательно аргументируя свой выбор.
Например, для приложения на Android я советую выбирать Clean Architecture (картинка presentation -> domain <- data) и  mvvm + state.
	•	

Проектирование библиотеки

Здесь немного отличается конечный пользователь - это разработчик, поэтому отдавать наружу нужно не экран в презентационном слое (кроме дизайн-систем), а API взаимодействия с библиотекой, остальные подходы совершенно применимы из предыдущих пунктов.

Как правильно собрать нужные требования

Далее в этой главе мы ещё глубже посмотрим на то, какие требования нужно собрать для разных этапов

Узкие места для мобильного разработчика

Бутылочным горлышком на интервью можно считать все api кроме rest api: websocket, push, short и long polling, шифрование, работу ssl/tsl сертификатов, авторизацию, сохранение ключей и других sensitive-данных
По стабильности работы можно выделить краш фри, а также мониторинг скорости работы приложения и аналитику для фичей

Паттерны мобильного разработчика

На интервью стоит подготовить стандартные паттерны, используемые в мобильном приложении - observer, dependency injection, repository, mv* паттерны, factory и decoration, adapter

Что оценивают на собеседовании 

Как я говорил выше, на собеседовании ценится каждый этап интервью, потому только сделав каждый этап хорошо мы получим хороший результат. 
Например, при неполном сборе требований получится скомканная API для бекендов, а также уже по первым двум этапам будет тяжело хорошо спроектировать верхнеуровневую архитектуру приложения.
Но это вовсе не значит, если сбор требований прошел хорошо, то на остальных этапах можно расслабиться. Так будет видно вашу неподготовленность, поэтому обязательно смотри эту главу до конца чтобы научиться грокать систем-дизайн интервью

Как подготовиться
	•	Посмотреть эту главу
	•	Сделать домашку и проверить себя на бусти
	•	Пройти реальное или мок-интервью несколько раз
	•	Профит! Ты стал сеньором (или сеньоркой)
Вывод
Архитектурное интервью это, пожалуй, один из самых лучших способов проверить сеньорность мобильного разработчика, потому что на собеседовании используются исключительно практические навыки, которые применимы в работе.

Часть 2. API и Rest API
Цель раздела
С нуля показать, как спроектировать мобильный API так, чтобы он был быстрым, устойчивым и дружелюбным к клиенту. На собеседовании важно узнать направление и глубину проработки API, обычно на этот этап не стоит тратить более 5-10 минут. 

План на проектировани API: Вопросы → протоколы → конвенции → надёжность → безопасность → контракт и тесты


1) Сначала вопросы (до рисования эндпоинтов)
	•	Целевая аудитория/регионы, лимиты трафика, дешёвые устройства.
	•	Ожидаемая нагрузка (MAU/DAU, пиковый RPS, фан‑аут). Нужны backoff и rate‑limiting.
	•	Оффлайн/кэш и допустимая устаревшая дата 
	•	Требования к realtime (триггеры, задержки, «доставить не позже»).
	•	Команда и скорость: API First? кто владеет схемой?
2) Стиль и протоколы
	•	REST API — для CRUD, простых списков и операций
	•	WebSocket/SSE — для realtime‑потоков.
	•	WebSocket: полнодуплексное соединение, авто‑reconnect, backpressure, heartbeats, лимиты на fan‑out.
	•	SSE: однонаправленно, дешёво по ресурсу, дружит с прокси/CDN.
	•	Long‑polling: мгновенные уведомления, но дороже по ресурсам сервера.
	•	Выбор: лайки/счётчики → SSE + Push; чат/игра → WS; «нет соединения» → поллинг.
	•	gRPC — межсервисные стримы и низкие задержки.
	•	GraphQL/агрегирующие шлюзы - для мобилки через BFF для более сложной реализации двунаправленного соединения
	•	PushNotifications
	•	Привязываем push‑token к сессии и пользователю.
	•	Если пуши недоступны → поллинг с backoff (WorkManager/BackgroundTasks).
	•	Конфигурируем частоты с сервера; уважаем батарею/дешёвые сети.
	•	  
3) Конвенции
	•	Версионирование: путь /v1/... и заголовок X-Client-Version позволяют серверу безопасно вводить изменения/фичефлаги и управлять депрекейтами без ломки клиентов. Клиент явно сообщает свою версию — бэкенд может отдать совместимый ответ. 
	•	Идентификаторы: стабильные строковые id (не зависят от позиции в списке) не «дрейфуют» при вставках/удалениях и упрощают кеш, диффы и переиспользование данных. 
	•	Пагинация (cursor): курсоры/next_cursor исключают дубли и пропуски при добавлении новых элементов и масштабируются лучше, чем offset/page. Также вполне можно пагинироваться по дате для отсортированных по дате списков 
	•	Идемпотентность: ключ идемподентности на POST-запросы делает повторы безопасными (двойное нажатие, сетевой таймаут) — сервер вернёт тот же результат, а не создаст вторую операцию. 
	•	Ошибки: единый формат {code, message, details[]} упрощает обработку на клиенте; для троттлинга используйте 429 + Retry-After, чтобы автоматически бэкофиться. 
	•	Локаль/таймзона: Accept-Language и X-Timezone дают локализованный текст и корректные даты/сроки (например, дедлайны и расписания). 
4) Надёжность
	•	Таймауты клиента короче серверных: клиент не «висит» в неопределённости и успевает сделать контролируемый ретрай. 
	•	Короткие TTL на «горячем» трафике: снижают задержки и нагрузку на бэкенд при приемлемой «устарелости» (можно дополнить ETag/If-None-Match). 
	•	Фоллбеки/деградация payload: при плохой сети/пиках возвращаем меньше полей или более лёгкие ресурсы (thumb вместо full), чтобы сохранить UX. 
	•	Отключение тяжёлой логики при высоком RPS: сервер по фичефлагам отдаёт только важные поля/упрощённые ответы, клиент обязан корректно пережить «урезанный» формат. 
5) Безопасность
	•	OAuth2 или кастом: Access 1–2 ч + Refresh 7 дн.: короткий Access снижает риск, Refresh даёт бесшовное продление сессии. 
	•	Хранение токенов: Access — в Keystore/EncryptedPrefs, Refresh — в EncryptedPrefs; никогда не кладём в незашифрованное хранилище/логи. 
	•	Scopes на уровне эндпоинтов: минимум прав под задачу, чтобы утечка ключа не открыла доступ ко всему API. 
	•	Подпись и анти-replay: подписывайте ключами шифрования критичные действия и проверяйте nonce/timestamp, чтобы отклонять повторные/просроченные запросы. 
6) Документация и контракт
	•	API-first (OpenAPI/AsyncAPI) + линтер: схема — источник правды, из неё генерятся клиенты/стабы и ловятся ошибки совместимости до релиза. 
	•	Примеры, лимиты, SLA, ошибки: убирают догадки на клиенте, помогают быстро настроить валидацию и мониторинг. 
	•	Тест-чек-лист: проверяем идемпотентность, корректность курсорной пагинации, ретраи 429/5xx с backoff и поведение кеша — это ядро стабильного мобильного клиента. 



Часть 3. Архитектура мобильного приложения

За 5 шагов от пустого проекта до продуманной архитектуры

Шаг 1. Выбор стека (современный базис)
	•	Kotlin + Coroutines + Flow + Compose + Retrofit/OkHttp.
	•	Почему: востребованность на рынке, скорость разработки, меньше бойлерплейта.
Вопросы, которые нужно задать при выборе стека
	•	Команда и найм: чем уже владеет команда? легко ли нанимать под выбранный стек? нужны ли редкие экспертизы?
	•	Цели продукта и time‑to‑market: насколько важны быстрые итерации/А/В? сколько длится релизный цикл?
	•	Платформенная стратегия: нативно vs KMP/Flutter/React Native? какие требования к платформенным фичам (виджеты, пуши, бэкап, плееры, BLE, платежи)?
	•	Нефункциональные‑требования: оффлайн, realtime, производительность, энергопотребление, старт‑тайм, размер APK/IPA.
	•	Инфраструктура: поддержка CI/CD, инструменты профилирования, линтеры, статанализ, тестовые фреймворки.
	•	Совместимость/наследие: нужно ли встраиваться в существующий код/SDK/дизайн‑систему? есть ли жёсткие версии ОС/библиотек?
	•	Безопасность и комплаенс: крипто‑хранилища, анти‑рут/джейл‑детект, сертификат‑пиннинг, требования регуляторов.
	•	Наблюдаемость: метрики, трассировка (trace_id), логирование, краши, алерты.
	•	Стоимость владения: обучение, поддержка, вероятность vendor lock‑in, риски «одиночной технологии».
Шаг 2. Многомодульность (масштабируемая сборка) 
	•	Feature‑модули: featureX-api (контракты) и featureX-impl (реализация).
	•	Core‑модули: core-network, core-navigation, core-ui, core-utils, core-database (+ при необходимости core-analytics, core-auth).
	•	Связи:
	•	app зависит от всех core и подключает только API фич; конкретные impl биндятся через DI в app.
	•	Фичи импортируют только core + API других фич, избегая циклов.
	•	Профит: параллельные сборки, меньше merge‑конфликтов, «песочницы» команд.
	•	Многомодульность: 
	•	Монолит:
	•	
Вопросы, чтобы решить: многомодульная или монолит
	•	Размер и рост команды: сколько разработчиков сейчас и через 6–12 мес.? (>5–7 devов — аргумент за модули).
	•	Параллельная работа: есть ли независимые фичи с разными владельцами/дедлайнами?
	•	Скорость сборки: критичны ли инкрементальные сборки и быстрая локальная разработка?
	•	Границы доменов: можно ли внятно выделить bounded context‑ы (профиль, каталог, корзина и т.д.)?
	•	Релизы и ответственность: нужен ли релиз‑поезд и независимое включение/выключение фич (feature flags, dynamic features)?
	•	Стабильность контрактов: готовы ли держать API интерфейсы между модулями и соблюдать обратную совместимость?
	•	Навигация и зависимости: удобнее ли описывать маршруты контрактами, чем прямыми ссылками на экраны из других частей?
	•	Тестируемость и изоляция: нужна ли изолированная интеграционная сборка/тесты на уровне модуля?
	•	Стоимость сложности: готовы ли платить оверхедом (DI, контракты, version catalogs) ради масштабируемости?
	•	Долгосрочный план: есть ли перспектива выделения модулей в SDK/библиотеки, повторного использования между проектами?
Правило простое: монолит быстрее стартует для маленькой команды и прототипа; как только растёт команда/домены/релизная сложность — переходите к модульности.
Шаг 3. Навигация без циклов
	•	В core-navigation — контракты маршрутов (интерфейсы/DEEPLINK‑константы).
	•	Каждая фича реализует свой Router и отдаёт его через API‑контракт.
	•	Внедрение реализаций в app через DI: фичи знают контракт, но не зависят от реализаций друг друга.
Шаг 4. Архитектура фичи (P‑D‑D) + UDF
	•	Presentation: Screen (Compose) ↔ ViewModel (StateFlow, one‑shot Commands). Вся UI‑логика — в ScreenMapper (Humble Object: легко юнит‑тестить).
	•	
	•	Domain: UseCase/Interactor + Domain‑модели + интерфейсы Repository. Бизнес‑логика независима от UI/DB/сети.
	•	
	•	Data: RepositoryImpl, DataStore(Remote/Local), DTO и DB‑модели, мапперы.
	•	
	•	Поток данных: View → VM → UseCase → Repo → Store → VM → Screen. Только в одну сторону (UDF).  
Шаг 5. Надёжность, оффлайн и производительность
	•	Онлайн‑first → оффлайн‑ready: добавить core-database (Room/SQLDelight), стратегию staleness, фоновые синки.
	•	WebSocket‑устойчивость: авто‑reconnect, экспоненциальный backoff, «линейка» задержек, буфер исходящих событий.
	•	Кэш‑политики: memory/LRU, диск, ETag/If-None-Match, дельта‑апдейты.
	•	Тестирование: юнит‑тесты ScreenMapper/UseCase, интеграционные для Repo, контрактные тесты API.
	•	Нефункциональные требования по умолчанию: метрики, аналитика, crash‑репортинг, фиче‑флаги, логирование с trace_id.
Быстрый чек‑лист проектирования
	•	Собери требования: функциональные, нефункциональные, «за рамками».
	•	Спроси про аудиторию/нагрузку/регионы/оффлайн/реал‑тайм.
	•	Нарисуй high‑level: app, core, features, backend/BFF.
	•	Определи контракты feature API и зависимости

Часть 4. Backend-Driven Development (BDUI)


Введение
Концепция тонкий клиент, логика и UI на бэкенде.
	•	Как удешевить и ускорить разработку?
	•	Удешевить можно с KMP  
	•	Удешевить и ускорить можно с BDUI 
	•	Плюсы: меньше кода на клиентах, быстрый time-to-market (2 дня вместо 2 недель), кроссплатформа, быстрые эксперименты.
	•	Влияние BDUI на TTM 
	•	Минусы: высокая нагрузка на бэкенд, нужны сильная core-команда и инфраструктура, обучение разработчиков, ограничения по сложным экранам/видео/таймерам.

Как реализовать BDUI 

	•	Технологии: DivKit (UI), Kotlin/Spring (backend), JSON-описание экранов, action-handlers.
	•	Divkit -  UI-движок  
	•	 
	•	Mobile API 
	•	
	•	BDUI SDK
	•	
	•	  
	•	BDUI State
	•	
	•	
	•	Внимание, смена бизнес-домена экрана невозможна
	•	
	•	State management: кэш + синхронизация с backend.
Итерации внедрения
	•	Вынести domain+data на backend, UI оставить нативным.

	•	Подключить shared UI-библиотеку. 
	•	Полный переход на BDUI для части экранов. 
	•	Показ картинки на каждом подэтапе
	•	
Проблемы BDUI
	•	Много запросов
	•	
	•	
	•	Обучение команды
	•	
	•	
	•	
	•	Риск с персоналом
	•	
	•	Сложные экраны со сменой стейтов
	•	
	•	Работа с видео
	•	
	•	
	•	Тестирование
	•	
	•	Ранее было на андроид:
	•	
	•	Дизайн-система
	•	
	•	Релизы до:
	•	
	•	
	•	Релизы после:
	•	
	•	
	•	
Когда использовать
	•	 важна скорость релизов, готовность инвестировать в core-команду, есть потребность быстро обгонять конкурентов.
	•	


Картинки базируются на презентации, ее приложу в чате

Домашнее задание
Спроектировать масштабируемую архитектуру для приложения для просмотра акций.  4 бекенда:  1. Сервер аутентификации 2. Сервер пуш-уведомлений 3. Сервер отслеживания котировок акций 4. Сервер детальной информации  Составить контакты с бекендами  Команда состоит из 10 разработчиков middle/senior уровня
	•	Нужно спроектировать верхнеуровневую архитектуру приложения для расширения приложения
	•	Нужно спроектировать архитектуру детальной фичи для отображения информации об акции и продумать тесты для фичи и паттерны, которые должны там использоваться
	•	


Ответ на бусти
	•	Готовая схема для API бекенда
	•	Схема верхнеуровневой архитектуры приложения
	•	Схема архитектуры фичи
	•	

Видео-презентация ответа
API:  1. Сервер аутентификации - access & refresh tokens 2. Сервер пуш-уведомлений - mindbox и обработка диплинков 3. Сервер отслеживания котировок акций - webSocket 4. Сервер детальной информации -  
Верхнеуровневая архитектура приложения: 1. Многомодульность 2. Разбиение на core- и feature- modules, важность api/impl. Обоснование многомодульности - большая команда

Архитектура фичи: 1. Показ архитектуры MVVM + State 2. Важность отдельного стейта для экрана
3. Важность применения паттерна Humble Object для тестов

Картинка в виде ответа:
 

!Глава 8. Подготовка к интервью
Спикер: Антон
Часть 1. Разбор популярных вопросов по системному дизайну
Хотя интервью по системному дизайну обычно относят к типу технических, это тот этап, который очень легко слить, поскольку, как правило, нет единственно верного ответа на заданные вопросы. Здесь важно не только то, как ты будешь строить систему, но и то, как ты к этому решению придешь: насколько ты умеешь системно мыслить, выстраивать архитектуру, подбирать подходящий стек и аргументировать этот выбор.

Само собеседование подразумевает проектирование всей системы или отдельного ее компонента. Тебя могут попросить спроектировать Ebay или систему рекомендаций на Youtube – зависит и от компании, и от твоего профиля и грейда, и от нанимающего менеджера. 
Чтобы успешно пройти этот этап, тебе нужно будет соблюдать четкую структуру ответа. Во-первых, ты будешь ограничен во времени: скорее всего, у тебя будет не больше часа от начала интервью до его завершения. Во-вторых, как я уже сказал, будут обращать внимание на то, насколько ты умеешь системно мыслить, можешь ли самостоятельно спроектировать систему, аргументировать решения и т.д.

1 этап. Требования
Абсолютно всегда первый этап – это сбор требований. И тут кроется подвох: на этом собеседовании не тебе задают вопросы, а ты их задаешь.
Как ты уже знаешь, требования делятся на функциональные и нефункциональные.
Чтобы выявить функциональные требования, тебе нужны будут конкретные вопросы, касающиеся, как ни странно, функционала будущей системы, например:
	•	какие роли пользователей будут в системе;
	•	что пользователь в конкретной роли может сделать на сайте;
	•	как должен работать поиск;
	•	какие методы оплаты будут поддерживаться.

Если опираться на возможные ответы на данные вопросы, функциональные требования будут выглядеть так:
	•	должны быть роли администратора, продавца, покупателя, модератора;
	•	пользователь в роли покупателя может сменить пароль и личные данные, смотреть объявления в ленте и по отдельности, писать продавцу, оплатить товар через “безопасную сделку”, обратиться в техподдержку, удалить аккаунт;
	•	должен быть поиск по ключевым словам, расширенный фильтр, сортировка;
	•	должна быть поддержка оплаты банковской картой.

	•	Нефункциональные требования касаются непосредственно характеристик системы. Для дальнейших расчетов вам обязательно нужно запросить:
	•	DAU и MAU (нужно для расчета RPS: DAU/MAU дают средние значения, но системы проектируют под пиковую нагрузку. Обязательно нужно спросить: "В какое время суток/года нагрузка максимальна? Во сколько раз пик превышает среднюю?" (например, для мессенджера пик вечером, для eBay — в черную пятницу), скорости роста объема данных);
	•	количество пользователей через N лет (для расчета размера всех хранимых данных и дальнейшего масштабирования);
	•	размер загружаемых данных (для выбора хранилища, протокола обмена и дополнительных расчетов);
	•	соотношение записи/чтения данных (для выбора хранилища и разделения потоков данных);
	•	длительность хранения данных (также для выбора хранилища и протокола обмена).


Нефункциональные требования могут выглядеть так:
	•	DAU 1 млн, MAU 10 млн;
	•	прогноз через 5 лет – MAU 100 млн;
	•	одно изображение max 3 мб;
	•	1 пользователь – в среднем 4 чтения и 1 запись в день (80% запросов на чтение, 20% запросов на запись);
	•	1 год.

Именно функциональные и нефункциональные требования – результат работы на этом этапе.

2 этап. Расчеты
Теперь тебе нужно произвести все расчеты по твоей системе. Результаты могут быть приблизительными, то есть добиваться точного попадания не требуется, важен порядок этих расчетов. Идеально, если на собеседовании можно использовать калькулятор.
Когда я говорю о приблизительности расчетов, я имею в виду, что хотя среднее количество дней в месяце 30,44, в рамках этого этапа абсолютно нормально допустить, что их 30.
Главная цель расчетов — не получить точное число, а:
	•	понять, насколько система большая (нужно ли нам 10 серверов или 10 000?).
	•	определить критические компоненты (например, расчеты покажут, что трафик огромный -> нужен агрессивный кеш и CDN; объем данных большой -> нужен шардинг с самого начала).
	•	обосновать свои архитектурные решения.

Также тебе могут помочь готовые таблицы для работ со степенями, ну и стоит запомнить, что 10 в 6 степени – это мегабайт, а в 9 – гигабайт. Есть и другие полезные таблицы: например, таблица latency или SLA. Требование по соблюдению SLA встречается не всегда, но, опять же, для упрощения расчетов лучше иметь перед глазами такую таблицу:

(для монтажа: перевод заголовков столбцов: Доступность, Время простоя в год, Время простоя в месяц, Время простоя в неделю).

Результат работы на этом этапе – расчеты в твоей системе, которые зафиксированы письменно и устно. Скорость роста объема данных, размер всех хранимых данных, RPS и все остальное, что нужно для проектирования твоей системы.

3 этап. Домены и API
На этом этапе ты выписываешь все сущности, которые есть в твоей системе, и вместе с интервьюером обозначаешь, какие из них в текущую задачу не входят.
В случае с условным Ebay это может быть продавец, покупатель, добавление объявления, система поиска и т.д.
Здесь важно обозначить связи между сущностями.

Далее предстоит перейти к проектированию API. 
Как я упоминал ранее, необходимо объяснить свой выбор. Например, ты выбрал REST API из-за легкости тестирования и интеграции с нужными платформами; или, наоборот, выбрал GraphQL ради гибкости.
После этого нужно перейти к описанию API применительно к каждому домену системы, которые входят в задачу. Здесь также не требуется углубляться в детали, важен ход твоих мыслей, хотя, конечно, интервьюер может попросить подробнее описать какие-то решения.

4 этап. High Level Design
И вот здесь уже нужно нарисовать саму архитектуру системы. Опять же, не нужно уходить в подробности без необходимости, рисуйте верхнеуровнево. Вам нужно успеть нарисовать систему полностью и доказать, что она работает. 
На экране ты можешь увидеть пример верхнеуровневой архитектуры для индийской платежной системы (что-то вроде нашего МИРа):
Какие выводы из этой схемы можно сделать:
	•	самый простой способ – начинать от пользователя;
	•	подписанные стрелки упрощают восприятие системы;
	•	не нужно городить дата-центры, на этом этапе можно сделать допущение, что он один (но мы помним, что для масштабирования и отказоустойчивости мы можем рассмотреть multi-region deployment);
	•	лучше избегать циклических связей.

5 этап. Low level design
Теперь предстоит пройтись по созданной архитектуре и определиться с решениями для компонентов.
Не забывай рассуждать: какие варианты есть, почему выбрал именно этот, какие есть преимущества и риски. И не забудь прописать это, а не только проговорить.
Ниже ты можешь увидеть уже знакомую индийскую платежную систему, но уже в low level design:

Я специально взял пример, где не так много компонентов, чтобы разница была понятна: по сути, это две одинаковые схемы, но на второй больше внимания уделено деталям реализации внутри компонентов.

Итог
Если ты корректно прошел по всем этапам, то в конце тебя ждет несколько вопросов про твою систему: ее стабильность, отказоустойчивость, масштабируемость. К этому моменту эти вопросы уже не должны тебя смущать – все есть на доске, где ты описывал систему и ее характеристики.

В качестве тренировки рекомендую попросить кого-то из друзей провести мок-собеседование по системному дизайну для тебя. Если таких знакомых у тебя нет, обратись к нашим менторам – они помогут.
Часть 2. Ошибки на собеседованиях
Так как секция по системному дизайну достаточно сложная, кандидаты на ней часто спотыкаются, хотя кажется, что все идет нормально. Предлагаю разобрать несколько ошибок, которые встречаются особенно часто именно здесь.

	•	Использование в архитектуре незнакомых технологий
В low level design тебе, скорее всего, в той или иной степени придется объяснить, как работает каждый из компонентов. Если ты упомянул Kafka, будь готов объяснить, почему она лучше RabbitMQ для этого кейса. Если ты разбираешься в теме на уровне “где-то слышал”, то, скорее всего, это всплывет именно сейчас. Лучше выбери решение, которое тебе знакомо, даже если оно звучит не очень круто. В этом случае ты сможешь аргументированно ответить на вопрос “Почему именно так?”

	•	Требования ради требований
Возможно, кто-то посмотрел исключительно первую часть этой главы, еще пару подобных видео и пошел на собес, и это нормально: зачем прикладывать много усилий ради того, что может не окупиться. Но у такого подхода есть минус: без практики сложно понять, зачем нужен тот или иной этап на самом деле. Например, ты послушал меня, пошел спрашивать про то, сколько планируется через пять лет пользователей, и тут может быть две проблемы: 1) проект может быть вообще не рассчитан на долгосрок; 2) эти числа так и останутся в графе “Функциональные требования”. Поэтому, когда в начале курса я предлагал тебе подумать над сбором требований для онлайн-сервиса выгула котов (монтажер, вставь таймкод), было важно проделать это упражнение. Во-первых, важны вопросы, которые ты задаешь. Во-вторых, важно то, зачем ты их задаешь. И я дам дополнительное задание: представим, в рамках этого кейса ты спрашиваешь меня: “Сколько котов в среднем на одного платящего пользователя?” Я спрашиваю: “Зачем тебе эти данные?” Твоя задача – выписать все метрики и задачи, для которых этот показатель тебе понадобится.

	•	Строительство космолета
Сюда помещаются сразу несколько ошибок: попытка рассказать вообще все, что знаешь; расширение системы за границы задачи; стремление за час создать архитектуру сервиса, который перевернет мир. В первом случае помогают как раз тренировочные собесы – так ты научишься говорить только самое важное. Во втором – фиксация того, что именно от тебя требуют, мы говорили об этом в прошлой части: выписывай домены, которые нужны, и вычеркивай, которые не нужны, не затрагивай ненужные компоненты. Ну и третье: тебя наверняка будут просить спроектировать Google на собеседовании у какого-нибудь ИП, но помни, что есть и ограничения в реализации, о которых стоит спросить на этапе сбора требований. У большинства компаний нет триллионов на разработку и поддержание инфраструктуры, они хотят, чтобы система работала и приносила деньги.

	•	Отказ от продуктовых метрик
И при сборе требований, и при реализации важно помнить не только про технические аспекты, но и про продуктовые. Отсюда у нас все эти DAU, MAU и прочее. Да, может, ты разработчик, но уже не джун, и от тебя ожидают, что ты смотришь не только на свой маленький кусок работы, а оцениваешь проблему широко. Разработчик, который мыслит в терминах бизнеса (DAU, retention, conversion rate) и понимает, как технические решения влияют на эти метрики, ценится намного выше. 

На самом деле это, конечно, не все ошибки, о которых можно было бы рассказать. Есть ряд технических нюансов, а также советы вроде “не уходи в абстракцию”, но все это вытекает из предыдущих глав.

Напиши в комментах, какая из глав была тебе особенно полезна, и поставь лайк, просто потому что нам будет приятно.

Я надеюсь, этот курс был полезен для тебя. У нас впереди еще много интересных бесплатных образовательных видео по IT и не только, и будет здорово, если ты поделишься, какая тема интересует тебя больше всего.
